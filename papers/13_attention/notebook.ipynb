{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 13: Attention Is All You Need - The Transformer\n",
                "\n",
                "Interactive exploration of the architecture that changed everything.\n",
                "\n",
                "**Paper:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)\n",
                "\n",
                "**In this notebook:**\n",
                "1. Self-Attention Fundamentals\n",
                "2. Scaled Dot-Product Attention\n",
                "3. Multi-Head Attention\n",
                "4. Positional Encoding\n",
                "5. Complete Transformer\n",
                "6. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "np.random.seed(42)\n",
                "print(\"Ready to explore Transformers!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Why Attention?\n",
                "\n",
                "### The Problem with RNNs\n",
                "\n",
                "RNNs process sequences step-by-step:\n",
                "- Sequential = slow training (can't parallelize)\n",
                "- Long paths = vanishing gradients\n",
                "- Fixed hidden state = memory bottleneck\n",
                "\n",
                "### The Solution: Attention\n",
                "\n",
                "Allow every position to directly attend to every other position!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the difference\n",
                "\n",
                "# RNN: sequential information flow\n",
                "print(\"RNN Information Flow:\")\n",
                "print(\"word_1 -> word_2 -> word_3 -> ... -> word_n\")\n",
                "print(\"(Each word only sees previous context via hidden state)\")\n",
                "\n",
                "print(\"\\nTransformer Information Flow:\")\n",
                "print(\"All words connected directly via attention!\")\n",
                "\n",
                "# Simple adjacency matrix comparison\n",
                "seq_len = 5\n",
                "rnn_flow = np.tril(np.ones((seq_len, seq_len)))\n",
                "transformer_flow = np.ones((seq_len, seq_len))\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
                "\n",
                "axes[0].imshow(rnn_flow, cmap='Blues')\n",
                "axes[0].set_title('RNN: Sequential Flow', fontsize=12)\n",
                "axes[0].set_xlabel('Position')\n",
                "axes[0].set_ylabel('Position')\n",
                "\n",
                "axes[1].imshow(transformer_flow, cmap='Blues')\n",
                "axes[1].set_title('Attention: All-to-All', fontsize=12)\n",
                "axes[1].set_xlabel('Position')\n",
                "axes[1].set_ylabel('Position')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Scaled Dot-Product Attention\n",
                "\n",
                "The core computation:\n",
                "\n",
                "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
                "\n",
                "**Key insight:** Attention is a learnable, differentiable lookup!\n",
                "- **Q (Query):** What am I looking for?\n",
                "- **K (Key):** What do I contain?\n",
                "- **V (Value):** What do I return if matched?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def softmax(x, axis=-1):\n",
                "    \"\"\"Numerically stable softmax.\"\"\"\n",
                "    x_max = np.max(x, axis=axis, keepdims=True)\n",
                "    exp_x = np.exp(x - x_max)\n",
                "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
                "\n",
                "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
                "    \"\"\"\n",
                "    Compute attention.\n",
                "    \n",
                "    Args:\n",
                "        Q: Queries (batch, seq_q, d_k)\n",
                "        K: Keys (batch, seq_k, d_k)\n",
                "        V: Values (batch, seq_k, d_v)\n",
                "        mask: Optional mask\n",
                "    \"\"\"\n",
                "    d_k = K.shape[-1]\n",
                "    \n",
                "    # Step 1: Compute scores\n",
                "    scores = np.matmul(Q, K.transpose(0, 2, 1))\n",
                "    \n",
                "    # Step 2: Scale\n",
                "    scores = scores / np.sqrt(d_k)\n",
                "    \n",
                "    # Step 3: Mask (optional)\n",
                "    if mask is not None:\n",
                "        scores = np.where(mask, -1e9, scores)\n",
                "    \n",
                "    # Step 4: Softmax\n",
                "    weights = softmax(scores, axis=-1)\n",
                "    \n",
                "    # Step 5: Weighted sum\n",
                "    output = np.matmul(weights, V)\n",
                "    \n",
                "    return output, weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demo: Self-attention on a simple sequence\n",
                "\n",
                "# Imagine 3 tokens with 4-dimensional representations\n",
                "seq = np.array([[\n",
                "    [1, 0, 1, 0],  # Token 0\n",
                "    [0, 1, 0, 1],  # Token 1\n",
                "    [1, 1, 0, 0],  # Token 2\n",
                "]])  # Shape: (1, 3, 4)\n",
                "\n",
                "# In self-attention, Q = K = V = input (projected)\n",
                "Q = K = V = seq\n",
                "\n",
                "output, weights = scaled_dot_product_attention(Q, K, V)\n",
                "\n",
                "print(\"Input sequences:\")\n",
                "print(seq[0])\n",
                "\n",
                "print(\"\\nAttention weights (who attends to whom):\")\n",
                "print(weights[0].round(3))\n",
                "\n",
                "print(\"\\nOutput (weighted sum of values):\")\n",
                "print(output[0].round(3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attention weights\n",
                "\n",
                "plt.figure(figsize=(6, 5))\n",
                "plt.imshow(weights[0], cmap='Blues', vmin=0, vmax=1)\n",
                "plt.colorbar(label='Attention Weight')\n",
                "\n",
                "# Add values\n",
                "for i in range(3):\n",
                "    for j in range(3):\n",
                "        plt.text(j, i, f'{weights[0, i, j]:.2f}', \n",
                "                ha='center', va='center', fontsize=12)\n",
                "\n",
                "plt.xlabel('Key Position')\n",
                "plt.ylabel('Query Position')\n",
                "plt.title('Self-Attention Weights')\n",
                "plt.xticks([0, 1, 2], ['Token 0', 'Token 1', 'Token 2'])\n",
                "plt.yticks([0, 1, 2], ['Token 0', 'Token 1', 'Token 2'])\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Why Scale by sqrt(d_k)?\n",
                "\n",
                "Without scaling, dot products grow with dimension, pushing softmax into saturation!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate the scaling problem\n",
                "\n",
                "d_k_values = [8, 64, 512]\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "for d_k in d_k_values:\n",
                "    # Random unit-variance vectors\n",
                "    q = np.random.randn(1000, d_k)\n",
                "    k = np.random.randn(1000, d_k)\n",
                "    \n",
                "    # Dot products\n",
                "    dots = (q * k).sum(axis=1)\n",
                "    dots_scaled = dots / np.sqrt(d_k)\n",
                "    \n",
                "    axes[0].hist(dots, bins=50, alpha=0.5, label=f'd_k={d_k}', density=True)\n",
                "    axes[1].hist(dots_scaled, bins=50, alpha=0.5, label=f'd_k={d_k}', density=True)\n",
                "\n",
                "axes[0].set_title('Without Scaling: Variance grows with d_k', fontsize=11)\n",
                "axes[0].set_xlabel('Dot Product Value')\n",
                "axes[0].legend()\n",
                "\n",
                "axes[1].set_title('With Scaling: Variance is stable', fontsize=11)\n",
                "axes[1].set_xlabel('Scaled Dot Product Value')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Without scaling, large d_k -> large dot products -> extreme softmax!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 4: Multi-Head Attention\n",
                "\n",
                "One attention head learns one type of relationship. But language has MANY types!\n",
                "\n",
                "Solution: Run multiple attention heads in parallel, each focusing on different patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention:\n",
                "    def __init__(self, d_model, n_heads):\n",
                "        self.d_model = d_model\n",
                "        self.n_heads = n_heads\n",
                "        self.d_k = d_model // n_heads\n",
                "        \n",
                "        # Projection matrices\n",
                "        scale = np.sqrt(2.0 / d_model)\n",
                "        self.W_Q = np.random.randn(d_model, d_model) * scale\n",
                "        self.W_K = np.random.randn(d_model, d_model) * scale\n",
                "        self.W_V = np.random.randn(d_model, d_model) * scale\n",
                "        self.W_O = np.random.randn(d_model, d_model) * scale\n",
                "    \n",
                "    def forward(self, query, key, value, mask=None):\n",
                "        batch = query.shape[0]\n",
                "        seq_q, seq_k = query.shape[1], key.shape[1]\n",
                "        \n",
                "        # Project\n",
                "        Q = query @ self.W_Q\n",
                "        K = key @ self.W_K\n",
                "        V = value @ self.W_V\n",
                "        \n",
                "        # Reshape to (batch, n_heads, seq, d_k)\n",
                "        Q = Q.reshape(batch, seq_q, self.n_heads, self.d_k).transpose(0, 2, 1, 3)\n",
                "        K = K.reshape(batch, seq_k, self.n_heads, self.d_k).transpose(0, 2, 1, 3)\n",
                "        V = V.reshape(batch, seq_k, self.n_heads, self.d_k).transpose(0, 2, 1, 3)\n",
                "        \n",
                "        # Attention per head\n",
                "        scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(self.d_k)\n",
                "        if mask is not None:\n",
                "            scores = np.where(mask, -1e9, scores)\n",
                "        self.weights = softmax(scores, axis=-1)\n",
                "        attn = self.weights @ V\n",
                "        \n",
                "        # Concatenate heads\n",
                "        attn = attn.transpose(0, 2, 1, 3).reshape(batch, seq_q, self.d_model)\n",
                "        \n",
                "        return attn @ self.W_O\n",
                "\n",
                "# Test\n",
                "mha = MultiHeadAttention(d_model=64, n_heads=8)\n",
                "x = np.random.randn(1, 5, 64)  # 5 tokens, 64 dims\n",
                "out = mha.forward(x, x, x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {out.shape}\")\n",
                "print(f\"Attention weights shape: {mha.weights.shape} (batch, heads, seq, seq)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize different heads\n",
                "\n",
                "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
                "\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    ax.imshow(mha.weights[0, i], cmap='Blues', vmin=0, vmax=1)\n",
                "    ax.set_title(f'Head {i}')\n",
                "    ax.set_xlabel('Key')\n",
                "    ax.set_ylabel('Query')\n",
                "\n",
                "plt.suptitle('Multi-Head Attention: Each Head Learns Different Patterns', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 5: Positional Encoding\n",
                "\n",
                "Without recurrence, the model has no sense of position!\n",
                "\n",
                "\"cat sat mat\" = \"mat cat sat\" (same to pure attention)\n",
                "\n",
                "Solution: Add sinusoidal position information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_positional_encoding(max_len, d_model):\n",
                "    \"\"\"Create sinusoidal positional encoding.\"\"\"\n",
                "    pe = np.zeros((max_len, d_model))\n",
                "    position = np.arange(max_len)[:, np.newaxis]\n",
                "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
                "    \n",
                "    pe[:, 0::2] = np.sin(position * div_term)\n",
                "    pe[:, 1::2] = np.cos(position * div_term)\n",
                "    \n",
                "    return pe\n",
                "\n",
                "# Create and visualize\n",
                "pe = create_positional_encoding(100, 64)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Heatmap\n",
                "im = axes[0].imshow(pe.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
                "axes[0].set_xlabel('Position')\n",
                "axes[0].set_ylabel('Dimension')\n",
                "axes[0].set_title('Positional Encoding Heatmap')\n",
                "plt.colorbar(im, ax=axes[0])\n",
                "\n",
                "# Curves\n",
                "for dim in range(0, 8, 2):\n",
                "    axes[1].plot(pe[:50, dim], label=f'dim {dim}')\n",
                "axes[1].set_xlabel('Position')\n",
                "axes[1].set_ylabel('Encoding Value')\n",
                "axes[1].set_title('Sinusoidal Curves at Different Dimensions')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Low dimensions = slow waves (global position)\")\n",
                "print(\"High dimensions = fast waves (local details)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Position similarity: nearby positions should be similar\n",
                "\n",
                "ref_positions = [0, 10, 25, 50]\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "\n",
                "for ref in ref_positions:\n",
                "    similarities = [np.dot(pe[ref], pe[i]) for i in range(100)]\n",
                "    plt.plot(similarities, label=f'Position {ref}')\n",
                "\n",
                "plt.xlabel('Position')\n",
                "plt.ylabel('Similarity (dot product)')\n",
                "plt.title('Position Similarity: Nearby Positions Are More Similar')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 6: Putting It Together - Complete Transformer\n",
                "\n",
                "Let's see the full architecture in action!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import our implementation\n",
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "from implementation import Transformer, create_causal_mask\n",
                "\n",
                "# Create a small Transformer\n",
                "transformer = Transformer(\n",
                "    src_vocab_size=50,\n",
                "    tgt_vocab_size=50,\n",
                "    d_model=64,\n",
                "    n_heads=4,\n",
                "    n_encoder_layers=2,\n",
                "    n_decoder_layers=2,\n",
                "    d_ff=256,\n",
                "    dropout_p=0.0  # No dropout for demo\n",
                ")\n",
                "transformer.eval()\n",
                "\n",
                "print(\"Transformer created!\")\n",
                "print(f\"  d_model: 64\")\n",
                "print(f\"  n_heads: 4\")\n",
                "print(f\"  n_layers: 2 encoder, 2 decoder\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Forward pass demo\n",
                "\n",
                "# Source: \"tokens\" 1-5\n",
                "src = np.array([[1, 2, 3, 4, 5]])\n",
                "\n",
                "# Target: start token + first 3 tokens\n",
                "tgt = np.array([[0, 1, 2, 3]])\n",
                "\n",
                "# Create causal mask\n",
                "tgt_mask = create_causal_mask(tgt.shape[1])\n",
                "\n",
                "# Forward pass\n",
                "logits = transformer.forward(src, tgt, tgt_mask=tgt_mask)\n",
                "\n",
                "print(f\"Source tokens: {src[0]}\")\n",
                "print(f\"Target tokens: {tgt[0]}\")\n",
                "print(f\"Output logits shape: {logits.shape}\")\n",
                "\n",
                "# Predictions\n",
                "predictions = logits.argmax(axis=-1)\n",
                "print(f\"Predicted next tokens: {predictions[0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Greedy decoding\n",
                "\n",
                "def greedy_decode(model, src, max_len=10, start_token=0):\n",
                "    \"\"\"Generate output sequence greedily.\"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    # Start with start token\n",
                "    tgt = np.array([[start_token]])\n",
                "    \n",
                "    for _ in range(max_len - 1):\n",
                "        tgt_mask = create_causal_mask(tgt.shape[1])\n",
                "        logits = model.forward(src, tgt, tgt_mask=tgt_mask)\n",
                "        \n",
                "        # Get next token\n",
                "        next_token = logits[:, -1, :].argmax(axis=-1, keepdims=True)\n",
                "        tgt = np.concatenate([tgt, next_token], axis=1)\n",
                "    \n",
                "    return tgt\n",
                "\n",
                "# Generate\n",
                "src = np.array([[1, 2, 3, 4, 5]])\n",
                "output = greedy_decode(transformer, src, max_len=5)\n",
                "\n",
                "print(f\"Input: {src[0]}\")\n",
                "print(f\"Generated: {output[0]}\")\n",
                "print(\"\\n(Random weights, so output is random - but the architecture works!)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 7: The Causal Mask\n",
                "\n",
                "The decoder can't look at future tokens during training!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize causal mask\n",
                "\n",
                "seq_len = 6\n",
                "mask = create_causal_mask(seq_len)\n",
                "\n",
                "plt.figure(figsize=(6, 5))\n",
                "plt.imshow(~mask[0, 0], cmap='Greens')  # Show where attention IS allowed\n",
                "\n",
                "for i in range(seq_len):\n",
                "    for j in range(seq_len):\n",
                "        text = 'OK' if not mask[0, 0, i, j] else 'X'\n",
                "        color = 'white' if not mask[0, 0, i, j] else 'red'\n",
                "        plt.text(j, i, text, ha='center', va='center', fontsize=10, color=color)\n",
                "\n",
                "plt.xlabel('Key Position')\n",
                "plt.ylabel('Query Position')\n",
                "plt.title('Causal Mask: Decoder Can Only See Past')\n",
                "plt.xticks(range(seq_len))\n",
                "plt.yticks(range(seq_len))\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Each position can only attend to itself and previous positions!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### Key Components\n",
                "\n",
                "1. **Scaled Dot-Product Attention**: The core mechanism - Q, K, V with sqrt(d_k) scaling\n",
                "\n",
                "2. **Multi-Head Attention**: Parallel attention heads for different patterns\n",
                "\n",
                "3. **Positional Encoding**: Sinusoidal position information\n",
                "\n",
                "4. **Encoder Block**: Self-attention + FFN + residuals + layer norm\n",
                "\n",
                "5. **Decoder Block**: Masked self-attention + cross-attention + FFN\n",
                "\n",
                "### Why Transformers Won\n",
                "\n",
                "- Parallelizable (fast training on GPUs)\n",
                "- Direct long-range connections\n",
                "- Flexible attention patterns\n",
                "- Scale well with data and compute\n",
                "\n",
                "This architecture powers: BERT, GPT, T5, ViT, DALL-E, Stable Diffusion, and more!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Day 13 Complete!\")\n",
                "print(\"\\nYou now understand:\")\n",
                "print(\"  - How attention replaces recurrence\")\n",
                "print(\"  - Why scaling by sqrt(d_k) is crucial\")\n",
                "print(\"  - How multi-head attention captures multiple patterns\")\n",
                "print(\"  - Why positional encoding is needed\")\n",
                "print(\"  - The complete Transformer architecture\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"  - Try the exercises in exercises/\")\n",
                "print(\"  - Train on a real task with train_minimal.py\")\n",
                "print(\"  - Explore pre-trained models (BERT, GPT, etc.)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}