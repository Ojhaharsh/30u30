{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45942cc2",
   "metadata": {},
   "source": [
    "# RNN Regularization: Preventing Overfitting\n",
    "\n",
    "In this notebook, we'll explore four powerful regularization techniques:\n",
    "1. **Dropout** - Randomly disable neurons\n",
    "2. **Layer Normalization** - Stabilize training\n",
    "3. **Weight Decay** - Penalize large weights\n",
    "4. **Early Stopping** - Stop before overfitting\n",
    "\n",
    "Let's see them in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ae1c4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementation import (\n",
    "    dropout_forward, dropout_backward,\n",
    "    layer_norm_forward, layer_norm_backward,\n",
    "    RegularizedLSTM, EarlyStoppingMonitor,\n",
    "    compute_l2_penalty, regularized_loss\n",
    ")\n",
    "from visualization import (\n",
    "    plot_learning_curves,\n",
    "    plot_regularization_comparison,\n",
    "    plot_weight_distributions\n",
    ")\n",
    "from train_minimal import generate_simple_dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"âœ“ All modules loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1914fa0",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Overfitting\n",
    "\n",
    "### The Problem\n",
    "\n",
    "A model can memorize training data without learning generalizable patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b903a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate overfitting scenario\n",
    "epochs = range(30)\n",
    "\n",
    "# Without regularization: training loss decreases, validation loss increases\n",
    "train_loss_overfit = 2.0 / (1 + np.array(epochs) * 0.1) + np.random.normal(0, 0.05, len(epochs))\n",
    "val_loss_overfit = 2.0 / (1 + np.array(epochs) * 0.05) + np.random.normal(0, 0.08, len(epochs))\n",
    "val_loss_overfit[15:] = val_loss_overfit[15:] + np.array(epochs[15:]) * 0.02\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss_overfit, 'b-o', label='Training Loss', linewidth=2)\n",
    "plt.plot(epochs, val_loss_overfit, 'r-s', label='Validation Loss', linewidth=2)\n",
    "plt.axvline(x=15, color='green', linestyle='--', alpha=0.7, label='Overfitting starts')\n",
    "plt.fill_between(epochs[15:], train_loss_overfit[15:], val_loss_overfit[15:], alpha=0.2, color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('WITHOUT Regularization\\n(Overfitting Gap Grows)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# With regularization: both decrease together\n",
    "train_loss_regular = 2.0 / (1 + np.array(epochs) * 0.08) + np.random.normal(0, 0.04, len(epochs))\n",
    "val_loss_regular = 2.0 / (1 + np.array(epochs) * 0.07) + np.random.normal(0, 0.05, len(epochs))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_loss_regular, 'b-o', label='Training Loss', linewidth=2)\n",
    "plt.plot(epochs, val_loss_regular, 'r-s', label='Validation Loss', linewidth=2)\n",
    "plt.fill_between(epochs, train_loss_regular, val_loss_regular, alpha=0.2, color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('WITH Regularization\\n(Controlled, Stable)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: Without regularization - BIG GAP = overfitting\")\n",
    "print(\"Right: With regularization - SMALL GAP = good generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce321a0",
   "metadata": {},
   "source": [
    "## Part 2: Dropout - Random Deactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some activations\n",
    "activations = np.random.randn(10)\n",
    "\n",
    "print(\"Original activations:\")\n",
    "print(activations.round(2))\n",
    "\n",
    "# Apply dropout during training\n",
    "output_train, mask = dropout_forward(activations, keep_prob=0.8, training=True)\n",
    "print(f\"\\nWith dropout (keep_prob=0.8):\")\n",
    "print(output_train.round(2))\n",
    "print(f\"Mask: {mask.round(2)}\")\n",
    "print(f\"Notice: Some values are 0, others are scaled up\")\n",
    "\n",
    "# No dropout during testing\n",
    "output_test, _ = dropout_forward(activations, keep_prob=0.8, training=False)\n",
    "print(f\"\\nDuring testing (training=False):\")\n",
    "print(output_test.round(2))\n",
    "print(f\"All values pass through unchanged!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dropout effect\n",
    "activations_matrix = np.random.randn(20, 50)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Without dropout\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(activations_matrix, cmap='RdYlGn', aspect='auto')\n",
    "plt.colorbar(label='Activation value')\n",
    "plt.title('WITHOUT Dropout\\n(All neurons active)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Hidden units')\n",
    "plt.ylabel('Time steps')\n",
    "\n",
    "# With dropout\n",
    "dropped_matrix = np.zeros_like(activations_matrix)\n",
    "for i in range(activations_matrix.shape[0]):\n",
    "    dropped_matrix[i], _ = dropout_forward(activations_matrix[i], keep_prob=0.7, training=True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(dropped_matrix, cmap='RdYlGn', aspect='auto')\n",
    "plt.colorbar(label='Activation value')\n",
    "plt.title('WITH Dropout (keep_prob=0.7)\\n(30% randomly deactivated)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Hidden units')\n",
    "plt.ylabel('Time steps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Black areas = deactivated neurons\")\n",
    "print(f\"Dropout forces network to learn from different subsets of neurons\")\n",
    "print(f\"This prevents co-adaptation and improves generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec0d84",
   "metadata": {},
   "source": [
    "## Part 3: Layer Normalization - Stabilizing Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d547d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate activations from untrained network\n",
    "batch_size = 32\n",
    "hidden_size = 64\n",
    "\n",
    "# Random activations (before training)\n",
    "activations_raw = np.random.randn(batch_size, hidden_size) * 10  # Large variance\n",
    "\n",
    "print(\"Raw activations statistics:\")\n",
    "print(f\"  Mean: {activations_raw.mean(axis=1).mean():.4f}\")\n",
    "print(f\"  Std: {activations_raw.std(axis=1).mean():.4f}\")\n",
    "print(f\"  Min: {activations_raw.min():.4f}\")\n",
    "print(f\"  Max: {activations_raw.max():.4f}\")\n",
    "\n",
    "# Apply layer normalization\n",
    "gamma = np.ones((hidden_size,))\n",
    "beta = np.zeros((hidden_size,))\n",
    "\n",
    "activations_norm, cache = layer_norm_forward(activations_raw, gamma, beta)\n",
    "\n",
    "print(\"\\nAfter layer normalization:\")\n",
    "print(f\"  Mean: {activations_norm.mean(axis=1).mean():.4f}\")\n",
    "print(f\"  Std: {activations_norm.std(axis=1).mean():.4f}\")\n",
    "print(f\"  Min: {activations_norm.min():.4f}\")\n",
    "print(f\"  Max: {activations_norm.max():.4f}\")\n",
    "print(\"\\nâœ“ Much more stable! Mean â‰ˆ 0, Std â‰ˆ 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f311243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer norm effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(activations_raw.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].set_title('Raw Activations\\n(Unstable: large range)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Activation value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(activations_norm.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].set_title('After Layer Norm\\n(Stable: centered at 0, std=1)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Activation value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim([-4, 4])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Layer Normalization benefits:\")\n",
    "print(\"  1. Prevents gradient explosion (gradients stay in reasonable range)\")\n",
    "print(\"  2. Speeds up convergence (stable gradient flow)\")\n",
    "print(\"  3. Allows higher learning rates\")\n",
    "print(\"  4. Reduces dependence on weight initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638bb7a",
   "metadata": {},
   "source": [
    "## Part 4: Weight Decay - Pulling Weights to Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d1463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate weight evolution\n",
    "np.random.seed(42)\n",
    "initial_weights = np.random.randn(1000) * 0.01\n",
    "\n",
    "# Training without weight decay: weights grow large\n",
    "weights_no_decay = initial_weights.copy()\n",
    "for epoch in range(50):\n",
    "    weights_no_decay += np.random.randn(1000) * 0.05  # Gradient updates\n",
    "\n",
    "# Training with weight decay: weights stay small\n",
    "weights_with_decay = initial_weights.copy()\n",
    "weight_decay_coeff = 0.01\n",
    "for epoch in range(50):\n",
    "    weights_with_decay += np.random.randn(1000) * 0.05  # Gradient updates\n",
    "    weights_with_decay -= weight_decay_coeff * weights_with_decay  # L2 penalty pulls toward zero\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(initial_weights, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].set_title('Initial Weights\\n(Small random)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_xlabel('Weight value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "axes[1].hist(weights_no_decay, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1].set_title('After Training\\n(NO Weight Decay - Large spread)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_xlabel('Weight value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "axes[2].hist(weights_with_decay, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[2].set_title('After Training\\n(WITH Weight Decay - Concentrated near 0)', fontsize=11, fontweight='bold')\n",
    "axes[2].set_xlabel('Weight value')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Weight Decay Statistics:\")\n",
    "print(f\"Without decay: mean={weights_no_decay.mean():.4f}, std={weights_no_decay.std():.4f}\")\n",
    "print(f\"With decay:    mean={weights_with_decay.mean():.4f}, std={weights_with_decay.std():.4f}\")\n",
    "print(\"\\nWeight decay encourages simple models (Occam's Razor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show weight decay formula\n",
    "print(\"Weight Decay (L2 Regularization) Formula:\")\n",
    "print()\n",
    "print(\"  Loss_total = Loss_model + Î» * (1/2) * Î£(wÂ²)\")\n",
    "print()\n",
    "print(\"Where:\")\n",
    "print(f\"  Loss_model = cross-entropy or other model loss\")\n",
    "print(f\"  Î» (lambda) = weight decay coefficient (e.g., 0.0001)\")\n",
    "print(f\"  Î£(wÂ²) = sum of squared weights\")\n",
    "print()\n",
    "print(\"Typical values:\")\n",
    "print(f\"  Light:   Î» = 0.00001 (very gentle)\")\n",
    "print(f\"  Normal:  Î» = 0.0001  (standard)\")\n",
    "print(f\"  Strong:  Î» = 0.001   (heavy regularization)\")\n",
    "\n",
    "# Example\n",
    "print(\"\\nExample:\")\n",
    "model_loss = 0.5\n",
    "weights_example = [np.random.randn(10), np.random.randn(20)]\n",
    "weight_decay = 0.0001\n",
    "\n",
    "l2_penalty = compute_l2_penalty(weights_example, weight_decay)\n",
    "total_loss = regularized_loss(model_loss, weights_example, weight_decay)\n",
    "\n",
    "print(f\"  Model loss: {model_loss:.4f}\")\n",
    "print(f\"  L2 penalty: {l2_penalty:.6f}\")\n",
    "print(f\"  Total loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac74da62",
   "metadata": {},
   "source": [
    "## Part 5: Early Stopping - Know When to Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa766410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training with early stopping\n",
    "epochs_data = [\n",
    "    (0, 2.50, 2.51),\n",
    "    (1, 2.00, 2.10),\n",
    "    (2, 1.50, 1.80),\n",
    "    (3, 1.00, 1.70),\n",
    "    (4, 0.70, 1.75),  # Validation stops improving\n",
    "    (5, 0.50, 1.85),\n",
    "    (6, 0.35, 2.00),\n",
    "    (7, 0.25, 2.15),  # Would stop here with patience=3\n",
    "]\n",
    "\n",
    "epochs, train_losses, val_losses = zip(*epochs_data)\n",
    "\n",
    "monitor = EarlyStoppingMonitor(patience=3, verbose=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, 'b-o', linewidth=2.5, markersize=8, label='Training Loss')\n",
    "plt.plot(epochs, val_losses, 'r-s', linewidth=2.5, markersize=8, label='Validation Loss')\n",
    "\n",
    "# Mark best epoch\n",
    "best_epoch = np.argmin(val_losses)\n",
    "plt.scatter([best_epoch], [val_losses[best_epoch]], color='green', s=300, marker='*', \n",
    "           zorder=5, label=f'Best (epoch {best_epoch})')\n",
    "\n",
    "# Mark early stopping\n",
    "stop_epoch = 7\n",
    "plt.axvline(x=stop_epoch, color='orange', linestyle='--', linewidth=2.5, \n",
    "            label=f'Early Stop (patience=3, epoch {stop_epoch})')\n",
    "\n",
    "# Shade regions\n",
    "plt.axvspan(-0.5, best_epoch + 0.5, alpha=0.1, color='green', label='Improving')\n",
    "plt.axvspan(best_epoch + 0.5, stop_epoch + 0.5, alpha=0.1, color='red', label='No improvement')\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Early Stopping: Stop Before Overfitting', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Early Stopping Rules:\")\n",
    "print(f\"1. Best validation loss: epoch {best_epoch} (loss={val_losses[best_epoch]:.2f})\")\n",
    "print(f\"2. Validation stopped improving after epoch {best_epoch}\")\n",
    "print(f\"3. With patience=3, we wait 3 epochs with no improvement\")\n",
    "print(f\"4. Stop at epoch {stop_epoch} to save best model state\")\n",
    "print(f\"\\nBenefit: Saved {7-best_epoch} epochs of training + get best model! ðŸŽ¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7bc3f3",
   "metadata": {},
   "source": [
    "## Part 6: Complete Example - Training with All Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0340eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating regularized LSTM model...\")\n",
    "\n",
    "# Model\n",
    "model = RegularizedLSTM(\n",
    "    vocab_size=50,\n",
    "    hidden_size=32,\n",
    "    output_size=50,\n",
    "    dropout_keep_prob=0.8,\n",
    "    use_layer_norm=True\n",
    ")\n",
    "\n",
    "print(f\"Model created: {model.parameter_count():,} parameters\")\n",
    "print(f\"  - Dropout: keep_prob=0.8\")\n",
    "print(f\"  - Layer Normalization: enabled\")\n",
    "print(f\"  - Weight Decay: will use Î»=0.0001\")\n",
    "print(f\"  - Early Stopping: patience=5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating training data...\")\n",
    "\n",
    "train_data, train_targets = generate_simple_dataset(\n",
    "    seq_length=15,\n",
    "    vocab_size=50,\n",
    "    num_samples=200\n",
    ")\n",
    "\n",
    "val_data, val_targets = generate_simple_dataset(\n",
    "    seq_length=15,\n",
    "    vocab_size=50,\n",
    "    num_samples=50\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Sequence length: 15\")\n",
    "print(f\"Vocabulary size: 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a90907",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training with all regularization techniques...\\n\")\n",
    "\n",
    "# Configuration\n",
    "config_dict = {\n",
    "    'dropout_keep_prob': 0.8,\n",
    "    'weight_decay': 0.0001,\n",
    "    'patience': 5\n",
    "}\n",
    "\n",
    "early_stop = EarlyStoppingMonitor(patience=config_dict['patience'], verbose=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "h_state = np.zeros((model.hidden_size, 1))\n",
    "c_state = np.zeros((model.hidden_size, 1))\n",
    "\n",
    "print(f\"{'Epoch':<6} {'Train Loss':<12} {'Val Loss':<12} {'Status':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(15):\n",
    "    # Training\n",
    "    train_loss_total = 0\n",
    "    for x, y in zip(train_data, train_targets):\n",
    "        loss, h_state, c_state = model.forward(x, y, h_state, c_state, training=True)\n",
    "        weights = [model.Wf, model.Wi, model.Wc, model.Wo, model.Why]\n",
    "        train_loss_total += regularized_loss(loss, weights, config_dict['weight_decay'])\n",
    "    \n",
    "    train_loss_avg = train_loss_total / len(train_data)\n",
    "    train_losses.append(train_loss_avg)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss_total = 0\n",
    "    h_val = np.zeros((model.hidden_size, 1))\n",
    "    c_val = np.zeros((model.hidden_size, 1))\n",
    "    for x, y in zip(val_data, val_targets):\n",
    "        loss, h_val, c_val = model.forward(x, y, h_val, c_val, training=False)\n",
    "        weights = [model.Wf, model.Wi, model.Wc, model.Wo, model.Why]\n",
    "        val_loss_total += regularized_loss(loss, weights, config_dict['weight_decay'])\n",
    "    \n",
    "    val_loss_avg = val_loss_total / len(val_data)\n",
    "    val_losses.append(val_loss_avg)\n",
    "    \n",
    "    # Check early stopping\n",
    "    should_continue = early_stop.check(val_loss_avg, epoch)\n",
    "    \n",
    "    if val_loss_avg < early_stop.best_loss:\n",
    "        status = \"âœ“ IMPROVED\"\n",
    "    else:\n",
    "        status = f\"âœ— No improve\"\n",
    "    \n",
    "    print(f\"{epoch:<6} {train_loss_avg:<12.4f} {val_loss_avg:<12.4f} {status:<15}\")\n",
    "    \n",
    "    if not should_continue:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch}!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ“ Training complete!\")\n",
    "print(f\"  Best validation loss: {early_stop.best_loss:.4f} (epoch {early_stop.best_epoch})\")\n",
    "print(f\"  Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Final val loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plot_learning_curves(train_losses, val_losses, \n",
    "                     title='Learning Curves with All Regularization Techniques')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"1. Training loss decreases steadily\")\n",
    "print(\"2. Validation loss decreases and then stabilizes\")\n",
    "print(\"3. Gap between train and val stays small (no overfitting!)\")\n",
    "print(\"4. Early stopping prevents wasted training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b076c1",
   "metadata": {},
   "source": [
    "## Summary: When to Use Each Technique\n",
    "\n",
    "| Technique | When to use | Typical value | Cost |\n",
    "|-----------|-----------|---------------|------|\n",
    "| **Dropout** | Large networks, high variance | keep_prob=0.8 | Medium (slower training) |\n",
    "| **Layer Norm** | Deep RNNs, unstable training | Always on | Low (tiny computation) |\n",
    "| **Weight Decay** | All models, overfitting | Î»=0.0001 | None (tiny overhead) |\n",
    "| **Early Stopping** | All models, prevent waste | patience=5 | Negative! (saves time) |\n",
    "\n",
    "**Pro Tip:** Use all four together for best results! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fe567",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ“ You've learned:\")\n",
    "print(\"  âœ“ What overfitting is and why it happens\")\n",
    "print(\"  âœ“ How dropout prevents co-adaptation\")\n",
    "print(\"  âœ“ How layer norm stabilizes training\")\n",
    "print(\"  âœ“ How weight decay encourages simplicity\")\n",
    "print(\"  âœ“ How early stopping prevents memorization\")\n",
    "print(\"\\nðŸ“š Next: Check out the exercises to practice!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
