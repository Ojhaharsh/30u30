{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 24: GPipe - Efficient Training of Giant Neural Networks\n",
    "\n",
    "> Huang et al. (2018/2019) - [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/abs/1811.06965)\n",
    "\n",
    "### What You'll Learn:\n",
    "1. **Model Partitioning**: How to logically split a sequential model across $K$ stages.\n",
    "2. **Micro-batching**: The mechanics of slicing mini-batches to minimize the 'Pipeline Bubble'.\n",
    "3. **Pipeline Scheduling**: Visualizing the flow of data through stages (Section 3.1).\n",
    "4. **Efficiency Math**: Calculating theoretical bubble overhead vs. empirical timing.\n",
    "5. **Re-materialization**: Implementing activation checkpointing to break the Memory Wall (Section 3.2).\n",
    "6. **Gradient Equivalence**: Proving that synchronous updates match standard SGD precisely.\n",
    "7. **Memory Scaling**: Benchmarking peak memory reduction on simulated 'giant' networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment\n",
    "We use PyTorch for the core logic and Matplotlib for visualizing the pipeline schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from implementation import GPipe, get_peak_memory, summarize_results\n",
    "from visualization import plot_pipeline_schedule\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "print(f\"Experiment running on: {device.type.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Micro-batching Logic (Section 3.1)\n",
    "In GPipe, we don't pass the whole batch to a partition. We slice it into $M$ micro-batches. \n",
    "Note how `torch.chunk` handles the slicing across the batch dimension (Dim 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_microbatches = 8\n",
    "hidden_dim = 512\n",
    "\n",
    "mini_batch = torch.randn(batch_size, hidden_dim)\n",
    "micro_batches = torch.chunk(mini_batch, n_microbatches, dim=0)\n",
    "\n",
    "print(f\"Full Batch:  {list(mini_batch.shape)}\")\n",
    "print(f\"Micro-batch: {list(micro_batches[0].shape)}\")\n",
    "print(f\"Total MBs:   {len(micro_batches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Pipeline Schedule\n",
    "Wait time (the 'bubble') occurs at the beginning and end of the pipeline. \n",
    "We want $M$ (micro-batches) to be much larger than $K$ (partitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, M = 4, 16\n",
    "print(f\"Theoretical Efficiency: {M / (M + K - 1):.2%}\")\n",
    "\n",
    "plot_pipeline_schedule(n_partitions=K, n_microbatches=M, output_dir='plots')\n",
    "# The resulting Gantt chart shows how devices spend 90% of their time active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Re-materialization: Breaking the Memory Wall\n",
    "Standard training stores all intermediate activations ($O(L)$). \n",
    "GPipe with Re-materialization stores only inputs to partitions ($O(L/K)$).\n",
    "\n",
    "We build a 'Giant' 60-layer model to test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_giant_model(layers=60, dim=1024):\n",
    "    return nn.Sequential(*[nn.Linear(dim, dim) for _ in range(layers)])\n",
    "\n",
    "giant_model = build_giant_model()\n",
    "\n",
    "# 1. GPipe WITHOUT Checkpointing\n",
    "model_no_ckpt = GPipe(giant_model, n_partitions=4, n_microbatches=4, use_checkpoint=False).to(device)\n",
    "\n",
    "# 2. GPipe WITH Checkpointing (The Paper's Proposed Method)\n",
    "model_with_ckpt = GPipe(giant_model, n_partitions=4, n_microbatches=4, use_checkpoint=True).to(device)\n",
    "\n",
    "print(\"Models ready for benchmarking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Empirical Memory Benchmark\n",
    "We measure peak memory during a forward+backward pass. \n",
    "*Note: Results are most visible on GPU devices.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 1024).to(device)\n",
    "y = torch.randn(64, 1024).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def run_step(model, data, target):\n",
    "    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n",
    "    out = model(data)\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    return get_peak_memory()\n",
    "\n",
    "mem_no = run_step(model_no_ckpt, x, y)\n",
    "mem_yes = run_step(model_with_ckpt, x, y)\n",
    "\n",
    "print(f\"Peak Memory (No Checkpoint):   {mem_no:.2f} MB\")\n",
    "print(f\"Peak Memory (With Checkpoint): {mem_yes:.2f} MB\")\n",
    "print(f\"Reduction Factor:               {mem_no/mem_yes if mem_yes > 0 else 0:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Proving Gradient Equivalence\n",
    "One of GPipe's strongest claims is that it is mathematically identical to sequential training. \n",
    "We verify this by comparing gradients of a GPipe model vs a standard Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two identical models\n",
    "base_model = nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10))\n",
    "seq_model = base_model.to(device)\n",
    "piped_model = GPipe(base_model, n_partitions=2, n_microbatches=4).to(device)\n",
    "\n",
    "test_input = torch.randn(16, 10).to(device)\n",
    "\n",
    "# Forward / Backward on Sequential\n",
    "seq_out = seq_model(test_input)\n",
    "seq_out.sum().backward()\n",
    "seq_grads = [p.grad.clone() for p in seq_model.parameters()]\n",
    "\n",
    "# Reset\n",
    "for p in seq_model.parameters(): p.grad.zero_() \n",
    "\n",
    "# Forward / Backward on GPipe\n",
    "piped_out = piped_model(test_input)\n",
    "piped_out.sum().backward()\n",
    "piped_grads = [p.grad.clone() for p in piped_model.parameters()]\n",
    "\n",
    "# Compare\n",
    "diff = sum(torch.norm(s - p) for s, p in zip(seq_grads, piped_grads))\n",
    "print(f\"Total Gradient Difference: {diff.item():.2e}\")\n",
    "assert diff < 1e-5, \"Gradients do not match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Operational Summary & Best Practices\n",
    "\n",
    "1. **Partition Balancing**: If Stage 2 is 2x slower than Stage 1, the whole pipeline throttles to Stage 2's speed.\n",
    "2. **Communication Frequency**: Increasing $M$ helps utilization but increases the number of cross-device syncs. \n",
    "3. **Synchronous Pipelining**: GPipe's synchronous nature makes it reliable for research, unlike asynchronous methods that might introduce 'Stale Gradient' noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
