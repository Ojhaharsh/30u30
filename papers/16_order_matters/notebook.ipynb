{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea95c2ce",
   "metadata": {},
   "source": [
    "# üéØ Day 16: Order Matters - Pointer Networks\n",
    "\n",
    "**Paper:** *Order Matters: Sequence to Sequence for Sets* (Vinyals et al., 2015)\n",
    "\n",
    "Welcome to this interactive notebook! Today we'll explore **Pointer Networks** - a neural architecture that can process unordered sets and output ordered sequences.\n",
    "\n",
    "## üçï The Pizza Delivery Analogy\n",
    "\n",
    "Imagine you're a pizza delivery driver:\n",
    "\n",
    "- **Input (Set):** You get 5 pizza orders from different addresses - order doesn't matter\n",
    "- **Output (Sequence):** You need to plan your delivery route - order DOES matter!\n",
    "\n",
    "That's what Pointer Networks do:\n",
    "1. Read a SET of items (no order)\n",
    "2. Process them intelligently\n",
    "3. Output a SEQUENCE by \"pointing\" to items one-by-one\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba3aab",
   "metadata": {},
   "source": [
    "## üìö Part 1: Understanding Pointer Attention\n",
    "\n",
    "The core innovation is the **pointer mechanism**:\n",
    "- Instead of generating words from a vocabulary\n",
    "- We \"point\" to positions in the input\n",
    "\n",
    "Let's build it step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66863e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePointerAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified pointer attention mechanism.\n",
    "    \n",
    "    Formula: attention(query, keys) = softmax(v^T tanh(W_q * query + W_k * keys))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_key = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_query = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, query, keys, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: [batch, hidden_dim] - what we're looking for\n",
    "            keys: [batch, seq_len, hidden_dim] - where we search\n",
    "            mask: [batch, seq_len] - 1 for positions to mask\n",
    "        Returns:\n",
    "            attention_weights: [batch, seq_len]\n",
    "        \"\"\"\n",
    "        # Transform query and keys\n",
    "        query_proj = self.W_query(query).unsqueeze(1)  # [batch, 1, hidden_dim]\n",
    "        keys_proj = self.W_key(keys)                    # [batch, seq_len, hidden_dim]\n",
    "        \n",
    "        # Additive attention\n",
    "        scores = self.v(torch.tanh(query_proj + keys_proj))  # [batch, seq_len, 1]\n",
    "        scores = scores.squeeze(-1)  # [batch, seq_len]\n",
    "        \n",
    "        # Apply mask (set masked positions to -inf)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.bool(), float('-inf'))\n",
    "        \n",
    "        # Softmax to get probabilities\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        return attention_weights\n",
    "\n",
    "# Test it!\n",
    "attention = SimplePointerAttention(hidden_dim=32)\n",
    "\n",
    "query = torch.randn(1, 32)\n",
    "keys = torch.randn(1, 5, 32)\n",
    "\n",
    "weights = attention(query, keys)\n",
    "\n",
    "print(\"Attention weights:\", weights[0].tolist())\n",
    "print(f\"Sum: {weights.sum():.4f} (should be 1.0)\")\n",
    "print(f\"\\n‚úÖ Pointer attention works! It's deciding which input position to focus on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6afc9b",
   "metadata": {},
   "source": [
    "### üéØ Interactive: Visualize Attention\n",
    "\n",
    "Let's see what the attention \"sees\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd76fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_example():\n",
    "    # Create 5 input items with different values\n",
    "    items = torch.tensor([[0.2, 0.8, 0.1, 0.9, 0.5]])\n",
    "    \n",
    "    # Simple embedding\n",
    "    embed = nn.Linear(1, 32)\n",
    "    keys = embed(items.unsqueeze(-1))\n",
    "    \n",
    "    # Query looking for \"high values\"\n",
    "    query = torch.randn(1, 32)\n",
    "    \n",
    "    # Compute attention\n",
    "    attention_layer = SimplePointerAttention(32)\n",
    "    weights = attention_layer(query, keys)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n",
    "    \n",
    "    # Input values\n",
    "    ax1.bar(range(5), items[0].numpy(), color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('Input Values', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Position')\n",
    "    ax1.set_ylabel('Value')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Attention weights\n",
    "    ax2.bar(range(5), weights[0].detach().numpy(), color='coral', alpha=0.7)\n",
    "    ax2.set_title('Attention Weights (Where to Point)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Position')\n",
    "    ax2.set_ylabel('Probability')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show which position has highest attention\n",
    "    best_pos = torch.argmax(weights[0]).item()\n",
    "    print(f\"\\nüëâ Model points to position {best_pos} (value: {items[0, best_pos]:.2f})\")\n",
    "\n",
    "visualize_attention_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37e463",
   "metadata": {},
   "source": [
    "## üî¢ Part 2: Sorting with Pointer Networks\n",
    "\n",
    "Let's tackle the simplest problem: **sorting numbers**\n",
    "\n",
    "- Input: `[0.7, 0.2, 0.9, 0.1, 0.5]` (unordered set)\n",
    "- Output: `[3, 1, 4, 0, 2]` (indices in sorted order)\n",
    "\n",
    "This means: point to position 3 (0.1), then 1 (0.2), then 4 (0.5), then 0 (0.7), then 2 (0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNetwork(nn.Module):\n",
    "    \"\"\"Full Pointer Network for sorting.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=1, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Encoder: Process input set\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Decoder: Generate output sequence\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Pointer attention\n",
    "        self.attention = SimplePointerAttention(hidden_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: [batch, seq_len] numbers to sort\n",
    "        Returns:\n",
    "            all_pointers: [batch, seq_len, seq_len] attention logits\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = inputs.shape\n",
    "        inputs = inputs.unsqueeze(-1)  # [batch, seq_len, 1]\n",
    "        \n",
    "        # Encode\n",
    "        encoder_outputs, (h, c) = self.encoder(inputs)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_state = (h, c)\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.hidden_dim, device=inputs.device)\n",
    "        \n",
    "        all_pointers = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # One decoding step\n",
    "            decoder_output, decoder_state = self.decoder(decoder_input, decoder_state)\n",
    "            \n",
    "            # Compute where to point\n",
    "            weights = self.attention(decoder_output.squeeze(1), encoder_outputs)\n",
    "            all_pointers.append(weights)\n",
    "            \n",
    "            # Next input: weighted sum of encoder outputs\n",
    "            context = torch.bmm(weights.unsqueeze(1), encoder_outputs)\n",
    "            decoder_input = context\n",
    "        \n",
    "        return torch.stack(all_pointers, dim=1)\n",
    "\n",
    "# Create model\n",
    "model = PointerNetwork(input_dim=1, hidden_dim=64)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0713b",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Train on Sorting Task\n",
    "\n",
    "Let's train the model to sort numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sorting_batch(batch_size=32, seq_len=5):\n",
    "    \"\"\"Generate random sorting problems.\"\"\"\n",
    "    # Random numbers\n",
    "    inputs = torch.rand(batch_size, seq_len)\n",
    "    \n",
    "    # Sorting indices (targets)\n",
    "    targets = torch.argsort(inputs, dim=1)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "def train_sorting(model, num_epochs=20, batch_size=32, seq_len=5):\n",
    "    \"\"\"Train model to sort numbers.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Generate training batch\n",
    "        inputs, targets = generate_sorting_batch(batch_size, seq_len)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        logits_flat = logits.view(-1, seq_len)\n",
    "        targets_flat = targets.view(-1)\n",
    "        loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        accuracy = (predictions == targets).all(dim=1).float().mean().item()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}, Acc: {accuracy:.2%}\")\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "# Train!\n",
    "print(\"üèãÔ∏è Training on sorting task...\\n\")\n",
    "losses, accuracies = train_sorting(model, num_epochs=20, seq_len=5)\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(losses, color='red', linewidth=2)\n",
    "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(accuracies, color='green', linewidth=2)\n",
    "ax2.set_title('Accuracy (Perfect Sequences)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Final accuracy: {accuracies[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8264dfe",
   "metadata": {},
   "source": [
    "### üéØ Interactive: Test Your Own Numbers!\n",
    "\n",
    "Let's sort some numbers and visualize what the model is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_visualize(model, numbers):\n",
    "    \"\"\"\n",
    "    Test model on custom numbers and visualize attention.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PointerNetwork\n",
    "        numbers: List of numbers to sort\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    inputs = torch.tensor([numbers]).float()\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs)\n",
    "        predictions = torch.argmax(logits, dim=-1)[0]\n",
    "    \n",
    "    # Ground truth\n",
    "    true_order = torch.argsort(inputs[0])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # 1. Input numbers\n",
    "    ax1 = plt.subplot(3, 1, 1)\n",
    "    bars = ax1.bar(range(len(numbers)), numbers, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    for i, (bar, num) in enumerate(zip(bars, numbers)):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{num:.2f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, -0.05, \n",
    "                f'idx:{i}', ha='center', va='top', fontsize=10, color='gray')\n",
    "    ax1.set_title('Input Numbers (Unsorted)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylim([0, max(numbers) + 0.2])\n",
    "    ax1.set_xticks([])\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 2. Attention heatmap\n",
    "    ax2 = plt.subplot(3, 1, 2)\n",
    "    attention_matrix = logits[0].softmax(dim=-1).numpy()\n",
    "    im = ax2.imshow(attention_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    ax2.set_title('Attention Heatmap (Where Model Points)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Output Step')\n",
    "    ax2.set_xlabel('Input Position')\n",
    "    ax2.set_yticks(range(len(numbers)))\n",
    "    ax2.set_yticklabels([f'Step {i+1}' for i in range(len(numbers))])\n",
    "    ax2.set_xticks(range(len(numbers)))\n",
    "    plt.colorbar(im, ax=ax2, label='Attention Weight')\n",
    "    \n",
    "    # 3. Predicted order\n",
    "    ax3 = plt.subplot(3, 1, 3)\n",
    "    sorted_nums = [numbers[i] for i in predictions.tolist()]\n",
    "    bars = ax3.bar(range(len(sorted_nums)), sorted_nums, \n",
    "                   color='lightgreen' if torch.equal(predictions, true_order) else 'salmon', \n",
    "                   alpha=0.7, edgecolor='black')\n",
    "    for i, (bar, num, idx) in enumerate(zip(bars, sorted_nums, predictions.tolist())):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{num:.2f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, -0.05, \n",
    "                f'from:{idx}', ha='center', va='top', fontsize=10, color='gray')\n",
    "    \n",
    "    title = 'Predicted Order ‚úÖ CORRECT!' if torch.equal(predictions, true_order) else 'Predicted Order ‚ùå WRONG'\n",
    "    ax3.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylim([0, max(numbers) + 0.2])\n",
    "    ax3.set_xticks([])\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ Sorting Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Input:          {numbers}\")\n",
    "    print(f\"Predicted order: {predictions.tolist()}\")\n",
    "    print(f\"True order:      {true_order.tolist()}\")\n",
    "    print(f\"Sorted values:   {sorted_nums}\")\n",
    "    \n",
    "    if torch.equal(predictions, true_order):\n",
    "        print(\"\\n‚úÖ Perfect sorting!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Not quite right. Try training longer!\")\n",
    "\n",
    "# Test on example\n",
    "test_numbers = [0.7, 0.2, 0.9, 0.1, 0.5]\n",
    "print(f\"Testing on: {test_numbers}\\n\")\n",
    "test_and_visualize(model, test_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426b84d",
   "metadata": {},
   "source": [
    "### üéÆ Try Your Own Numbers!\n",
    "\n",
    "Change the numbers below and see how the model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88755e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è EDIT THESE NUMBERS!\n",
    "my_numbers = [0.3, 0.8, 0.1, 0.6, 0.4]\n",
    "\n",
    "test_and_visualize(model, my_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad402f",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Part 3: Traveling Salesman Problem (TSP)\n",
    "\n",
    "Now let's tackle a **harder problem**: visiting cities in optimal order\n",
    "\n",
    "- Input: Set of city locations `[(x‚ÇÅ, y‚ÇÅ), (x‚ÇÇ, y‚ÇÇ), ...]`\n",
    "- Output: Tour order `[0, 3, 1, 4, 2]` that minimizes total distance\n",
    "\n",
    "This is **NP-hard** (no known efficient optimal algorithm)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52556f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPPointerNetwork(nn.Module):\n",
    "    \"\"\"Pointer Network for TSP with masking.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = SimplePointerAttention(hidden_dim)\n",
    "    \n",
    "    def forward(self, cities):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cities: [batch, num_cities, 2] - city coordinates\n",
    "        Returns:\n",
    "            tour: [batch, num_cities] - predicted tour\n",
    "            all_logits: [batch, num_cities, num_cities] - attention scores\n",
    "        \"\"\"\n",
    "        batch_size, num_cities, _ = cities.shape\n",
    "        \n",
    "        # Encode\n",
    "        encoder_outputs, (h, c) = self.encoder(cities)\n",
    "        \n",
    "        # Decode with masking\n",
    "        decoder_state = (h, c)\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.hidden_dim, device=cities.device)\n",
    "        \n",
    "        mask = torch.zeros(batch_size, num_cities, device=cities.device)\n",
    "        tour = []\n",
    "        all_logits = []\n",
    "        \n",
    "        for t in range(num_cities):\n",
    "            decoder_output, decoder_state = self.decoder(decoder_input, decoder_state)\n",
    "            \n",
    "            # Compute attention with masking\n",
    "            weights = self.attention(decoder_output.squeeze(1), encoder_outputs, mask)\n",
    "            all_logits.append(weights)\n",
    "            \n",
    "            # Greedy selection\n",
    "            _, selected = weights.max(dim=-1)\n",
    "            tour.append(selected)\n",
    "            \n",
    "            # Update mask (mark selected city as visited)\n",
    "            mask.scatter_(1, selected.unsqueeze(1), 1)\n",
    "            \n",
    "            # Update decoder input\n",
    "            context = torch.bmm(weights.unsqueeze(1), encoder_outputs)\n",
    "            decoder_input = context\n",
    "        \n",
    "        tour = torch.stack(tour, dim=1)\n",
    "        all_logits = torch.stack(all_logits, dim=1)\n",
    "        \n",
    "        return tour, all_logits\n",
    "\n",
    "def compute_tour_length(cities, tour):\n",
    "    \"\"\"Compute total tour distance.\"\"\"\n",
    "    length = 0.0\n",
    "    for i in range(len(tour)):\n",
    "        current = tour[i]\n",
    "        next_city = tour[(i + 1) % len(tour)]\n",
    "        length += np.linalg.norm(cities[current] - cities[next_city])\n",
    "    return length\n",
    "\n",
    "# Create TSP model\n",
    "tsp_model = TSPPointerNetwork(input_dim=2, hidden_dim=64)\n",
    "print(f\"TSP model created with {sum(p.numel() for p in tsp_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2c17a",
   "metadata": {},
   "source": [
    "### üó∫Ô∏è Visualize TSP Tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb06e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsp(cities, tour, title=\"TSP Tour\"):\n",
    "    \"\"\"Visualize a TSP tour.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Plot cities\n",
    "    plt.scatter(cities[:, 0], cities[:, 1], c='blue', s=200, alpha=0.6, zorder=3, edgecolors='black', linewidth=2)\n",
    "    \n",
    "    # Label cities\n",
    "    for i, (x, y) in enumerate(cities):\n",
    "        plt.text(x, y, str(i), fontsize=12, ha='center', va='center', \n",
    "                fontweight='bold', color='white', zorder=4)\n",
    "    \n",
    "    # Draw tour\n",
    "    for i in range(len(tour)):\n",
    "        start_idx = tour[i]\n",
    "        end_idx = tour[(i + 1) % len(tour)]\n",
    "        \n",
    "        start = cities[start_idx]\n",
    "        end = cities[end_idx]\n",
    "        \n",
    "        # Draw arrow\n",
    "        dx = end[0] - start[0]\n",
    "        dy = end[1] - start[1]\n",
    "        plt.arrow(start[0], start[1], dx*0.9, dy*0.9,\n",
    "                 head_width=0.03, head_length=0.03, \n",
    "                 fc='red', ec='red', alpha=0.7, zorder=2,\n",
    "                 length_includes_head=True, linewidth=2)\n",
    "    \n",
    "    # Compute and display tour length\n",
    "    length = compute_tour_length(cities, tour)\n",
    "    \n",
    "    plt.title(f\"{title}\\nTour Length: {length:.3f}\", fontsize=14, fontweight='bold')\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate random cities\n",
    "np.random.seed(42)\n",
    "num_cities = 8\n",
    "cities = np.random.rand(num_cities, 2)\n",
    "\n",
    "# Test untrained model\n",
    "tsp_model.eval()\n",
    "cities_tensor = torch.from_numpy(cities).float().unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tour, _ = tsp_model(cities_tensor)\n",
    "    tour = tour[0].numpy()\n",
    "\n",
    "visualize_tsp(cities, tour, \"Untrained Model\")\n",
    "\n",
    "print(f\"\\nTour: {tour.tolist()}\")\n",
    "print(f\"Length: {compute_tour_length(cities, tour):.3f}\")\n",
    "print(\"\\nüí° Notice: Untrained model makes a random tour\")\n",
    "print(\"   Training would make it find shorter tours!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8baa71",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### ‚ú® What We Learned\n",
    "\n",
    "1. **Pointer Networks** solve set-to-sequence problems:\n",
    "   - Input: Unordered set\n",
    "   - Output: Ordered sequence by \"pointing\" to input elements\n",
    "\n",
    "2. **Three Key Components:**\n",
    "   - **Encoder:** Process input set\n",
    "   - **Decoder:** Generate sequence step-by-step\n",
    "   - **Pointer Attention:** Decide which input to point to at each step\n",
    "\n",
    "3. **Applications:**\n",
    "   - ‚úÖ Sorting numbers\n",
    "   - ‚úÖ Convex hull computation\n",
    "   - ‚úÖ Traveling Salesman Problem\n",
    "   - ‚úÖ Any problem where output references input!\n",
    "\n",
    "### üöÄ Going Further\n",
    "\n",
    "Want to dive deeper? Check out:\n",
    "\n",
    "1. **Exercise files** (`exercises/exercise_*.py`) - Build components from scratch\n",
    "2. **Solution files** (`solutions/solution_*.py`) - See complete implementations\n",
    "3. **PAPER_NOTES.md** - Deep dive into the original paper\n",
    "4. **CHEATSHEET.md** - Quick reference guide\n",
    "\n",
    "### üéØ Challenges\n",
    "\n",
    "Try these on your own:\n",
    "\n",
    "1. Train the sorting model on longer sequences (10-20 numbers)\n",
    "2. Implement beam search instead of greedy decoding\n",
    "3. Add a 2-opt post-processing step for TSP\n",
    "4. Try reinforcement learning instead of supervised learning\n",
    "\n",
    "---\n",
    "\n",
    "**Happy learning! üéâ**\n",
    "\n",
    "Remember: The key insight is that **order matters** in the output, even when it doesn't in the input. Pointer Networks elegantly handle this by learning to point!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
