{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ff50454d",
            "metadata": {},
            "source": [
                "# Day 15: Bahdanau Attention\n",
                "\n",
                "**Paper:** \"Neural Machine Translation by Jointly Learning to Align and Translate\" - Bahdanau, Cho, Bengio (2014)\n",
                "\n",
                "We implement the additive attention mechanism from scratch and train a seq2seq model on a sequence reversal task to visualize how attention learns to align input and output positions.\n",
                "\n",
                "---\n",
                "\n",
                "## What You'll Learn\n",
                "\n",
                "1. How the fixed-length bottleneck in vanilla seq2seq motivates attention\n",
                "2. The additive (Bahdanau) scoring function: $a(s_{i-1}, h_j) = v^T \\tanh(W s_{i-1} + U h_j)$\n",
                "3. How alignment weights produce a dynamic context vector at each decoding step\n",
                "4. Bidirectional encoder design (Section 3.2 of the paper)\n",
                "5. Visualizing attention matrices to verify learned alignments"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "12f27ec1",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d6f7aec5",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import random\n",
                "\n",
                "# Set seeds for reproducibility\n",
                "torch.manual_seed(42)\n",
                "random.seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "385c8edb",
            "metadata": {},
            "source": [
                "## 1. The Attention Mechanism\n",
                "\n",
                "The core idea (Section 3). Given:\n",
                "- Decoder state `s` (what we're trying to predict)\n",
                "- Encoder outputs `h1, h2, ..., hn` (what we're attending to)\n",
                "\n",
                "We compute:\n",
                "```\n",
                "score(s, hj) = v^T * tanh(W_s*s + W_h*hj)   # How relevant is position j?\n",
                "alpha_j = softmax(scores)                   # Normalize to probabilities\n",
                "context = sum(alpha_j * hj)                 # Weighted sum\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "694ab941",
            "metadata": {},
            "outputs": [],
            "source": [
                "class BahdanauAttention(nn.Module):\n",
                "    \"\"\"\n",
                "    Additive Attention (Bahdanau et al., 2014)\n",
                "    \n",
                "    The 'additive' name comes from: tanh(W_s*s + W_h*h)\n",
                "    We ADD the transformed query and key, then apply tanh.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, encoder_dim, decoder_dim, attention_dim=None):\n",
                "        super().__init__()\n",
                "        if attention_dim is None:\n",
                "            attention_dim = decoder_dim\n",
                "        \n",
                "        self.W_h = nn.Linear(encoder_dim, attention_dim, bias=False)  # Key transform\n",
                "        self.W_s = nn.Linear(decoder_dim, attention_dim, bias=False)  # Query transform\n",
                "        self.v = nn.Linear(attention_dim, 1, bias=False)              # Score projection\n",
                "    \n",
                "    def forward(self, decoder_state, encoder_outputs, mask=None):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            decoder_state: (batch, decoder_dim) - current decoder hidden state\n",
                "            encoder_outputs: (batch, src_len, encoder_dim) - all encoder states\n",
                "            mask: (batch, src_len) - True for positions to ignore (padding)\n",
                "        \n",
                "        Returns:\n",
                "            context: (batch, encoder_dim) - weighted sum of encoder outputs\n",
                "            weights: (batch, src_len) - attention distribution\n",
                "        \"\"\"\n",
                "        # Transform encoder outputs: (batch, src_len, attention_dim)\n",
                "        encoder_proj = self.W_h(encoder_outputs)\n",
                "        \n",
                "        # Transform decoder state: (batch, attention_dim) -> (batch, 1, attention_dim)\n",
                "        decoder_proj = self.W_s(decoder_state).unsqueeze(1)\n",
                "        \n",
                "        # Additive attention: (batch, src_len, attention_dim)\n",
                "        combined = torch.tanh(encoder_proj + decoder_proj)\n",
                "        \n",
                "        # Get scores: (batch, src_len)\n",
                "        scores = self.v(combined).squeeze(-1)\n",
                "        \n",
                "        # Apply mask (set padded positions to -inf)\n",
                "        if mask is not None:\n",
                "            scores = scores.masked_fill(mask, float('-inf'))\n",
                "        \n",
                "        # Normalize with softmax\n",
                "        weights = F.softmax(scores, dim=-1)\n",
                "        \n",
                "        # Compute context: (batch, encoder_dim)\n",
                "        context = torch.bmm(weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
                "        \n",
                "        return context, weights\n",
                "\n",
                "# Verification\n",
                "attention = BahdanauAttention(encoder_dim=128, decoder_dim=128)\n",
                "decoder_state = torch.randn(2, 128)\n",
                "encoder_outputs = torch.randn(2, 10, 128)\n",
                "\n",
                "context, weights = attention(decoder_state, encoder_outputs)\n",
                "print(f\"Context shape: {context.shape}\")\n",
                "print(f\"Weights shape: {weights.shape}\")\n",
                "print(f\"Weights sum: {weights.sum(dim=-1)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b61841a6",
            "metadata": {},
            "source": [
                "## 2. Bidirectional Encoder\n",
                "\n",
                "Why bidirectional? When we attend to position 3, we want context from:\n",
                "- What came BEFORE (positions 1, 2)\n",
                "- What comes AFTER (positions 4, 5, ...)\n",
                "\n",
                "This is crucial for understanding the full sentence context (Section 3.2 of the paper)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47744e6f",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Encoder(nn.Module):\n",
                "    \"\"\"Bidirectional GRU Encoder\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "        \n",
                "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
                "        self.gru = nn.GRU(\n",
                "            embed_size, hidden_size,\n",
                "            batch_first=True,\n",
                "            bidirectional=True\n",
                "        )\n",
                "        # Project bidirectional output back to hidden_size\n",
                "        self.projection = nn.Linear(hidden_size * 2, hidden_size)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(self, src, src_lengths):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            src: (batch, src_len) - source token IDs\n",
                "            src_lengths: (batch,) - actual lengths (for packing)\n",
                "        \n",
                "        Returns:\n",
                "            outputs: (batch, src_len, hidden_size) - encoder states\n",
                "            hidden: (1, batch, hidden_size) - final hidden state\n",
                "        \"\"\"\n",
                "        embedded = self.dropout(self.embedding(src))\n",
                "        \n",
                "        # Pack for efficiency with variable lengths\n",
                "        packed = pack_padded_sequence(\n",
                "            embedded, src_lengths.cpu(),\n",
                "            batch_first=True, enforce_sorted=False\n",
                "        )\n",
                "        \n",
                "        outputs, hidden = self.gru(packed)\n",
                "        \n",
                "        # Unpack\n",
                "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
                "        \n",
                "        # Project to hidden_size\n",
                "        outputs = self.projection(outputs)\n",
                "        \n",
                "        # Combine forward and backward hidden states\n",
                "        hidden = torch.cat([hidden[0], hidden[1]], dim=-1)\n",
                "        hidden = torch.tanh(self.projection(hidden)).unsqueeze(0)\n",
                "        \n",
                "        return outputs, hidden\n",
                "\n",
                "# Test encoder\n",
                "encoder = Encoder(vocab_size=50, embed_size=64, hidden_size=128)\n",
                "src = torch.randint(3, 50, (2, 8))\n",
                "src_lengths = torch.tensor([8, 5])\n",
                "\n",
                "outputs, hidden = encoder(src, src_lengths)\n",
                "print(f\"Encoder outputs: {outputs.shape}\")\n",
                "print(f\"Final hidden: {hidden.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32feca08",
            "metadata": {},
            "source": [
                "## 3. Attention Decoder\n",
                "\n",
                "At each decoding step:\n",
                "1. **Attend** - Compute attention over encoder outputs\n",
                "2. **Combine** - Merge context with current input\n",
                "3. **Update** - Run through GRU\n",
                "4. **Predict** - Generate next token"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "15bd7191",
            "metadata": {},
            "outputs": [],
            "source": [
                "class AttentionDecoder(nn.Module):\n",
                "    \"\"\"GRU Decoder with Bahdanau Attention\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size, embed_size, hidden_size, encoder_hidden_size, dropout=0.1):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
                "        self.attention = BahdanauAttention(encoder_hidden_size, hidden_size)\n",
                "        self.gru = nn.GRU(embed_size + encoder_hidden_size, hidden_size, batch_first=True)\n",
                "        self.output = nn.Linear(hidden_size + encoder_hidden_size + embed_size, vocab_size)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward_step(self, input_token, hidden, encoder_outputs, mask=None):\n",
                "        \"\"\"\n",
                "        One decoding step.\n",
                "        \n",
                "        Args:\n",
                "            input_token: (batch,) - previous token\n",
                "            hidden: (1, batch, hidden_size) - previous hidden state\n",
                "            encoder_outputs: (batch, src_len, encoder_hidden_size)\n",
                "            mask: (batch, src_len) - padding mask\n",
                "        \n",
                "        Returns:\n",
                "            output: (batch, vocab_size) - token probabilities\n",
                "            hidden: (1, batch, hidden_size) - new hidden state\n",
                "            attention: (batch, src_len) - attention weights\n",
                "        \"\"\"\n",
                "        embedded = self.dropout(self.embedding(input_token))\n",
                "        \n",
                "        # Compute attention\n",
                "        context, attention = self.attention(hidden.squeeze(0), encoder_outputs, mask)\n",
                "        \n",
                "        # Combine embedding and context\n",
                "        gru_input = torch.cat([embedded, context], dim=-1).unsqueeze(1)\n",
                "        \n",
                "        # GRU step\n",
                "        gru_output, hidden = self.gru(gru_input, hidden)\n",
                "        gru_output = gru_output.squeeze(1)\n",
                "        \n",
                "        # Predict next token\n",
                "        combined = torch.cat([gru_output, context, embedded], dim=-1)\n",
                "        output = self.output(combined)\n",
                "        \n",
                "        return output, hidden, attention\n",
                "    \n",
                "    def forward(self, trg, hidden, encoder_outputs, mask=None):\n",
                "        \"\"\"\n",
                "        Full decoding pass with teacher forcing.\n",
                "        \"\"\"\n",
                "        batch_size, trg_len = trg.shape\n",
                "        vocab_size = self.output.out_features\n",
                "        src_len = encoder_outputs.size(1)\n",
                "        \n",
                "        outputs = torch.zeros(batch_size, trg_len - 1, vocab_size, device=trg.device)\n",
                "        attentions = torch.zeros(batch_size, trg_len - 1, src_len, device=trg.device)\n",
                "        \n",
                "        input_token = trg[:, 0]  # SOS token\n",
                "        \n",
                "        for t in range(trg_len - 1):\n",
                "            output, hidden, attn = self.forward_step(input_token, hidden, encoder_outputs, mask)\n",
                "            outputs[:, t] = output\n",
                "            attentions[:, t] = attn\n",
                "            input_token = trg[:, t + 1]  # Teacher forcing\n",
                "        \n",
                "        return outputs, attentions\n",
                "\n",
                "# Test decoder\n",
                "decoder = AttentionDecoder(\n",
                "    vocab_size=50, embed_size=64, \n",
                "    hidden_size=128, encoder_hidden_size=128\n",
                ")\n",
                "\n",
                "trg = torch.randint(3, 50, (2, 6))\n",
                "trg[:, 0] = 1  # SOS token\n",
                "\n",
                "outputs, attentions = decoder(trg, hidden, outputs, mask=None)\n",
                "print(f\"Decoder outputs: {outputs.shape}\")\n",
                "print(f\"Attention matrices: {attentions.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c5c9dcb6",
            "metadata": {},
            "source": [
                "## 4. Complete Seq2Seq Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0472715",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Seq2SeqWithAttention(nn.Module):\n",
                "    \"\"\"Complete sequence-to-sequence model with attention.\"\"\"\n",
                "    \n",
                "    def __init__(self, src_vocab, trg_vocab, embed_size=64, hidden_size=128, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.encoder = Encoder(src_vocab, embed_size, hidden_size, dropout)\n",
                "        self.decoder = AttentionDecoder(trg_vocab, embed_size, hidden_size, hidden_size, dropout)\n",
                "        self.pad_idx = 0\n",
                "    \n",
                "    def forward(self, src, src_lengths, trg):\n",
                "        mask = (src == self.pad_idx)\n",
                "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
                "        outputs, attentions = self.decoder(trg, hidden, encoder_outputs, mask)\n",
                "        return outputs, attentions\n",
                "    \n",
                "    @torch.no_grad()\n",
                "    def translate(self, src, src_lengths, max_len=20, sos_idx=1, eos_idx=2):\n",
                "        \"\"\"Greedy decoding.\"\"\"\n",
                "        self.eval()\n",
                "        batch_size = src.size(0)\n",
                "        \n",
                "        mask = (src == self.pad_idx)\n",
                "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
                "        \n",
                "        input_token = torch.full((batch_size,), sos_idx, device=src.device)\n",
                "        translations = []\n",
                "        attentions = []\n",
                "        \n",
                "        for _ in range(max_len):\n",
                "            output, hidden, attn = self.decoder.forward_step(\n",
                "                input_token, hidden, encoder_outputs, mask\n",
                "            )\n",
                "            input_token = output.argmax(dim=-1)\n",
                "            translations.append(input_token)\n",
                "            attentions.append(attn)\n",
                "            \n",
                "            if (input_token == eos_idx).all():\n",
                "                break\n",
                "        \n",
                "        return torch.stack(translations, dim=1), torch.stack(attentions, dim=1)\n",
                "\n",
                "# Create model\n",
                "model = Seq2SeqWithAttention(src_vocab=50, trg_vocab=50).to(device)\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2d360769",
            "metadata": {},
            "source": [
                "## 5. Dataset: Sequence Reversal\n",
                "\n",
                "A toy task to verify our attention works:\n",
                "- Input: `[5, 3, 8, 2, 1]`\n",
                "- Output: `[1, 2, 8, 3, 5]`\n",
                "\n",
                "The attention should form a **reversed diagonal pattern**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed308e66",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ReversalDataset(Dataset):\n",
                "    \"\"\"\n",
                "    Generate (sequence, reversed_sequence) pairs.\n",
                "    \n",
                "    Tokens: 0=PAD, 1=SOS, 2=EOS, 3+=data\n",
                "    \"\"\"\n",
                "    def __init__(self, num_samples=5000, min_len=4, max_len=10, vocab_size=50):\n",
                "        self.samples = []\n",
                "        for _ in range(num_samples):\n",
                "            length = random.randint(min_len, max_len)\n",
                "            seq = [random.randint(3, vocab_size - 1) for _ in range(length)]\n",
                "            self.samples.append((seq, list(reversed(seq))))\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        src, trg = self.samples[idx]\n",
                "        trg = [1] + trg + [2]  # Add SOS and EOS\n",
                "        return torch.tensor(src), torch.tensor(trg)\n",
                "\n",
                "def collate_fn(batch):\n",
                "    srcs, trgs = zip(*batch)\n",
                "    src_lengths = torch.tensor([len(s) for s in srcs])\n",
                "    src_padded = nn.utils.rnn.pad_sequence(srcs, batch_first=True, padding_value=0)\n",
                "    trg_padded = nn.utils.rnn.pad_sequence(trgs, batch_first=True, padding_value=0)\n",
                "    return src_padded, src_lengths, trg_padded\n",
                "\n",
                "# Create datasets\n",
                "train_data = ReversalDataset(5000)\n",
                "val_data = ReversalDataset(500)\n",
                "\n",
                "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
                "val_loader = DataLoader(val_data, batch_size=32, collate_fn=collate_fn)\n",
                "\n",
                "# Show example\n",
                "src, trg = train_data[0]\n",
                "print(f\"Source:  {src.tolist()}\")\n",
                "print(f\"Target:  {trg.tolist()} (with SOS=1, EOS=2)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dd0c2640",
            "metadata": {},
            "source": [
                "## 6. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bcb9f296",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, criterion):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for src, src_lengths, trg in loader:\n",
                "        src, src_lengths, trg = src.to(device), src_lengths.to(device), trg.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs, _ = model(src, src_lengths, trg)\n",
                "        \n",
                "        # Reshape for loss\n",
                "        outputs = outputs.reshape(-1, outputs.size(-1))\n",
                "        trg = trg[:, 1:].reshape(-1)  # Skip SOS\n",
                "        \n",
                "        loss = criterion(outputs, trg)\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "def evaluate(model, loader, criterion):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for src, src_lengths, trg in loader:\n",
                "            src, src_lengths, trg = src.to(device), src_lengths.to(device), trg.to(device)\n",
                "            outputs, _ = model(src, src_lengths, trg)\n",
                "            outputs = outputs.reshape(-1, outputs.size(-1))\n",
                "            trg = trg[:, 1:].reshape(-1)\n",
                "            loss = criterion(outputs, trg)\n",
                "            total_loss += loss.item()\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "def accuracy(model, dataset, num_samples=200):\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    \n",
                "    for i in range(min(num_samples, len(dataset))):\n",
                "        src, trg = dataset[i]\n",
                "        src = src.unsqueeze(0).to(device)\n",
                "        src_len = torch.tensor([len(src[0])]).to(device)\n",
                "        \n",
                "        pred, _ = model.translate(src, src_len)\n",
                "        pred = pred[0].cpu().tolist()\n",
                "        if 2 in pred:\n",
                "            pred = pred[:pred.index(2)]\n",
                "        \n",
                "        target = trg[1:-1].tolist()\n",
                "        if pred == target:\n",
                "            correct += 1\n",
                "    \n",
                "    return correct / min(num_samples, len(dataset))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "66cf8b62",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "EPOCHS = 30\n",
                "history = {'train_loss': [], 'val_loss': [], 'accuracy': []}\n",
                "\n",
                "print(\"Training...\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "for epoch in range(1, EPOCHS + 1):\n",
                "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
                "    val_loss = evaluate(model, val_loader, criterion)\n",
                "    \n",
                "    history['train_loss'].append(train_loss)\n",
                "    history['val_loss'].append(val_loss)\n",
                "    \n",
                "    if epoch % 5 == 0:\n",
                "        acc = accuracy(model, val_data)\n",
                "        history['accuracy'].append(acc)\n",
                "        print(f\"Epoch {epoch:2d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | Acc: {acc:.1%}\")\n",
                "    else:\n",
                "        print(f\"Epoch {epoch:2d} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
                "\n",
                "print(\"=\" * 50)\n",
                "final_acc = accuracy(model, val_data, 500)\n",
                "print(f\"Final Accuracy: {final_acc:.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8066b6f5",
            "metadata": {},
            "source": [
                "## 7. Visualize Attention\n",
                "\n",
                "For the reversal task, the attention should form a **reversed diagonal pattern** - output position $i$ attends to input position $n - 1 - i$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1adbbf12",
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_attention(model, dataset, idx=0):\n",
                "    \"\"\"Visualize attention for a single example.\"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    src, trg = dataset[idx]\n",
                "    src_t = src.unsqueeze(0).to(device)\n",
                "    src_len = torch.tensor([len(src)]).to(device)\n",
                "    \n",
                "    pred, attentions = model.translate(src_t, src_len)\n",
                "    \n",
                "    # Process outputs\n",
                "    attn = attentions.squeeze(0).cpu().numpy()\n",
                "    pred_list = pred.squeeze(0).cpu().tolist()\n",
                "    \n",
                "    if 2 in pred_list:\n",
                "        eos_idx = pred_list.index(2)\n",
                "        pred_list = pred_list[:eos_idx]\n",
                "        attn = attn[:eos_idx]\n",
                "    \n",
                "    source = [str(t) for t in src.tolist()]\n",
                "    target = [str(t) for t in pred_list]\n",
                "    expected = [str(t) for t in trg[1:-1].tolist()]\n",
                "    \n",
                "    correct = pred_list == trg[1:-1].tolist()\n",
                "    \n",
                "    # Plot\n",
                "    fig, ax = plt.subplots(figsize=(10, 8))\n",
                "    \n",
                "    im = ax.imshow(attn, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
                "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
                "    \n",
                "    ax.set_xticks(range(len(source)))\n",
                "    ax.set_xticklabels(source, fontsize=12)\n",
                "    ax.set_yticks(range(len(target)))\n",
                "    ax.set_yticklabels(target, fontsize=12)\n",
                "    \n",
                "    ax.set_xlabel('Source (Input)', fontsize=13)\n",
                "    ax.set_ylabel('Target (Output)', fontsize=13)\n",
                "    \n",
                "    status = \"Correct\" if correct else \"[FAIL] Wrong\"\n",
                "    ax.set_title(f'Attention Pattern for Reversal Task\\n{status}', fontsize=14)\n",
                "    \n",
                "    # Add values\n",
                "    for i in range(len(target)):\n",
                "        for j in range(len(source)):\n",
                "            val = attn[i, j]\n",
                "            color = 'white' if val > 0.5 else 'black'\n",
                "            ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=10)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"Source:   {src.tolist()}\")\n",
                "    print(f\"Predicted: {pred_list}\")\n",
                "    print(f\"Expected:  {trg[1:-1].tolist()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4c4a33d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize a few examples\n",
                "for i in range(3):\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Example {i+1}\")\n",
                "    print('='*50)\n",
                "    visualize_attention(model, val_data, i)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8274c9ef",
            "metadata": {},
            "source": [
                "## 8. Key Takeaways\n",
                "\n",
                "### What We Learned\n",
                "\n",
                "1. **Attention solves the bottleneck** - No more cramming everything into one vector\n",
                "\n",
                "2. **It's differentiable** - End-to-end training with backpropagation\n",
                "\n",
                "3. **It's interpretable** - We can visualize what the model focuses on\n",
                "\n",
                "4. **The pattern tells the story** - Reversed diagonal = correct behavior\n",
                "\n",
                "### What's Next?\n",
                "\n",
                "- **Self-Attention** - Attend within the same sequence\n",
                "- **Transformers** - Replace RNNs entirely with attention\n",
                "\n",
                "---\n",
                "\n",
                "*Bahdanau attention is a direct ancestor of the Transformer (Vaswani et al. 2017). The core idea - letting the decoder dynamically attend to encoder states - carried forward into modern architectures, though the mechanism evolved from additive scoring over RNNs to scaled dot-product self-attention.*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
