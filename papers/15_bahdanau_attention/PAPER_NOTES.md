# Paper Notes: Neural Machine Translation by Jointly Learning to Align and Translate (ELI5)

> Making Attention simple enough for anyone to understand

**Authors:** Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio  
**Published:** ICLR 2015 (arXiv: September 2014)  
**Citations:** 40,000+ (one of the most cited ML papers ever)

---

## ðŸŽˆ The 5-Year-Old Explanation

**You:** "Why can't the old translation model remember long sentences?"

**Me:** "Imagine I read you a really long bedtime story, then close the book and ask you to tell me the whole story. You'd forget parts, right?"

**You:** "Yeah, especially the beginning!"

**Me:** "Exactly! The old model had to remember EVERYTHING at once. But attention is like having the book still openâ€”you can look back at any page whenever you need to!"

**You:** "So it cheats?"

**Me:** "No, it's smart! When translating 'cat', it looks back at where 'cat' was in the English sentence. When translating 'house', it looks at 'house'. It has a magic flashlight that shines on the right words!"

---

## ðŸ§  The Core Problem (No Math)

### The Telephone Game Problem

Picture playing telephone with 20 friends, but there's a twist:

```
Friend 1 (hears full story) â†’ Friend 2 â†’ Friend 3 â†’ ... â†’ Friend 20
    ðŸ“– "The cat sat on          ðŸ¤” "Cat...        ðŸ˜µ "Something
       the fluffy mat              mat...            about cats?"
       next to the dog"            dog?"
```

By friend #20, most details are lost! That's the **bottleneck problem**.

### The Attention Solution: Open Book Test

Now imagine Friend 20 can ASK questions and look back:

```
Friend 20: "Wait, what was the cat doing?"
           *looks back* â†’ "sitting on mat" âœ“

Friend 20: "And where was the dog?"
           *looks back* â†’ "next to the mat" âœ“
```

That's attention! Instead of memorizing everything, you **look up what you need, when you need it**.

---

## ðŸ”¦ The Three Key Players (Meet the Team!)

### 1. The Librarian (Encoder)

The encoder reads the entire input and creates a **card catalog**:

```
"The cat sat on the mat"

ðŸ“š Librarian creates index cards:
   Card 1: "The" + context (it's an article, starts sentence)
   Card 2: "cat" + context (noun, subject, furry animal)
   Card 3: "sat" + context (verb, past tense, action)
   Card 4: "on" + context (preposition, location coming)
   Card 5: "the" + context (another article)
   Card 6: "mat" + context (noun, object, floor thing)
```

**Key insight:** The librarian reads FORWARDS and BACKWARDS (bidirectional), so each card knows what comes before AND after!

### 2. The Translator with a Flashlight (Attention)

The translator is writing the French translation in a dark room, but has a **magic flashlight**:

```
Writing "Le"...    ðŸ”¦ shines on â†’ "The" (card 1)
Writing "chat"...  ðŸ”¦ shines on â†’ "cat" (card 2)  
Writing "assis"... ðŸ”¦ shines on â†’ "sat" (card 3)
Writing "sur"...   ðŸ”¦ shines on â†’ "on" (card 4)
Writing "le"...    ðŸ”¦ shines on â†’ "the" (card 5)
Writing "tapis"... ðŸ”¦ shines on â†’ "mat" (card 6)
```

The flashlight can shine on **multiple cards at once** (soft attention), but it's brighter on the most relevant ones!

### 3. The Writer (Decoder)

The writer produces one word at a time, using:
- What they just wrote (previous word)
- What the flashlight is showing (context)
- Their writing state (hidden state)

```
ðŸ–Šï¸ Writer's thought process:

Step 1: "Starting translation... flashlight shows 'The'... I'll write 'Le'"
Step 2: "'Le' written... flashlight shows 'cat'... I'll write 'chat'"
Step 3: "'chat' written... flashlight shows 'sat'... I'll write 'assis'"
...
```

---

## ðŸ“ One-Paragraph Summary

The paper introduces the **attention mechanism** for neural machine translation. Traditional encoder-decoder models compress the entire input sentence into a single fixed-length vector, which becomes a bottleneck for long sentences. The authors propose letting the decoder "attend" to different parts of the source sentence at each decoding step, creating a dynamic context vector. This simple idea dramatically improves translation quality, especially for long sentences, and the attention weights provide interpretable alignment between source and target words.

---

## ðŸŽ¯ Problem Statement (Technical)

### The Bottleneck Problem
Previous seq2seq models (Sutskever et al., Cho et al.) worked like this:

```
[The] [cat] [sat] [on] [the] [mat] 
              â†“
         Encoder RNN
              â†“
    [Single Fixed Vector c]  â† BOTTLENECK!
              â†“
         Decoder RNN
              â†“
[Le] [chat] [Ã©tait] [assis] [sur] [le] [tapis]
```

**The problem:** That single vector `c` must encode EVERYTHING about the source sentence. For long sentences, information gets lost.

### Evidence from Prior Work
- Performance degraded significantly for sentences > 20 words
- The fixed vector couldn't capture all nuances
- No way to focus on relevant parts during decoding

---

## ðŸ’¡ Key Innovation: Attention

### The Core Idea
Instead of compressing to one vector, **keep all encoder states** and let the decoder **choose which to focus on** at each step.

```
Encoder outputs: [hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„, hâ‚…, hâ‚†]
                   â†‘   â†‘   â†‘   â†‘   â†‘   â†‘
                   â””â”€â”€â”€â”´â”€â”€â”€â”¼â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
                           â†“
                    Attention weights
                     [0.1, 0.1, 0.6, 0.1, 0.05, 0.05]
                           â†“
                    Context vector câ‚ƒ
                           â†“
                    Decoder step 3
```

### The Alignment Model
For each decoder step `i` and encoder position `j`:

```
e_ij = a(s_{i-1}, h_j)                    # Alignment score
Î±_ij = exp(e_ij) / Î£_k exp(e_ik)          # Attention weight (softmax)
c_i = Î£_j Î±_ij Â· h_j                      # Context vector
```

Where `a()` is a learned alignment function:
```
a(s, h) = v^T Â· tanh(W_s Â· s + W_h Â· h)
```

This is called **additive attention** because we ADD the transformed vectors.

---

## ðŸ—ï¸ Architecture Details

### Encoder: Bidirectional RNN
```
Forward:  hâ‚â†’ = f(xâ‚, hâ‚€â†’)
Backward: hâ‚â† = f(xâ‚, hâ‚‚â†)
Combined: hâ‚ = [hâ‚â†’; hâ‚â†]
```

**Why bidirectional?** Each position gets context from BOTH directions. When attending to position 3, we know what came before AND after.

### Decoder: Attention-Augmented RNN
```python
for each output position i:
    # 1. Compute attention
    for j in encoder_positions:
        e_ij = alignment_model(s_{i-1}, h_j)
    Î±_i = softmax(e_i)
    c_i = sum(Î±_ij * h_j)
    
    # 2. Update decoder state
    s_i = GRU(s_{i-1}, [y_{i-1}; c_i])
    
    # 3. Predict next word
    y_i = softmax(W_o Â· [s_i; c_i])
```

### Key Design Choices
1. **GRU over LSTM** - Simpler, fewer parameters
2. **Additive attention** - More expressive than dot product
3. **Attention before GRU** - Context informs state update
4. **No separate alignment model** - Learned end-to-end

---

## ðŸ“Š Experimental Results

### Dataset: WMT'14 Englishâ†’French
- 348M words training data
- Vocabulary: 30,000 most frequent words

### Main Results

| Model | BLEU Score |
|-------|------------|
| Baseline RNN Enc-Dec | 26.75 |
| **RNNsearch-50** (attention) | **28.45** |
| RNNsearch-50* (large vocab) | **34.16** |
| Best phrase-based (Moses) | 33.30 |

**+7.4 BLEU improvement** with attention!

### Performance on Long Sentences

This is the killer result:

```
Sentence Length | Baseline | With Attention
----------------|----------|---------------
10-20 words     |   25     |      28
20-30 words     |   20     |      26
30-40 words     |   15     |      25
40-50 words     |   10     |      24
50+ words       |    5     |      22
```

Attention maintains quality even as sentences get longer!

---

## ðŸ” Qualitative Analysis

### Attention Visualization
The paper shows attention matrices that reveal soft alignments:

```
English: The agreement on the European Economic Area was signed in August 1992.
French:  L' accord sur la zone Ã©conomique europÃ©enne a Ã©tÃ© signÃ© en aoÃ»t 1992.

Attention shows:
- "agreement" â†’ "accord"
- "European Economic Area" â†’ "zone Ã©conomique europÃ©enne"
- "August 1992" â†’ "aoÃ»t 1992"
```

The model learns to:
- Handle word reordering (adjective placement differs)
- Align multi-word phrases
- Deal with different sentence structures

### Discovered Alignments
Without explicit alignment labels, the model discovers:
- One-to-one alignments (most words)
- One-to-many (e.g., "the" â†’ "l'/le/la")
- Many-to-one (e.g., "did not" â†’ "n'a pas")

---

## ï¿½ Step-by-Step Example: Translating "The cat sat on the mat"

Let's watch attention in action!

### Setup
```
Source: "The cat sat on the mat"
Target: "Le chat Ã©tait assis sur le tapis"
```

### Step 1: Generating "Le"
```
ðŸ§  Decoder asks: "What should I write first?"
ðŸ”¦ Flashlight scans all source words...

Attention weights:
  "The" â†’ 0.85 â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬œâ¬œ  (BRIGHT!)
  "cat" â†’ 0.05 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "sat" â†’ 0.03 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "on"  â†’ 0.02 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "the" â†’ 0.03 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "mat" â†’ 0.02 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ

ðŸ“ Context vector mostly = "The"
âœï¸ Output: "Le" âœ“
```

### Step 2: Generating "chat"
```
ðŸ§  Decoder: "Just wrote 'Le', what's next?"
ðŸ”¦ Flashlight moves...

Attention weights:
  "The" â†’ 0.08 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "cat" â†’ 0.82 â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬œâ¬œ  (BRIGHT!)
  "sat" â†’ 0.04 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "on"  â†’ 0.02 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "the" â†’ 0.02 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "mat" â†’ 0.02 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ

ðŸ“ Context vector mostly = "cat"
âœï¸ Output: "chat" âœ“
```

### Step 3: Generating "Ã©tait assis" (was sitting)
```
ðŸ§  Decoder: "Translating the verb..."
ðŸ”¦ Flashlight focuses on action...

Attention weights:
  "The" â†’ 0.02 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "cat" â†’ 0.10 â¬›â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "sat" â†’ 0.78 â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬œâ¬œ  (BRIGHT!)
  "on"  â†’ 0.05 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "the" â†’ 0.02 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
  "mat" â†’ 0.03 â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ

ðŸ“ Context vector mostly = "sat" (with a bit of "cat" for subject)
âœï¸ Output: "Ã©tait assis" âœ“
```

**The magic:** The flashlight automatically learned to shine on the right words!

---

## ï¿½ðŸŽ“ Key Insights from the Paper

### 1. Soft vs Hard Attention
The paper uses **soft attention** (differentiable weighted sum).
- Pros: End-to-end trainable with backprop
- Cons: Must compute all alignments

Alternative: Hard attention (sample one position)
- Pros: Computationally cheaper
- Cons: Requires reinforcement learning

### 2. Why "Jointly Learning to Align and Translate"
Previous systems had separate alignment models (IBM Models).
Here, alignment (attention) is learned JOINTLY with translation.
The model discovers alignments that help translation, not linguistically "correct" ones.

### 3. The Annotation Vector
Each encoder state `h_j` is an "annotation" of word `x_j` with context.
Bidirectional encoding means each annotation summarizes the whole sentence focused on that position.

---

## ðŸ’­ Critical Analysis

### Strengths
1. **Elegant solution** to a real problem (bottleneck)
2. **Interpretable** - attention weights show alignment
3. **Strong empirical results** - significant BLEU gains
4. **Generalizable** - attention used everywhere now
5. **End-to-end** - no separate alignment step

### Limitations
1. **Quadratic complexity** - O(nÃ—m) for all attention scores
2. **Sequential decoding** - can't parallelize decoder
3. **Still uses RNNs** - slow to train
4. **Soft attention only** - computes all positions even when few matter

### What Came Next
- **Luong Attention (2015)** - Simpler dot-product scoring
- **Self-Attention (2017)** - Attend within same sequence
- **Transformer (2017)** - Remove RNNs entirely, use only attention
- **BERT, GPT (2018+)** - Pretrained attention-based models

---

## ðŸ“Œ Memorable Quotes

> "The use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture."

> "Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated."

> "The proposed approach provides an intuitive way to inspect the (soft-)alignment between the words in a generated translation and those in a source sentence."

---

## ðŸ”— Connections to Other Papers

### Builds On
- **Sutskever et al. (2014)** - Sequence to Sequence Learning
- **Cho et al. (2014)** - Learning Phrase Representations (GRU)

### Influenced
- **Luong et al. (2015)** - Effective Approaches to Attention-based NMT
- **Vaswani et al. (2017)** - Attention Is All You Need (Transformer)
- **Xu et al. (2015)** - Show, Attend and Tell (image captioning)

### Key Differences from Transformer
| Aspect | Bahdanau | Transformer |
|--------|----------|-------------|
| Attention type | Encoder-decoder only | Self + cross |
| Base architecture | RNN | None (pure attention) |
| Parallelization | Sequential | Fully parallel |
| Positions | Implicit in RNN | Positional encodings |

---

## âœ… Implementation Checklist

When implementing this paper:

- [ ] Bidirectional encoder (GRU or LSTM)
- [ ] Additive attention with learnable weights
- [ ] Proper masking for padding
- [ ] Context vector concatenated with input
- [ ] Teacher forcing during training
- [ ] Greedy or beam search for inference
- [ ] Attention visualization

---

## ðŸ“š Further Reading

1. **Original Paper**: [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)
2. **Luong Attention**: [arXiv:1508.04025](https://arxiv.org/abs/1508.04025)
3. **Transformer**: [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
4. **Illustrated Guide**: [jalammar.github.io/visualizing-neural-machine-translation](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

---

*This paper is foundational. Understanding Bahdanau attention is essential
for understanding modern NLP, from Transformers to GPT to BERT.*
