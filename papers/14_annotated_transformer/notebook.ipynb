{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 14: The Annotated Transformer\n",
                "\n",
                "> **Interactive notebook for understanding the Transformer architecture**\n",
                "\n",
                "This notebook walks through:\n",
                "1. Attention mechanism visualization\n",
                "2. Multi-head attention\n",
                "3. Positional encoding\n",
                "4. Full model training on copy task\n",
                "5. Inference and decoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import copy\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "\n",
                "%matplotlib inline\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Scaled Dot-Product Attention\n",
                "\n",
                "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def attention(query, key, value, mask=None, dropout=None):\n",
                "    \"\"\"Scaled Dot-Product Attention\"\"\"\n",
                "    d_k = query.size(-1)\n",
                "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
                "    if mask is not None:\n",
                "        scores = scores.masked_fill(mask == 0, -1e9)\n",
                "    p_attn = scores.softmax(dim=-1)\n",
                "    if dropout is not None:\n",
                "        p_attn = dropout(p_attn)\n",
                "    return torch.matmul(p_attn, value), p_attn\n",
                "\n",
                "# Demo\n",
                "torch.manual_seed(42)\n",
                "Q = torch.randn(1, 4, 8)  # (batch, seq, d_k)\n",
                "K = torch.randn(1, 4, 8)\n",
                "V = torch.randn(1, 4, 8)\n",
                "\n",
                "output, attn_weights = attention(Q, K, V)\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Attention weights shape: {attn_weights.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attention weights\n",
                "plt.figure(figsize=(6, 5))\n",
                "sns.heatmap(attn_weights[0].detach().numpy(), annot=True, fmt='.2f', cmap='Blues')\n",
                "plt.xlabel('Key Position')\n",
                "plt.ylabel('Query Position')\n",
                "plt.title('Attention Weights')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Causal (Subsequent) Mask\n",
                "\n",
                "For autoregressive decoding, position $i$ can only attend to positions $\\leq i$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def subsequent_mask(size):\n",
                "    \"\"\"Mask out subsequent positions.\"\"\"\n",
                "    mask = torch.triu(torch.ones(1, size, size), diagonal=1) == 0\n",
                "    return mask\n",
                "\n",
                "mask = subsequent_mask(6)\n",
                "plt.figure(figsize=(5, 4))\n",
                "sns.heatmap(mask[0].int().numpy(), annot=True, cmap='Greens', cbar=False)\n",
                "plt.xlabel('Key Position')\n",
                "plt.ylabel('Query Position')\n",
                "plt.title('Causal Mask (1 = can attend)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Attention with mask\n",
                "output_masked, attn_masked = attention(Q, K, V, mask=subsequent_mask(4))\n",
                "\n",
                "plt.figure(figsize=(6, 5))\n",
                "sns.heatmap(attn_masked[0].detach().numpy(), annot=True, fmt='.2f', cmap='Blues')\n",
                "plt.xlabel('Key Position')\n",
                "plt.ylabel('Query Position')\n",
                "plt.title('Masked Attention Weights')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Positional Encoding\n",
                "\n",
                "Since attention has no notion of position, we add positional information:\n",
                "\n",
                "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
                "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def positional_encoding(max_len, d_model):\n",
                "    pe = torch.zeros(max_len, d_model)\n",
                "    position = torch.arange(0, max_len).unsqueeze(1).float()\n",
                "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
                "    pe[:, 0::2] = torch.sin(position * div_term)\n",
                "    pe[:, 1::2] = torch.cos(position * div_term)\n",
                "    return pe\n",
                "\n",
                "pe = positional_encoding(100, 64)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Heatmap\n",
                "ax = axes[0]\n",
                "im = ax.imshow(pe.T, aspect='auto', cmap='RdBu')\n",
                "ax.set_xlabel('Position')\n",
                "ax.set_ylabel('Dimension')\n",
                "ax.set_title('Positional Encoding')\n",
                "plt.colorbar(im, ax=ax)\n",
                "\n",
                "# Line plot\n",
                "ax = axes[1]\n",
                "for dim in [0, 1, 2, 3, 10, 20]:\n",
                "    ax.plot(pe[:, dim].numpy(), label=f'dim {dim}')\n",
                "ax.set_xlabel('Position')\n",
                "ax.set_ylabel('Value')\n",
                "ax.set_title('PE Values by Dimension')\n",
                "ax.legend()\n",
                "ax.grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Noam Learning Rate Schedule"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rate(step, d_model, warmup):\n",
                "    if step == 0:\n",
                "        step = 1\n",
                "    return d_model ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
                "\n",
                "steps = list(range(20000))\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "for warmup in [400, 2000, 4000, 8000]:\n",
                "    rates = [rate(s, 512, warmup) for s in steps]\n",
                "    plt.plot(steps, rates, label=f'warmup={warmup}')\n",
                "\n",
                "plt.xlabel('Training Step')\n",
                "plt.ylabel('Learning Rate')\n",
                "plt.title('Noam Learning Rate Schedule (d_model=512)')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Label Smoothing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def label_smoothing_demo(vocab_size=10, true_class=3, smoothing=0.1):\n",
                "    dist = np.ones(vocab_size) * (smoothing / (vocab_size - 1))\n",
                "    dist[true_class] = 1.0 - smoothing\n",
                "    return dist\n",
                "\n",
                "fig, axes = plt.subplots(1, 4, figsize=(14, 4))\n",
                "for ax, smooth in zip(axes, [0.0, 0.1, 0.2, 0.3]):\n",
                "    dist = label_smoothing_demo(smoothing=smooth)\n",
                "    colors = ['steelblue'] * 10\n",
                "    colors[3] = 'coral'\n",
                "    ax.bar(range(10), dist, color=colors)\n",
                "    ax.set_xlabel('Token Index')\n",
                "    ax.set_ylabel('Probability')\n",
                "    ax.set_title(f'Smoothing = {smooth}')\n",
                "    ax.set_ylim(0, 1.1)\n",
                "\n",
                "plt.suptitle('Label Smoothing Effect', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Demo: Copy Task\n",
                "\n",
                "Train a small Transformer to copy input sequences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import full implementation\n",
                "from implementation import make_model, Batch, subsequent_mask, greedy_decode\n",
                "\n",
                "# Create small model\n",
                "VOCAB = 11\n",
                "model = make_model(VOCAB, VOCAB, N=2, d_model=64, d_ff=128, h=2)\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "losses = []\n",
                "model.train()\n",
                "\n",
                "for epoch in range(20):\n",
                "    epoch_loss = 0\n",
                "    for _ in range(20):\n",
                "        # Generate batch\n",
                "        data = torch.randint(1, VOCAB, (32, 10))\n",
                "        data[:, 0] = 1\n",
                "        batch = Batch(data.clone(), data.clone())\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        out = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
                "        loss = criterion(out.reshape(-1, VOCAB), batch.tgt_y.reshape(-1))\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        epoch_loss += loss.item()\n",
                "    \n",
                "    losses.append(epoch_loss / 20)\n",
                "    if (epoch + 1) % 5 == 0:\n",
                "        print(f\"Epoch {epoch + 1}: Loss = {losses[-1]:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training curve\n",
                "plt.figure(figsize=(8, 4))\n",
                "plt.plot(losses, marker='o')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Training Loss')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test\n",
                "model.eval()\n",
                "test_src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
                "test_mask = torch.ones(1, 1, 10)\n",
                "\n",
                "with torch.no_grad():\n",
                "    output = greedy_decode(model, test_src, test_mask, max_len=10, start_symbol=1)\n",
                "\n",
                "print(f\"Input:  {test_src[0].tolist()}\")\n",
                "print(f\"Output: {output[0].tolist()}\")\n",
                "\n",
                "match = (test_src[0] == output[0][:10]).sum().item()\n",
                "print(f\"Match:  {match}/10 ({100*match/10:.0f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Visualize Learned Attention\n",
                "\n",
                "Let's see what the trained model is attending to."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get attention from encoder\n",
                "model.eval()\n",
                "_ = model.encode(test_src, test_mask)\n",
                "\n",
                "# Plot attention from first encoder layer\n",
                "enc_attn = model.encoder.layers[0].self_attn.attn[0]  # (heads, seq, seq)\n",
                "\n",
                "fig, axes = plt.subplots(1, enc_attn.size(0), figsize=(12, 4))\n",
                "for head in range(enc_attn.size(0)):\n",
                "    ax = axes[head]\n",
                "    sns.heatmap(enc_attn[head].detach().numpy(), ax=ax, cmap='Blues', cbar=False,\n",
                "                xticklabels=test_src[0].tolist(), yticklabels=test_src[0].tolist())\n",
                "    ax.set_title(f'Head {head}')\n",
                "\n",
                "plt.suptitle('Encoder Self-Attention (Layer 0)', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "In this notebook we:\n",
                "1. ✅ Implemented and visualized **scaled dot-product attention**\n",
                "2. ✅ Created and understood **causal masking**\n",
                "3. ✅ Visualized **positional encoding** patterns\n",
                "4. ✅ Explored the **Noam learning rate schedule**\n",
                "5. ✅ Understood **label smoothing**\n",
                "6. ✅ Trained a Transformer on the **copy task**\n",
                "7. ✅ Visualized **learned attention patterns**\n",
                "\n",
                "**Next:** Try the exercises in `exercises/` or explore the full implementation in `implementation.py`!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}