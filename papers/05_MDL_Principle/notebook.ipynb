{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24eded48",
   "metadata": {},
   "source": [
    "# Day 5: The MDL Principle - Interactive Notebook\n",
    "\n",
    "**Paper:** \"A Tutorial Introduction to the Minimum Description Length Principle\" - Grünwald (2004)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **The Core Idea:** The best model is the one that gives the shortest total description of the data\n",
    "2. **Two-Part Codes:** Model description + data-given-model description\n",
    "3. **Prequential MDL:** Sequential prediction coding\n",
    "4. **MDL vs AIC vs BIC:** How they compare on polynomial model selection\n",
    "5. **Practical Application:** Polynomial degree selection (the paper's running example)\n",
    "\n",
    "---\n",
    "\n",
    "## The Coding Framework (Section 3 of the paper)\n",
    "\n",
    "Imagine you need to transmit a sequence of temperature readings to a receiver:\n",
    "\n",
    "- **Naive approach:** Send each number individually (expensive!)\n",
    "- **Smart approach:** Send a formula + small corrections (cheap!)\n",
    "\n",
    "The **MDL Principle** says: Find the hypothesis that minimizes total description length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"[OK] Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e4c2c",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Two-Part Codes\n",
    "\n",
    "The fundamental MDL equation (Section 3):\n",
    "\n",
    "$$L_{\\text{total}} = L(H) + L(D|H)$$\n",
    "\n",
    "Where:\n",
    "- $L(H)$ = bits to describe the **model** (hypothesis)\n",
    "- $L(D|H)$ = bits to describe the **data given the model** (residuals)\n",
    "\n",
    "The best model minimizes this sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal code for integers (fair encoding with unknown range)\n",
    "def universal_code_length(n: int) -> float:\n",
    "    \"\"\"Bits needed to encode positive integer n.\"\"\"\n",
    "    if n <= 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    c0 = 2.865064  # Normalization constant\n",
    "    length = np.log2(c0)\n",
    "    current = float(n)\n",
    "    \n",
    "    while current > 1:\n",
    "        length += np.log2(current)\n",
    "        current = np.log2(current)\n",
    "        if current <= 0:\n",
    "            break\n",
    "    \n",
    "    return max(length, 0)\n",
    "\n",
    "# Test it\n",
    "for n in [1, 2, 10, 100, 1000]:\n",
    "    print(f\"Encoding integer {n}: {universal_code_length(n):.2f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_part_mdl(x, y, degree, precision=32):\n",
    "    \"\"\"\n",
    "    Compute Two-Part MDL for polynomial regression.\n",
    "    \n",
    "    Returns: (total_mdl, model_cost, data_cost)\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # Fit polynomial\n",
    "    coeffs = np.polyfit(x, y, degree)\n",
    "    poly = np.poly1d(coeffs)\n",
    "    residuals = y - poly(x)\n",
    "    \n",
    "    # === PART 1: Model Cost L(H) ===\n",
    "    # Cost to specify degree\n",
    "    degree_cost = universal_code_length(degree + 1)\n",
    "    # Cost to specify coefficients\n",
    "    coeff_cost = (degree + 1) * (precision + 1)  # +1 for sign\n",
    "    model_cost = degree_cost + coeff_cost\n",
    "    \n",
    "    # === PART 2: Data Cost L(D|H) ===\n",
    "    # Encode residuals using Gaussian model\n",
    "    sigma = np.std(residuals) if np.std(residuals) > 1e-10 else 1e-10\n",
    "    \n",
    "    # Negative log-likelihood in bits\n",
    "    nll = 0.5 * n * np.log2(2 * np.pi * sigma**2) + \\\n",
    "          np.sum(residuals**2) / (2 * sigma**2 * np.log(2))\n",
    "    \n",
    "    # Add cost to encode sigma\n",
    "    sigma_cost = precision + 10\n",
    "    data_cost = nll + sigma_cost\n",
    "    \n",
    "    return model_cost + data_cost, model_cost, data_cost\n",
    "\n",
    "print(\"[OK] MDL function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f45e5f",
   "metadata": {},
   "source": [
    "## Part 2: Generate Synthetic Data\n",
    "\n",
    "Let's create polynomial data with known structure to test MDL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with TRUE degree = 2 (parabola)\n",
    "TRUE_DEGREE = 2\n",
    "N_SAMPLES = 50\n",
    "NOISE_STD = 2.0\n",
    "\n",
    "x = np.linspace(0, 10, N_SAMPLES)\n",
    "\n",
    "# True polynomial: y = 3 + 2x - 0.5x²\n",
    "true_coeffs = [-0.5, 2, 3]  # [x², x, constant]\n",
    "y_true = 3 + 2*x - 0.5*x**2\n",
    "y = y_true + np.random.randn(N_SAMPLES) * NOISE_STD\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(x, y, c='gray', alpha=0.7, label='Noisy data')\n",
    "plt.plot(x, y_true, 'g-', linewidth=2, label=f'True (degree {TRUE_DEGREE})')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Synthetic Data: True Degree = {TRUE_DEGREE}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {N_SAMPLES} points with noise std = {NOISE_STD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94e13d",
   "metadata": {},
   "source": [
    "## Part 3: MDL Model Selection\n",
    "\n",
    "Now let's compute MDL scores for different polynomial degrees and see if it recovers the true degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad96d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MDL for degrees 0 through 10\n",
    "max_degree = 10\n",
    "degrees = list(range(max_degree + 1))\n",
    "\n",
    "mdl_scores = {}\n",
    "model_costs = {}\n",
    "data_costs = {}\n",
    "\n",
    "print(\"MDL Scores by Degree:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Degree':<8} {'Model L(H)':<15} {'Data L(D|H)':<15} {'Total':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for deg in degrees:\n",
    "    total, model, data = two_part_mdl(x, y, deg)\n",
    "    mdl_scores[deg] = total\n",
    "    model_costs[deg] = model\n",
    "    data_costs[deg] = data\n",
    "    \n",
    "    marker = \" <-- BEST\" if deg == min(mdl_scores, key=mdl_scores.get) else \"\"\n",
    "    print(f\"{deg:<8} {model:<15.1f} {data:<15.1f} {total:<15.1f}{marker}\")\n",
    "\n",
    "best_degree = min(mdl_scores, key=mdl_scores.get)\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nMDL selects: Degree {best_degree}\")\n",
    "print(f\"True degree: {TRUE_DEGREE}\")\n",
    "print(f\"{'[OK] Correct!' if best_degree == TRUE_DEGREE else '[FAIL] Incorrect'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the MDL breakdown\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.array(degrees)\n",
    "model_vals = [model_costs[d] for d in degrees]\n",
    "data_vals = [data_costs[d] for d in degrees]\n",
    "total_vals = [mdl_scores[d] for d in degrees]\n",
    "\n",
    "# Stacked bars\n",
    "ax.bar(x_pos, model_vals, label='Model Cost L(H)', color='#E94F37', alpha=0.8)\n",
    "ax.bar(x_pos, data_vals, bottom=model_vals, label='Data Cost L(D|H)', \n",
    "       color='#44AF69', alpha=0.8)\n",
    "\n",
    "# Total line\n",
    "ax.plot(x_pos, total_vals, 'o-', color='purple', linewidth=2.5, \n",
    "        markersize=8, label='Total MDL')\n",
    "\n",
    "# Mark best and true\n",
    "ax.axvline(x=best_degree, color='purple', linestyle='--', linewidth=2, \n",
    "           alpha=0.7, label=f'Best (Degree {best_degree})')\n",
    "ax.axvline(x=TRUE_DEGREE, color='green', linestyle=':', linewidth=2,\n",
    "           alpha=0.7, label=f'True (Degree {TRUE_DEGREE})')\n",
    "\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
    "ax.set_ylabel('Code Length (bits)', fontsize=12)\n",
    "ax.set_title('MDL Score Breakdown: Model Cost vs Data Cost', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xticks(degrees)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12b2d7",
   "metadata": {},
   "source": [
    "## Part 4: Visual Comparison of Fits\n",
    "\n",
    "Let's see what different polynomial degrees look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df707faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fits: Underfit, Optimal, Overfit\n",
    "degrees_to_show = [1, 2, 5, 9]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "x_fine = np.linspace(x.min(), x.max(), 200)\n",
    "\n",
    "for ax, deg in zip(axes, degrees_to_show):\n",
    "    # Plot data\n",
    "    ax.scatter(x, y, c='gray', alpha=0.6, s=30)\n",
    "    \n",
    "    # Fit polynomial\n",
    "    coeffs = np.polyfit(x, y, deg)\n",
    "    y_fit = np.poly1d(coeffs)(x_fine)\n",
    "    \n",
    "    # Color based on optimality\n",
    "    if deg == best_degree:\n",
    "        color = 'green'\n",
    "        status = '[ok] OPTIMAL'\n",
    "    elif deg < best_degree:\n",
    "        color = 'orange'\n",
    "        status = 'Underfit'\n",
    "    else:\n",
    "        color = 'red'\n",
    "        status = 'Overfit'\n",
    "    \n",
    "    ax.plot(x_fine, y_fit, color=color, linewidth=2.5)\n",
    "    ax.set_title(f'Degree {deg}\\n{status}\\nMDL: {mdl_scores[deg]:.0f} bits')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "plt.suptitle('Polynomial Fits: From Underfit to Overfit', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d846f4",
   "metadata": {},
   "source": [
    "## Part 5: MDL vs AIC vs BIC\n",
    "\n",
    "Comparing the three most popular model selection criteria (see Grünwald Section 8):\n",
    "\n",
    "| Criterion | Formula | Penalty |\n",
    "|-----------|---------|---------|\n",
    "| MDL | $L(H) + L(D\\|H)$ | Information-theoretic |\n",
    "| AIC | $2k - 2\\ln(L)$ | Fixed: 2 per parameter |\n",
    "| BIC | $k\\ln(n) - 2\\ln(L)$ | Grows with data size |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722f9288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aic(x, y, degree):\n",
    "    \"\"\"Akaike Information Criterion.\"\"\"\n",
    "    n = len(x)\n",
    "    k = degree + 2  # coefficients + variance\n",
    "    \n",
    "    coeffs = np.polyfit(x, y, degree)\n",
    "    residuals = y - np.poly1d(coeffs)(x)\n",
    "    sigma_ml = np.sqrt(np.sum(residuals**2) / n)\n",
    "    \n",
    "    log_lik = -0.5 * n * (np.log(2 * np.pi * sigma_ml**2) + 1)\n",
    "    return 2 * k - 2 * log_lik\n",
    "\n",
    "def bic(x, y, degree):\n",
    "    \"\"\"Bayesian Information Criterion.\"\"\"\n",
    "    n = len(x)\n",
    "    k = degree + 2\n",
    "    \n",
    "    coeffs = np.polyfit(x, y, degree)\n",
    "    residuals = y - np.poly1d(coeffs)(x)\n",
    "    sigma_ml = np.sqrt(np.sum(residuals**2) / n)\n",
    "    \n",
    "    log_lik = -0.5 * n * (np.log(2 * np.pi * sigma_ml**2) + 1)\n",
    "    return k * np.log(n) - 2 * log_lik\n",
    "\n",
    "# Compute all scores\n",
    "aic_scores = {d: aic(x, y, d) for d in degrees}\n",
    "bic_scores = {d: bic(x, y, d) for d in degrees}\n",
    "\n",
    "# Find best for each\n",
    "mdl_best = min(mdl_scores, key=mdl_scores.get)\n",
    "aic_best = min(aic_scores, key=aic_scores.get)\n",
    "bic_best = min(bic_scores, key=bic_scores.get)\n",
    "\n",
    "print(\"MODEL SELECTION COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"True degree:  {TRUE_DEGREE}\")\n",
    "print(f\"MDL selects:  {mdl_best} {'[ok]' if mdl_best == TRUE_DEGREE else '[FAIL]'}\")\n",
    "print(f\"AIC selects:  {aic_best} {'[ok]' if aic_best == TRUE_DEGREE else '[FAIL]'}\")\n",
    "print(f\"BIC selects:  {bic_best} {'[ok]' if bic_best == TRUE_DEGREE else '[FAIL]'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748fdd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "methods = [\n",
    "    ('MDL', mdl_scores, mdl_best, '#2E86AB'),\n",
    "    ('AIC', aic_scores, aic_best, '#A23B72'),\n",
    "    ('BIC', bic_scores, bic_best, '#F18F01')\n",
    "]\n",
    "\n",
    "for ax, (name, scores, best, color) in zip(axes, methods):\n",
    "    vals = [scores[d] for d in degrees]\n",
    "    \n",
    "    ax.bar(degrees, vals, color=color, alpha=0.7)\n",
    "    ax.axvline(x=best, color='black', linestyle='--', linewidth=2,\n",
    "               label=f'Selected: {best}')\n",
    "    ax.axvline(x=TRUE_DEGREE, color='green', linestyle=':', linewidth=2,\n",
    "               label=f'True: {TRUE_DEGREE}')\n",
    "    \n",
    "    ax.set_xlabel('Polynomial Degree')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(f'{name}: Selects Degree {best}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('MDL vs AIC vs BIC: Who Wins?', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f6746",
   "metadata": {},
   "source": [
    "## Part 6: Compression and Model Quality\n",
    "\n",
    "The key insight from Section 3: **A model that achieves shorter total description length has captured more of the data's regularity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e35198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression analysis\n",
    "raw_bits = N_SAMPLES * 32  # 32-bit floats\n",
    "best_mdl = mdl_scores[best_degree]\n",
    "compression_ratio = raw_bits / best_mdl\n",
    "\n",
    "print(\"COMPRESSION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Raw data:        {raw_bits} bits ({N_SAMPLES} x 32-bit)\")\n",
    "print(f\"MDL compressed:  {best_mdl:.0f} bits\")\n",
    "print(f\"Compression:     {compression_ratio:.2f}x\")\n",
    "print()\n",
    "if compression_ratio > 1:\n",
    "    print(\"[OK] Data has compressible structure.\")\n",
    "    print(\"   The model found a pattern that reduces description length.\")\n",
    "else:\n",
    "    print(\"[NOTE] Data might be random (no compressible structure).\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "compression_ratios = [raw_bits / mdl_scores[d] for d in degrees]\n",
    "colors = ['green' if d == best_degree else '#2E86AB' for d in degrees]\n",
    "\n",
    "ax.bar(degrees, compression_ratios, color=colors, alpha=0.8)\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='No compression')\n",
    "ax.set_xlabel('Polynomial Degree')\n",
    "ax.set_ylabel('Compression Ratio')\n",
    "ax.set_title('Compression Ratio by Model Complexity\\n(Higher = Better)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1464e",
   "metadata": {},
   "source": [
    "## Part 7: Monte Carlo - Testing Robustness\n",
    "\n",
    "Let's run many experiments to see how reliably MDL finds the true degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 100 experiments\n",
    "N_TRIALS = 100\n",
    "\n",
    "mdl_correct = 0\n",
    "aic_correct = 0\n",
    "bic_correct = 0\n",
    "\n",
    "for trial in range(N_TRIALS):\n",
    "    # Generate new data\n",
    "    np.random.seed(trial)\n",
    "    y_trial = y_true + np.random.randn(N_SAMPLES) * NOISE_STD\n",
    "    \n",
    "    # Compute scores\n",
    "    trial_mdl = {d: two_part_mdl(x, y_trial, d)[0] for d in degrees}\n",
    "    trial_aic = {d: aic(x, y_trial, d) for d in degrees}\n",
    "    trial_bic = {d: bic(x, y_trial, d) for d in degrees}\n",
    "    \n",
    "    # Check if correct\n",
    "    mdl_correct += (min(trial_mdl, key=trial_mdl.get) == TRUE_DEGREE)\n",
    "    aic_correct += (min(trial_aic, key=trial_aic.get) == TRUE_DEGREE)\n",
    "    bic_correct += (min(trial_bic, key=trial_bic.get) == TRUE_DEGREE)\n",
    "\n",
    "print(f\"MONTE CARLO RESULTS ({N_TRIALS} trials)\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"MDL accuracy: {mdl_correct}/{N_TRIALS} ({100*mdl_correct/N_TRIALS:.1f}%)\")\n",
    "print(f\"AIC accuracy: {aic_correct}/{N_TRIALS} ({100*aic_correct/N_TRIALS:.1f}%)\")\n",
    "print(f\"BIC accuracy: {bic_correct}/{N_TRIALS} ({100*bic_correct/N_TRIALS:.1f}%)\")\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "methods = ['MDL', 'AIC', 'BIC']\n",
    "accuracies = [mdl_correct, aic_correct, bic_correct]\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "\n",
    "bars = ax.bar(methods, accuracies, color=colors, alpha=0.8)\n",
    "ax.set_ylabel('Correct Selections')\n",
    "ax.set_title(f'Model Selection Accuracy ({N_TRIALS} trials)')\n",
    "ax.set_ylim(0, N_TRIALS)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{100*acc/N_TRIALS:.1f}%', ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92655f6",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 1. The MDL Principle\n",
    "> The best hypothesis is the one that yields the shortest total description of the data (Grünwald, Section 1).\n",
    "\n",
    "### 2. Two-Part Code\n",
    "> Total cost = Model description + Data-given-model description  \n",
    "> $L(H) + L(D|H)$\n",
    "\n",
    "### 3. Trade-off\n",
    "- **Simple model**: Cheap to describe, but poor fit (high residual cost)\n",
    "- **Complex model**: Expensive to describe, but good fit (low residual cost)\n",
    "- **Optimal model**: Minimizes the **sum**\n",
    "\n",
    "### 4. Compression and Regularity\n",
    "If you can describe data more briefly than the raw encoding, your model has captured regularity in the data. Random noise cannot be compressed (Section 3.4).\n",
    "\n",
    "### 5. MDL vs Others (Section 8)\n",
    "- **AIC**: Fixed penalty (2 per parameter) - can select overly complex models with large samples\n",
    "- **BIC**: log(n) penalty - equivalent to two-part MDL under certain conditions\n",
    "- **MDL**: Information-theoretic foundation - no arbitrary penalty constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393a8fc",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Try the exercises** in the `Exercises/` folder\n",
    "2. **Run `train_minimal.py`** with different parameters\n",
    "3. **Explore `visualization.py`** for more plots\n",
    "4. **Read the paper** for mathematical details on prequential MDL and NML"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
