{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 22: Deep Speech 2 — Interactive Walkthrough\n",
    "\n",
    "> Amodei et al. (2015) — End-to-End Speech Recognition\n",
    "\n",
    "This notebook walks through the core components of Deep Speech 2:\n",
    "1. Spectrogram feature extraction\n",
    "2. The DS2 model architecture\n",
    "3. CTC loss and greedy decoding\n",
    "4. Training on synthetic data\n",
    "\n",
    "See `README.md` for background and `paper_notes.md` for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from implementation import (\n",
    "    DeepSpeech2, CharEncoder, ClippedReLU,\n",
    "    compute_spectrogram, generate_synthetic_audio,\n",
    "    generate_synthetic_dataset, collate_batch,\n",
    "    greedy_decode, word_error_rate, train_step,\n",
    "    sortagrad_sampler\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spectrogram Features\n",
    "\n",
    "The model takes log power spectrograms as input (Section 3.1).\n",
    "20ms windows with 10ms stride convert a 1D audio signal into a 2D representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic audio for the word 'cat'\n",
    "audio = generate_synthetic_audio('cat', sample_rate=16000)\n",
    "spec = compute_spectrogram(audio, sample_rate=16000, n_fft=256)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "axes[0].plot(audio.numpy(), linewidth=0.5)\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlabel('Sample')\n",
    "axes[1].imshow(spec.numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1].set_title(f'Log Spectrogram: {spec.shape}')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Character Encoding and CTC\n",
    "\n",
    "The output alphabet is {a-z, space, apostrophe, blank} = 29 symbols.\n",
    "CTC decoding collapses repeated characters and removes blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CharEncoder('english')\n",
    "print(f'Vocabulary size: {encoder.vocab_size}')\n",
    "print(f'Blank index: {encoder.blank_idx}')\n",
    "\n",
    "# Encode and decode\n",
    "text = 'hello world'\n",
    "encoded = encoder.encode(text)\n",
    "decoded = encoder.decode(encoded)\n",
    "print(f'Encode: \"{text}\" -> {encoded}')\n",
    "print(f'Decode: {encoded} -> \"{decoded}\"')\n",
    "\n",
    "# CTC decoding example\n",
    "raw_ctc = [0, 0, 8, 8, 0, 5, 5, 5, 0, 12, 12, 0]\n",
    "ctc_decoded = encoder.ctc_decode(raw_ctc)\n",
    "print(f'CTC decode: {raw_ctc} -> \"{ctc_decoded}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Model\n",
    "\n",
    "Architecture: Conv2D -> Bidirectional GRU -> FC -> Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small DS2 model\n",
    "model = DeepSpeech2(\n",
    "    n_freq=129,       # for n_fft=256\n",
    "    vocab_size=encoder.vocab_size,\n",
    "    n_conv=1,\n",
    "    n_rnn=2,\n",
    "    rnn_hidden=128,\n",
    "    rnn_type='gru'\n",
    ")\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Parameters: {n_params:,}')\n",
    "print(f'Architecture:\\n{model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward Pass\n",
    "\n",
    "Run a batch through the model and inspect outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data and batch\n",
    "dataset = generate_synthetic_dataset(8, min_words=1, max_words=2)\n",
    "batch = collate_batch(dataset, encoder, n_fft=256)\n",
    "\n",
    "print(f'Features: {batch.features.shape}')\n",
    "print(f'Feature lengths: {batch.feature_lengths.tolist()}')\n",
    "print(f'Target lengths: {batch.target_lengths.tolist()}')\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    log_probs, out_lens = model(batch.features, batch.feature_lengths)\n",
    "    print(f'Output: {log_probs.shape} (time, batch, vocab)')\n",
    "    \n",
    "    # Greedy decode (untrained model, so output will be random)\n",
    "    decoded = greedy_decode(log_probs, encoder, out_lens)\n",
    "    for i in range(min(4, len(decoded))):\n",
    "        print(f'  Target: \"{dataset[i].transcript}\" | Decoded: \"{decoded[i]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Training Loop\n",
    "\n",
    "Train the model for a few epochs on synthetic data.\n",
    "Watch the CTC loss decrease and decoded output improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "train_data = generate_synthetic_dataset(40, min_words=1, max_words=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    indices = sortagrad_sampler(train_data, epoch)\n",
    "    \n",
    "    epoch_losses = []\n",
    "    for start in range(0, len(indices), 8):\n",
    "        batch_idx = indices[start:start+8]\n",
    "        samples = [train_data[i] for i in batch_idx]\n",
    "        batch = collate_batch(samples, encoder, n_fft=256)\n",
    "        loss = train_step(model, batch, optimizer)\n",
    "        epoch_losses.append(loss)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    sort = '(sorted)' if epoch == 0 else '(random)'\n",
    "    print(f'Epoch {epoch+1} {sort}: loss={np.mean(epoch_losses):.4f}')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('CTC Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate\n",
    "\n",
    "Check predictions after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_samples = train_data[:5]\n",
    "    batch = collate_batch(test_samples, encoder, n_fft=256)\n",
    "    log_probs, out_lens = model(batch.features, batch.feature_lengths)\n",
    "    decoded = greedy_decode(log_probs, encoder, out_lens)\n",
    "    \n",
    "    refs = [s.transcript for s in test_samples]\n",
    "    wer = word_error_rate(refs, decoded)\n",
    "    \n",
    "    print(f'WER: {wer:.2%}')\n",
    "    print()\n",
    "    for ref, hyp in zip(refs, decoded):\n",
    "        match = 'OK' if ref == hyp else 'MISS'\n",
    "        print(f'  [{match}] Target: \"{ref}\" | Output: \"{hyp}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- A single neural network (conv + BiGRU + CTC) replaces the entire traditional ASR pipeline\n",
    "- Spectrogram features: 20ms windows, log power, normalized per utterance\n",
    "- CTC loss handles variable-length alignment without frame-level labels\n",
    "- Sequence-wise BatchNorm computes stats over (batch x time), not per-timestep\n",
    "- SortaGrad sorts by length in epoch 0 only, then reverts to random\n",
    "- Deeper models (5-7 RNN layers) beat wider ones of similar parameter count\n",
    "- See `exercises/` to build each component from scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
