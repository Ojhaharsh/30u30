{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c02adef",
   "metadata": {},
   "source": [
    "# Day 2: Understanding LSTM Networks\n",
    "\n",
    "Day 2 of 30 Papers in 30 Days.\n",
    "\n",
    "Today: **Long Short-Term Memory (LSTM) networks** — one of the most important breakthroughs in sequence modeling. LSTMs solved the vanishing gradient problem that made vanilla RNNs unable to learn long-range dependencies.\n",
    "\n",
    "## What We'll Do\n",
    "\n",
    "1. **The Problem**: Why vanilla RNNs struggle with long sequences\n",
    "2. **The Solution**: How LSTMs use gates to control information flow\n",
    "3. **The Architecture**: The 4 key components (forget, input, cell, output)\n",
    "4. **The Implementation**: Building an LSTM from scratch in NumPy\n",
    "5. **The Visualization**: Seeing what LSTMs \"remember\" and \"forget\"\n",
    "\n",
    "## Colah's Core Insight\n",
    "\n",
    "The cell state runs parallel to the hidden state, like a conveyor belt (Colah's metaphor). Information flows along it unchanged unless the LSTM explicitly modifies it through three gates: forget, input, and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359888cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "# Import our LSTM implementation\n",
    "from implementation import LSTM\n",
    "from visualization import (\n",
    "    plot_gate_activations, \n",
    "    plot_cell_state_evolution,\n",
    "    plot_gradient_flow_comparison,\n",
    "    analyze_gate_patterns\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All imports successful.\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75720c1",
   "metadata": {},
   "source": [
    "## 1. The Vanishing Gradient Problem\n",
    "\n",
    "Before LSTMs, vanilla RNNs worked for short sequences but failed on long ones.\n",
    "\n",
    "### Why\n",
    "\n",
    "In backpropagation through time, gradients flow backward like this:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial h_T} \\prod_{t=2}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "Each term in the product is typically < 1, so:\n",
    "\n",
    "$$0.9 \\times 0.9 \\times 0.9 \\times ... \\times 0.9 \\text{ (50 times)} \\approx 0.005$$\n",
    "\n",
    "The gradient vanishes. The network can't learn what happened 50 steps ago.\n",
    "\n",
    "Colah's example from the post: predicting \"sky\" in \"the clouds are in the ___\" is easy (short range). Predicting \"French\" in \"I grew up in France... I speak fluent ___\" is hard (long range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bef166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vanishing gradients\n",
    "def simulate_gradient_flow(initial_grad=1.0, steps=50, factor=0.9):\n",
    "    \"\"\"Simulate gradient flowing backward through time.\"\"\"\n",
    "    gradients = [initial_grad]\n",
    "    for _ in range(steps):\n",
    "        gradients.append(gradients[-1] * factor)\n",
    "    return gradients\n",
    "\n",
    "# Compare different scenarios\n",
    "steps = range(51)\n",
    "vanilla_rnn = simulate_gradient_flow(1.0, 50, 0.9)\n",
    "lstm_sim = simulate_gradient_flow(1.0, 50, 0.99)  # LSTMs preserve gradients better\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(steps, vanilla_rnn, 'r-', linewidth=2, label='Vanilla RNN (0.9 factor)')\n",
    "plt.plot(steps, lstm_sim, 'g-', linewidth=2, label='LSTM (0.99 factor)')\n",
    "plt.axhline(y=0.1, color='orange', linestyle='--', alpha=0.5, label='Vanishing threshold')\n",
    "plt.xlabel('Time Steps Backward', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontsize=12)\n",
    "plt.title('Vanishing Gradients: RNN vs LSTM', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Vanilla RNN after 50 steps: {vanilla_rnn[-1]:.6f}\")\n",
    "print(f\"LSTM after 50 steps: {lstm_sim[-1]:.6f}\")\n",
    "print(f\"\\nLSTM preserves {lstm_sim[-1] / vanilla_rnn[-1]:.1f}x more gradient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e646db",
   "metadata": {},
   "source": [
    "## 2. The LSTM Solution: The Cell State\n",
    "\n",
    "LSTMs solve vanishing gradients with the **cell state** — a separate path that runs parallel to the hidden state.\n",
    "\n",
    "Colah's metaphor: the cell state is a \"conveyor belt\" that information rides along unchanged, unless the LSTM explicitly modifies it through gates.\n",
    "\n",
    "### The Key Equation\n",
    "\n",
    "The cell state updates via **addition** (not multiplication):\n",
    "\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "\n",
    "When gradients flow backward:\n",
    "\n",
    "$$\\frac{\\partial C_t}{\\partial C_{t-1}} = f_t$$\n",
    "\n",
    "Since $f_t \\approx 1$ (forget gate defaults to keeping information), gradients flow through without vanishing. This is the core reason LSTMs work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557dcf85",
   "metadata": {},
   "source": [
    "## 3. LSTM Architecture: The 4 Components\n",
    "\n",
    "An LSTM cell has 4 parts, following Colah's step-by-step walkthrough:\n",
    "\n",
    "### 1. Forget Gate ($f_t$) — What to throw away from cell state\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "- Outputs 0 (forget everything) to 1 (keep everything)\n",
    "- Colah's example: when you see a new subject, forget the old subject's gender\n",
    "\n",
    "### 2. Input Gate ($i_t$) — What new information to store\n",
    "\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "\n",
    "Controls how much of the new candidate values to write.\n",
    "\n",
    "### 3. Cell Candidate ($\\tilde{C}_t$) — The new values to potentially add\n",
    "\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "Outputs -1 to 1. These are the candidate values, scaled by the input gate before being added to cell state.\n",
    "\n",
    "### 4. Output Gate ($o_t$) — What to output from cell state\n",
    "\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "Colah's example: if we just saw a subject, output whether it's singular or plural (relevant for verb conjugation).\n",
    "\n",
    "### The Update\n",
    "\n",
    "1. **Forget**: $C_t = f_t \\odot C_{t-1}$ (scale down old info)\n",
    "2. **Add**: $C_t = C_t + i_t \\odot \\tilde{C}_t$ (add new info)\n",
    "3. **Output**: $h_t = o_t \\odot \\tanh(C_t)$ (filter what to expose)\n",
    "\n",
    "Cell state ($C_t$) is the memory. Hidden state ($h_t$) is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7298ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small vocabulary and LSTM for visualization\n",
    "chars = list(\"hello\")\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary: {chars}\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# Initialize a small LSTM\n",
    "hidden_size = 10  # Small for visualization\n",
    "lstm = LSTM(input_size=vocab_size, hidden_size=hidden_size, output_size=vocab_size)\n",
    "\n",
    "print(f\"\\nLSTM created:\")\n",
    "print(f\"  Input size: {vocab_size}\")\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "print(f\"  Output size: {vocab_size}\")\n",
    "print(f\"  Total parameters: {sum(p.size for p in [lstm.Wf, lstm.Wi, lstm.Wc, lstm.Wo, lstm.Wy])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6172e25",
   "metadata": {},
   "source": [
    "## 4. Forward Pass: Watching the Gates\n",
    "\n",
    "Run the sequence \"hello\" through the LSTM and capture gate activations at each step — forget, input, output — along with cell state evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequence\n",
    "text = \"hello\"\n",
    "inputs = [char_to_idx[ch] for ch in text]\n",
    "print(f\"Input sequence: {text}\")\n",
    "print(f\"As indices: {inputs}\")\n",
    "\n",
    "# Storage for gate activations\n",
    "gates_storage = {\n",
    "    'forget': [],\n",
    "    'input': [],\n",
    "    'output': []\n",
    "}\n",
    "cell_states_storage = []\n",
    "\n",
    "# Initialize hidden and cell states\n",
    "h_prev = np.zeros(hidden_size)\n",
    "C_prev = np.zeros(hidden_size)\n",
    "\n",
    "# Forward pass through sequence\n",
    "for t, idx in enumerate(inputs):\n",
    "    # Create one-hot encoded input\n",
    "    x = np.zeros(vocab_size)\n",
    "    x[idx] = 1.0\n",
    "    \n",
    "    # Compute all gates manually to capture them\n",
    "    concat = np.concatenate([h_prev, x])\n",
    "    \n",
    "    # Forget gate\n",
    "    f = lstm.sigmoid(np.dot(lstm.Wf, concat) + lstm.bf)\n",
    "    gates_storage['forget'].append(f.copy())\n",
    "    \n",
    "    # Input gate\n",
    "    i = lstm.sigmoid(np.dot(lstm.Wi, concat) + lstm.bi)\n",
    "    gates_storage['input'].append(i.copy())\n",
    "    \n",
    "    # Cell candidate\n",
    "    C_tilde = np.tanh(np.dot(lstm.Wc, concat) + lstm.bc)\n",
    "    \n",
    "    # Update cell state\n",
    "    C_prev = f * C_prev + i * C_tilde\n",
    "    cell_states_storage.append(C_prev.copy())\n",
    "    \n",
    "    # Output gate\n",
    "    o = lstm.sigmoid(np.dot(lstm.Wo, concat) + lstm.bo)\n",
    "    gates_storage['output'].append(o.copy())\n",
    "    \n",
    "    # Update hidden state\n",
    "    h_prev = o * np.tanh(C_prev)\n",
    "    \n",
    "    print(f\"\\nStep {t} ('{text[t]}'):\")\n",
    "    print(f\"  Forget gate avg: {f.mean():.3f} (1=keep, 0=forget)\")\n",
    "    print(f\"  Input gate avg:  {i.mean():.3f} (1=add, 0=ignore)\")\n",
    "    print(f\"  Output gate avg: {o.mean():.3f} (1=show, 0=hide)\")\n",
    "\n",
    "print(\"\\nForward pass complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41041844",
   "metadata": {},
   "source": [
    "## 5. Visualizing Gate Activations\n",
    "\n",
    "Heatmaps showing which hidden units are active at each time step, and how the cell state evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gate activations\n",
    "plot_gate_activations(gates_storage, text)\n",
    "\n",
    "# Visualize cell state evolution\n",
    "plot_cell_state_evolution(cell_states_storage, text)\n",
    "\n",
    "# Analyze patterns\n",
    "analyze_gate_patterns(gates_storage, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8bc61",
   "metadata": {},
   "source": [
    "## 6. Training on Real Text\n",
    "\n",
    "Train the LSTM on Shakespeare text to see how it learns next-character prediction. Same task as Day 1's vanilla RNN, but with the cell state machinery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training data\n",
    "training_text = \"\"\"\n",
    "To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them.\n",
    "\"\"\"\n",
    "\n",
    "# Create vocabulary\n",
    "chars_train = sorted(list(set(training_text)))\n",
    "char_to_idx_train = {ch: i for i, ch in enumerate(chars_train)}\n",
    "idx_to_char_train = {i: ch for i, ch in enumerate(chars_train)}\n",
    "vocab_size_train = len(chars_train)\n",
    "\n",
    "print(f\"Training text length: {len(training_text)} characters\")\n",
    "print(f\"Vocabulary size: {vocab_size_train}\")\n",
    "print(f\"Unique characters: {''.join(chars_train)}\")\n",
    "\n",
    "# Create LSTM\n",
    "lstm_train = LSTM(input_size=vocab_size_train, \n",
    "                  hidden_size=64, \n",
    "                  output_size=vocab_size_train)\n",
    "\n",
    "print(f\"\\nTraining LSTM created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ecde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "seq_length = 25\n",
    "learning_rate = 0.001\n",
    "num_iterations = 1000\n",
    "\n",
    "losses = []\n",
    "h_prev = np.zeros(lstm_train.hidden_size)\n",
    "C_prev = np.zeros(lstm_train.hidden_size)\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Sample random starting point\n",
    "    start_idx = np.random.randint(0, len(training_text) - seq_length - 1)\n",
    "    \n",
    "    # Get input and target sequences\n",
    "    input_seq = training_text[start_idx:start_idx + seq_length]\n",
    "    target_seq = training_text[start_idx + 1:start_idx + seq_length + 1]\n",
    "    \n",
    "    # Convert to indices\n",
    "    inputs = [char_to_idx_train[ch] for ch in input_seq]\n",
    "    targets = [char_to_idx_train[ch] for ch in target_seq]\n",
    "    \n",
    "    # Forward pass\n",
    "    loss = lstm_train.forward(inputs, targets, h_prev, C_prev)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    dh_next, dC_next = lstm_train.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    lstm_train.update_weights(learning_rate)\n",
    "    \n",
    "    # Update states\n",
    "    h_prev = lstm_train.h_states[-1].copy()\n",
    "    C_prev = lstm_train.C_states[-1].copy()\n",
    "    \n",
    "    # Print progress\n",
    "    if iteration % 100 == 0:\n",
    "        smooth_loss = np.mean(losses[-100:]) if len(losses) >= 100 else np.mean(losses)\n",
    "        print(f\"Iteration {iteration:4d} | Loss: {smooth_loss:.4f}\")\n",
    "        \n",
    "        # Sample text\n",
    "        if iteration % 500 == 0:\n",
    "            sample = lstm_train.sample(idx_to_char_train, char_to_idx_train['T'], 100)\n",
    "            print(f\"Sample: {sample[:60]}...\")\n",
    "            print()\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, alpha=0.3, label='Raw loss')\n",
    "# Smooth curve\n",
    "window = 50\n",
    "if len(losses) > window:\n",
    "    smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(losses)), smoothed, linewidth=2, label='Smoothed loss', color='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('LSTM Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d75cf6",
   "metadata": {},
   "source": [
    "## 7. Temperature Sampling\n",
    "\n",
    "Temperature controls sampling randomness:\n",
    "\n",
    "- **Low (0.5)**: Conservative, picks likely characters — more coherent but repetitive\n",
    "- **Medium (1.0)**: Balanced\n",
    "- **High (1.5)**: More random, picks unlikely characters — more diverse but less coherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different temperatures\n",
    "temperatures = [0.3, 0.7, 1.0, 1.5]\n",
    "seed_char = 'T'\n",
    "\n",
    "print(\"Sampling with different temperatures:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for temp in temperatures:\n",
    "    sample = lstm_train.sample(idx_to_char_train, \n",
    "                               char_to_idx_train[seed_char], \n",
    "                               length=150, \n",
    "                               temperature=temp)\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(f\"{sample[:120]}...\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fa4ea",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "**The problem:** Vanilla RNNs can't learn long-range dependencies because gradients vanish during backpropagation through time.\n",
    "\n",
    "**The solution (from Colah's post):** LSTMs add a cell state — a separate path where information flows via addition, not multiplication. Three gates (forget, input, output) control what gets written, kept, and exposed.\n",
    "\n",
    "**Why it works:** The gradient of $C_t$ w.r.t. $C_{t-1}$ is just $f_t$ (a scalar near 1), not a matrix multiplication. Gradients don't vanish.\n",
    "\n",
    "**Variants (also from the post):** Peephole connections, coupled gates, and GRUs (which merge forget and input gates into a single update gate).\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try the exercises in `exercises/` — especially the ablation study (exercise 3) which shows what happens when you remove individual gates\n",
    "- Compare training curves with Day 1's vanilla RNN on the same data\n",
    "- Read Colah's original post for the diagrams: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
