{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c02adef",
   "metadata": {},
   "source": [
    "# Day 2: Understanding LSTM Networks üß†\n",
    "\n",
    "Welcome to Day 2 of 30 Papers in 30 Days!\n",
    "\n",
    "Today we're diving into **Long Short-Term Memory (LSTM) networks** - one of the most important breakthroughs in deep learning. LSTMs solved a critical problem that plagued early neural networks: **vanishing gradients**.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **The Problem**: Why vanilla RNNs struggle with long sequences\n",
    "2. **The Solution**: How LSTMs use gates to control information flow\n",
    "3. **The Architecture**: Understanding the 4 key components (forget, input, cell, output)\n",
    "4. **The Implementation**: Building LSTMs from scratch in NumPy\n",
    "5. **The Visualization**: Seeing what LSTMs \"remember\" and \"forget\"\n",
    "\n",
    "## The Big Idea (in 30 seconds)\n",
    "\n",
    "Imagine you're managing a **to-do list** throughout your day:\n",
    "- **Forget gate**: Cross off completed tasks ‚úñÔ∏è\n",
    "- **Input gate**: Add new tasks ‚ûï\n",
    "- **Cell state**: The actual list (your memory) üìù\n",
    "- **Output gate**: What you're focusing on right now üëÅÔ∏è\n",
    "\n",
    "LSTMs do the same thing with information - they decide what to remember, what to forget, and what to output at each step!\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359888cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "# Import our LSTM implementation\n",
    "from implementation import LSTM\n",
    "from visualization import (\n",
    "    plot_gate_activations, \n",
    "    plot_cell_state_evolution,\n",
    "    plot_gradient_flow_comparison,\n",
    "    analyze_gate_patterns\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75720c1",
   "metadata": {},
   "source": [
    "## 1. The Vanishing Gradient Problem üìâ\n",
    "\n",
    "Before LSTMs, we had **vanilla RNNs**. They worked great for short sequences but failed miserably for long ones. Why?\n",
    "\n",
    "### The Bucket Brigade Analogy ü™£\n",
    "\n",
    "Imagine passing water down a line of people (a \"bucket brigade\"):\n",
    "- Person 1 fills a bucket and passes it\n",
    "- Person 2 receives it (but spills 10%) and passes it\n",
    "- Person 3 receives it (spills another 10%) and passes it\n",
    "- ...and so on\n",
    "\n",
    "By the time the bucket reaches Person 50, **almost all the water is gone**! \n",
    "\n",
    "This is the **vanishing gradient problem**: as information flows backward through time during training, the gradient gets multiplied by values < 1 at each step, eventually vanishing to zero.\n",
    "\n",
    "### The Math\n",
    "\n",
    "In a vanilla RNN, gradients flow backward like this:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial h_T} \\prod_{t=2}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "Each term in the product is typically < 1, so:\n",
    "\n",
    "$$0.9 \\times 0.9 \\times 0.9 \\times ... \\times 0.9 \\text{ (50 times)} \\approx 0.005$$\n",
    "\n",
    "**The gradient vanishes!** The network can't learn long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bef166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vanishing gradients\n",
    "def simulate_gradient_flow(initial_grad=1.0, steps=50, factor=0.9):\n",
    "    \"\"\"Simulate gradient flowing backward through time.\"\"\"\n",
    "    gradients = [initial_grad]\n",
    "    for _ in range(steps):\n",
    "        gradients.append(gradients[-1] * factor)\n",
    "    return gradients\n",
    "\n",
    "# Compare different scenarios\n",
    "steps = range(51)\n",
    "vanilla_rnn = simulate_gradient_flow(1.0, 50, 0.9)\n",
    "lstm_sim = simulate_gradient_flow(1.0, 50, 0.99)  # LSTMs preserve gradients better\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(steps, vanilla_rnn, 'r-', linewidth=2, label='Vanilla RNN (0.9 factor)')\n",
    "plt.plot(steps, lstm_sim, 'g-', linewidth=2, label='LSTM (0.99 factor)')\n",
    "plt.axhline(y=0.1, color='orange', linestyle='--', alpha=0.5, label='Vanishing threshold')\n",
    "plt.xlabel('Time Steps Backward', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontsize=12)\n",
    "plt.title('Vanishing Gradients: RNN vs LSTM', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Vanilla RNN after 50 steps: {vanilla_rnn[-1]:.6f}\")\n",
    "print(f\"LSTM after 50 steps: {lstm_sim[-1]:.6f}\")\n",
    "print(f\"\\nLSTM preserves {lstm_sim[-1] / vanilla_rnn[-1]:.1f}x more gradient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e646db",
   "metadata": {},
   "source": [
    "## 2. The LSTM Solution: A Memory Highway üõ£Ô∏è\n",
    "\n",
    "LSTMs solve vanishing gradients with a clever trick: **the cell state**.\n",
    "\n",
    "### The Highway Analogy\n",
    "\n",
    "Think of gradients traveling backward in time:\n",
    "\n",
    "**Vanilla RNN** = Country road with stop signs every block\n",
    "- Gradients must stop at each time step\n",
    "- Subject to multiplication by < 1 values\n",
    "- Gets slower and weaker over distance\n",
    "\n",
    "**LSTM** = Highway with direct exit ramps\n",
    "- Cell state provides a \"highway\" for gradients\n",
    "- Gradients can flow almost unchanged\n",
    "- Only need to exit (via gates) when needed\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "The cell state updates via **addition** (not multiplication):\n",
    "\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "\n",
    "When gradients flow backward:\n",
    "\n",
    "$$\\frac{\\partial C_t}{\\partial C_{t-1}} = f_t$$\n",
    "\n",
    "Since $f_t \\approx 1$ (forget gate usually keeps most information), **gradients flow freely!**\n",
    "\n",
    "This is why LSTMs can learn dependencies 100+ steps apart, while vanilla RNNs struggle beyond 10 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557dcf85",
   "metadata": {},
   "source": [
    "## 3. LSTM Architecture: The 4 Gates üö™\n",
    "\n",
    "An LSTM has **4 key components** that work together:\n",
    "\n",
    "### 1. Forget Gate ($f_t$) - The Bouncer üö´\n",
    "**Job**: Decide what to throw away from cell state\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "- Output: 0 (forget everything) to 1 (keep everything)\n",
    "- Analogy: A bouncer deciding who gets to stay in the club\n",
    "\n",
    "### 2. Input Gate ($i_t$) - The Security Guard ‚úÖ\n",
    "**Job**: Decide what new information to add\n",
    "\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "\n",
    "- Output: 0 (ignore new info) to 1 (accept it all)\n",
    "- Analogy: A security guard deciding what new items to let in\n",
    "\n",
    "### 3. Cell Candidate ($\\tilde{C}_t$) - The New Information üì¶\n",
    "**Job**: Create potential new information\n",
    "\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "- Output: -1 to 1 (new values to potentially add)\n",
    "- Analogy: The actual items trying to enter\n",
    "\n",
    "### 4. Output Gate ($o_t$) - The Librarian üìö\n",
    "**Job**: Decide what to output from cell state\n",
    "\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "- Output: 0 (hide everything) to 1 (show everything)\n",
    "- Analogy: A librarian deciding what books to show you\n",
    "\n",
    "### How They Work Together\n",
    "\n",
    "1. **Forget**: $C_t = f_t \\odot C_{t-1}$ (throw away old info)\n",
    "2. **Add**: $C_t = C_t + i_t \\odot \\tilde{C}_t$ (add new info)\n",
    "3. **Output**: $h_t = o_t \\odot \\tanh(C_t)$ (decide what to reveal)\n",
    "\n",
    "**The cell state ($C_t$)** is the memory. **The hidden state ($h_t$)** is what gets outputted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7298ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a tiny LSTM and see it in action!\n",
    "\n",
    "# Create a small vocabulary\n",
    "chars = list(\"hello\")\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary: {chars}\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# Initialize a small LSTM\n",
    "hidden_size = 10  # Small for visualization\n",
    "lstm = LSTM(input_size=vocab_size, hidden_size=hidden_size, output_size=vocab_size)\n",
    "\n",
    "print(f\"\\n‚úÖ LSTM created!\")\n",
    "print(f\"   - Input size: {vocab_size}\")\n",
    "print(f\"   - Hidden size: {hidden_size}\")\n",
    "print(f\"   - Output size: {vocab_size}\")\n",
    "print(f\"   - Total parameters: {sum(p.size for p in [lstm.Wf, lstm.Wi, lstm.Wc, lstm.Wo, lstm.Wy])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6172e25",
   "metadata": {},
   "source": [
    "## 4. Forward Pass: Watching the Gates Work üëÅÔ∏è\n",
    "\n",
    "Let's run a sequence through the LSTM and watch what the gates do!\n",
    "\n",
    "We'll feed it the sequence **\"hello\"** and capture:\n",
    "- Forget gate activations (what to keep)\n",
    "- Input gate activations (what to add)\n",
    "- Output gate activations (what to reveal)\n",
    "- Cell state evolution (the memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequence\n",
    "text = \"hello\"\n",
    "inputs = [char_to_idx[ch] for ch in text]\n",
    "print(f\"Input sequence: {text}\")\n",
    "print(f\"As indices: {inputs}\")\n",
    "\n",
    "# Storage for gate activations\n",
    "gates_storage = {\n",
    "    'forget': [],\n",
    "    'input': [],\n",
    "    'output': []\n",
    "}\n",
    "cell_states_storage = []\n",
    "\n",
    "# Initialize hidden and cell states\n",
    "h_prev = np.zeros(hidden_size)\n",
    "C_prev = np.zeros(hidden_size)\n",
    "\n",
    "# Forward pass through sequence\n",
    "for t, idx in enumerate(inputs):\n",
    "    # Create one-hot encoded input\n",
    "    x = np.zeros(vocab_size)\n",
    "    x[idx] = 1.0\n",
    "    \n",
    "    # Compute all gates (we'll manually compute to capture them)\n",
    "    concat = np.concatenate([h_prev, x])\n",
    "    \n",
    "    # Forget gate\n",
    "    f = lstm.sigmoid(np.dot(lstm.Wf, concat) + lstm.bf)\n",
    "    gates_storage['forget'].append(f.copy())\n",
    "    \n",
    "    # Input gate\n",
    "    i = lstm.sigmoid(np.dot(lstm.Wi, concat) + lstm.bi)\n",
    "    gates_storage['input'].append(i.copy())\n",
    "    \n",
    "    # Cell candidate\n",
    "    C_tilde = np.tanh(np.dot(lstm.Wc, concat) + lstm.bc)\n",
    "    \n",
    "    # Update cell state\n",
    "    C_prev = f * C_prev + i * C_tilde\n",
    "    cell_states_storage.append(C_prev.copy())\n",
    "    \n",
    "    # Output gate\n",
    "    o = lstm.sigmoid(np.dot(lstm.Wo, concat) + lstm.bo)\n",
    "    gates_storage['output'].append(o.copy())\n",
    "    \n",
    "    # Update hidden state\n",
    "    h_prev = o * np.tanh(C_prev)\n",
    "    \n",
    "    print(f\"\\nStep {t} ('{text[t]}'):\")\n",
    "    print(f\"  Forget gate avg: {f.mean():.3f} (1=keep, 0=forget)\")\n",
    "    print(f\"  Input gate avg:  {i.mean():.3f} (1=add, 0=ignore)\")\n",
    "    print(f\"  Output gate avg: {o.mean():.3f} (1=show, 0=hide)\")\n",
    "\n",
    "print(\"\\n‚úÖ Forward pass complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41041844",
   "metadata": {},
   "source": [
    "## 5. Visualizing Gate Activations üìä\n",
    "\n",
    "Now let's visualize what the gates are doing! This helps us understand:\n",
    "- **Which hidden units are active** (bright colors)\n",
    "- **When gates open/close** (across time steps)\n",
    "- **Patterns in gate behavior** (do they learn structure?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gate activations\n",
    "plot_gate_activations(gates_storage, text)\n",
    "\n",
    "# Visualize cell state evolution\n",
    "plot_cell_state_evolution(cell_states_storage, text)\n",
    "\n",
    "# Analyze patterns\n",
    "analyze_gate_patterns(gates_storage, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8bc61",
   "metadata": {},
   "source": [
    "## 6. Training on Real Text üìö\n",
    "\n",
    "Let's train our LSTM on a real text dataset! We'll use a small training example to see how the LSTM learns to predict the next character.\n",
    "\n",
    "For this demo, we'll use Shakespeare text (or any text you have)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training data (you can replace with Shakespeare or any text file)\n",
    "training_text = \"\"\"\n",
    "To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them.\n",
    "\"\"\"\n",
    "\n",
    "# Create vocabulary\n",
    "chars_train = sorted(list(set(training_text)))\n",
    "char_to_idx_train = {ch: i for i, ch in enumerate(chars_train)}\n",
    "idx_to_char_train = {i: ch for i, ch in enumerate(chars_train)}\n",
    "vocab_size_train = len(chars_train)\n",
    "\n",
    "print(f\"Training text length: {len(training_text)} characters\")\n",
    "print(f\"Vocabulary size: {vocab_size_train}\")\n",
    "print(f\"Unique characters: {''.join(chars_train)}\")\n",
    "\n",
    "# Create LSTM\n",
    "lstm_train = LSTM(input_size=vocab_size_train, \n",
    "                  hidden_size=64, \n",
    "                  output_size=vocab_size_train)\n",
    "\n",
    "print(\"\\n‚úÖ Training LSTM created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ecde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "seq_length = 25\n",
    "learning_rate = 0.001\n",
    "num_iterations = 1000\n",
    "\n",
    "losses = []\n",
    "h_prev = np.zeros(lstm_train.hidden_size)\n",
    "C_prev = np.zeros(lstm_train.hidden_size)\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Sample random starting point\n",
    "    start_idx = np.random.randint(0, len(training_text) - seq_length - 1)\n",
    "    \n",
    "    # Get input and target sequences\n",
    "    input_seq = training_text[start_idx:start_idx + seq_length]\n",
    "    target_seq = training_text[start_idx + 1:start_idx + seq_length + 1]\n",
    "    \n",
    "    # Convert to indices\n",
    "    inputs = [char_to_idx_train[ch] for ch in input_seq]\n",
    "    targets = [char_to_idx_train[ch] for ch in target_seq]\n",
    "    \n",
    "    # Forward pass\n",
    "    loss = lstm_train.forward(inputs, targets, h_prev, C_prev)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    dh_next, dC_next = lstm_train.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    lstm_train.update_weights(learning_rate)\n",
    "    \n",
    "    # Update states (with detachment to prevent gradient accumulation)\n",
    "    h_prev = lstm_train.h_states[-1].copy()\n",
    "    C_prev = lstm_train.C_states[-1].copy()\n",
    "    \n",
    "    # Print progress\n",
    "    if iteration % 100 == 0:\n",
    "        smooth_loss = np.mean(losses[-100:]) if len(losses) >= 100 else np.mean(losses)\n",
    "        print(f\"Iteration {iteration:4d} | Loss: {smooth_loss:.4f}\")\n",
    "        \n",
    "        # Sample text\n",
    "        if iteration % 500 == 0:\n",
    "            sample = lstm_train.sample(idx_to_char_train, char_to_idx_train['T'], 100)\n",
    "            print(f\"Sample: {sample[:60]}...\")\n",
    "            print()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, alpha=0.3, label='Raw loss')\n",
    "# Smooth curve\n",
    "window = 50\n",
    "if len(losses) > window:\n",
    "    smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(losses)), smoothed, linewidth=2, label='Smoothed loss', color='red')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('LSTM Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d75cf6",
   "metadata": {},
   "source": [
    "## 7. Temperature Sampling Experiments üå°Ô∏è\n",
    "\n",
    "Temperature controls how \"creative\" vs \"conservative\" the model is when generating text:\n",
    "\n",
    "- **Low temperature (0.5)**: Conservative, picks likely characters ‚Üí coherent but boring\n",
    "- **Medium temperature (1.0)**: Balanced ‚Üí good mix\n",
    "- **High temperature (1.5)**: Creative, picks unlikely characters ‚Üí diverse but chaotic\n",
    "\n",
    "Think of it like adjusting the \"randomness knob\" on the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different temperatures\n",
    "temperatures = [0.3, 0.7, 1.0, 1.5]\n",
    "seed_char = 'T'\n",
    "\n",
    "print(\"Sampling with different temperatures:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for temp in temperatures:\n",
    "    sample = lstm_train.sample(idx_to_char_train, \n",
    "                               char_to_idx_train[seed_char], \n",
    "                               length=150, \n",
    "                               temperature=temp)\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(f\"{sample[:120]}...\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fa4ea",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways üéØ\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **The Problem**: Vanilla RNNs suffer from vanishing gradients\n",
    "   - Gradients multiply by < 1 at each step\n",
    "   - Can't learn dependencies > 10 steps away\n",
    "\n",
    "2. **The Solution**: LSTMs use a cell state \"highway\"\n",
    "   - Cell state updates via **addition** (not multiplication)\n",
    "   - Gradients flow almost unchanged backward in time\n",
    "   - Can learn dependencies 100+ steps away\n",
    "\n",
    "3. **The Architecture**: 4 components work together\n",
    "   - **Forget gate**: What to remove from memory\n",
    "   - **Input gate**: What new info to add\n",
    "   - **Cell state**: The actual memory\n",
    "   - **Output gate**: What to reveal\n",
    "\n",
    "4. **The Intuition**: Think of it as a smart todo list\n",
    "   - Cross off completed tasks (forget)\n",
    "   - Add new tasks (input)\n",
    "   - Keep the list (cell state)\n",
    "   - Decide what's relevant now (output)\n",
    "\n",
    "### When to Use LSTMs\n",
    "\n",
    "‚úÖ **Use LSTMs when:**\n",
    "- You have sequential data (text, time series, audio)\n",
    "- Long-range dependencies matter (>10 steps)\n",
    "- You need interpretable gates\n",
    "- Dataset is small-to-medium sized\n",
    "\n",
    "‚ùå **Don't use LSTMs when:**\n",
    "- You have non-sequential data (images, tables)\n",
    "- Very long sequences (>1000 steps) ‚Üí use Transformers\n",
    "- You need maximum performance ‚Üí use Transformers\n",
    "- You have huge datasets ‚Üí Transformers train better at scale\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try exercise 1: Build LSTM from scratch\n",
    "- Experiment with different hyperparameters\n",
    "- Train on your own text data\n",
    "- Compare with vanilla RNN and GRU\n",
    "\n",
    "**Tomorrow (Day 3)**: We'll explore another foundational paper!\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You now understand how LSTMs work and why they revolutionized sequence modeling!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
