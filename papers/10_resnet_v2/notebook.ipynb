{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ddceee",
   "metadata": {},
   "source": [
    "# Day 10: ResNet V2 - Identity Mappings in Deep Residual Networks üéØ\n",
    "\n",
    "Welcome to Day 10 of 30 Papers in 30 Days!\n",
    "\n",
    "Today we're exploring **ResNet V2** - the refined version that fixed what was already great. By simply rearranging batch normalization and ReLU (pre-activation), ResNet V2 enables training networks over 1000 layers deep!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Pre-activation**: Why moving BN and ReLU before convolution matters\n",
    "2. **Perfect Identity Paths**: Creating clean gradient highways\n",
    "3. **Ultra-deep Networks**: Training 1000+ layer models successfully\n",
    "4. **Ablation Studies**: Understanding each component's contribution\n",
    "5. **Implementation**: Building ResNet V2 blocks from scratch\n",
    "6. **Comparison**: ResNet vs ResNet V2 performance\n",
    "\n",
    "## The Big Idea (in 30 seconds)\n",
    "\n",
    "**Original ResNet**: `output = ReLU(Conv(x) + x)`\n",
    "\n",
    "**ResNet V2**: `output = Conv(ReLU(BN(x))) + x`\n",
    "\n",
    "**Magic**: Moving activation to the beginning creates a clean identity path for gradient flow!\n",
    "\n",
    "**Result**: Networks can now be trained with 1000+ layers and converge better!\n",
    "\n",
    "Let's explore the power of pre-activation! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import our ResNet V2 implementation\n",
    "from implementation import PreActResNet, PreActBlock, BottleneckPreActBlock\n",
    "from visualization import ResNetV2Visualizer\n",
    "from train_minimal import train_resnet_v2, create_synthetic_dataset\n",
    "\n",
    "# Set up device and seeds\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"üî• Using device: {device}\")\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"üéØ Ready to explore pre-activation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf508e3",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Pre-activation\n",
    "\n",
    "The key insight of ResNet V2: **move batch normalization and ReLU before convolution** instead of after. This creates a clean identity path for gradients.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "**Original ResNet (post-activation)**:\n",
    "```\n",
    "x ‚Üí Conv ‚Üí BN ‚Üí ReLU ‚Üí Conv ‚Üí BN ‚Üí (+) ‚Üí ReLU ‚Üí output\n",
    "                                  ‚Üë\n",
    "                                  x\n",
    "```\n",
    "\n",
    "**ResNet V2 (pre-activation)**:\n",
    "```\n",
    "x ‚Üí BN ‚Üí ReLU ‚Üí Conv ‚Üí BN ‚Üí ReLU ‚Üí Conv ‚Üí (+) ‚Üí output\n",
    "                                             ‚Üë\n",
    "                                             x (clean!)\n",
    "```\n",
    "\n",
    "Notice: In V2, the identity connection `x` goes directly to the output without any transformation!\n",
    "\n",
    "Let's visualize this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the architectural difference\n",
    "def visualize_preactivation_difference():\n",
    "    \"\"\"Compare original ResNet vs ResNet V2 block structure.\"\"\"\n",
    "    \n",
    "    print(\"üèóÔ∏è Comparing ResNet vs ResNet V2 Architecture...\")\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Original ResNet (post-activation)\n",
    "    ax1.text(0.5, 0.95, 'Input x', ha='center', fontsize=14, weight='bold', color='blue')\n",
    "    ax1.arrow(0.5, 0.92, 0, -0.05, head_width=0.04, head_length=0.02, fc='blue')\n",
    "    \n",
    "    # Main path\n",
    "    y = 0.82\n",
    "    ax1.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "    ax1.text(0.5, y+0.04, 'Conv', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax1.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    y -= 0.11\n",
    "    ax1.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "    ax1.text(0.5, y+0.04, 'BN', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax1.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    y -= 0.11\n",
    "    ax1.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "    ax1.text(0.5, y+0.04, 'ReLU', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax1.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    y -= 0.11\n",
    "    ax1.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "    ax1.text(0.5, y+0.04, 'Conv', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax1.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    y -= 0.11\n",
    "    ax1.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "    ax1.text(0.5, y+0.04, 'BN', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax1.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    # Skip connection (with transformation)\n",
    "    ax1.arrow(0.15, 0.95, 0, -0.65, head_width=0, head_length=0, fc='red', ec='red', linestyle='--', linewidth=3)\n",
    "    ax1.add_patch(plt.Rectangle((0.08, 0.32), 0.14, 0.06, fill=True, facecolor='pink', edgecolor='red', linewidth=2))\n",
    "    ax1.text(0.15, 0.35, 'Transform?', ha='center', va='center', fontsize=9, weight='bold', color='red')\n",
    "    \n",
    "    # Addition\n",
    "    y -= 0.11\n",
    "    ax1.add_patch(plt.Circle((0.5, y+0.04), 0.05, fill=True, facecolor='orange', edgecolor='black', linewidth=2))\n",
    "    ax1.text(0.5, y+0.04, '+', ha='center', va='center', fontsize=16, weight='bold')\n",
    "    ax1.arrow(0.5, y-0.01, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    # Final ReLU\n",
    "    y -= 0.11\n",
    "    ax1.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "    ax1.text(0.5, y+0.04, 'ReLU', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax1.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    ax1.text(0.5, 0.05, 'Output', ha='center', fontsize=14, weight='bold', color='blue')\n",
    "    \n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Original ResNet (Post-Activation)\\n‚ùå Identity path can be transformed', \n",
    "                  fontsize=13, weight='bold', color='darkred')\n",
    "    \n",
    "    # ResNet V2 (pre-activation)\n",
    "    ax2.text(0.5, 0.95, 'Input x', ha='center', fontsize=14, weight='bold', color='blue')\n",
    "    ax2.arrow(0.5, 0.92, 0, -0.05, head_width=0.04, head_length=0.02, fc='blue')\n",
    "    \n",
    "    # Main path with pre-activation\n",
    "    y = 0.82\n",
    "    ax2.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "    ax2.text(0.5, y+0.04, 'BN', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax2.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    y -= 0.11\n",
    "    ax2.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "    ax2.text(0.5, y+0.04, 'ReLU', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax2.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    y -= 0.11\n",
    "    ax2.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "    ax2.text(0.5, y+0.04, 'Conv', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax2.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    y -= 0.11\n",
    "    ax2.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "    ax2.text(0.5, y+0.04, 'BN', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax2.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    y -= 0.11\n",
    "    ax2.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "    ax2.text(0.5, y+0.04, 'ReLU', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax2.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    y -= 0.11\n",
    "    ax2.add_patch(plt.Rectangle((0.35, y), 0.3, 0.08, fill=True, facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "    ax2.text(0.5, y+0.04, 'Conv', ha='center', va='center', fontsize=11, weight='bold')\n",
    "    ax2.arrow(0.5, y, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    # Skip connection (clean identity!)\n",
    "    ax2.arrow(0.15, 0.95, 0, -0.77, head_width=0.04, head_length=0.02, fc='green', ec='green', \n",
    "              linestyle='--', linewidth=4)\n",
    "    ax2.text(0.08, 0.55, 'Clean\\nIdentity!', ha='center', va='center', fontsize=11, \n",
    "             weight='bold', color='green', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "    \n",
    "    # Addition (no final activation!)\n",
    "    y -= 0.11\n",
    "    ax2.add_patch(plt.Circle((0.5, y+0.04), 0.05, fill=True, facecolor='orange', edgecolor='black', linewidth=2))\n",
    "    ax2.text(0.5, y+0.04, '+', ha='center', va='center', fontsize=16, weight='bold')\n",
    "    ax2.arrow(0.5, y-0.01, 0, -0.03, head_width=0.04, head_length=0.01, fc='blue')\n",
    "    \n",
    "    ax2.text(0.5, 0.05, 'Output', ha='center', fontsize=14, weight='bold', color='blue')\n",
    "    \n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('ResNet V2 (Pre-Activation)\\n‚úÖ Perfect identity path for gradients!', \n",
    "                  fontsize=13, weight='bold', color='darkgreen')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Key Differences:\")\n",
    "    print(\"  1. ‚úÖ Pre-activation: BN and ReLU BEFORE conv (not after)\")\n",
    "    print(\"  2. ‚úÖ Clean identity: x flows directly to output\")\n",
    "    print(\"  3. ‚úÖ No final ReLU: Output can be negative (important!)\")\n",
    "    print(\"  4. ‚úÖ Better gradients: Direct path for backpropagation\")\n",
    "    print(\"\\nüéØ Result: Better optimization, especially for ultra-deep networks!\")\n",
    "\n",
    "visualize_preactivation_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fea144",
   "metadata": {},
   "source": [
    "## Part 2: Building Pre-Activation Blocks\n",
    "\n",
    "Let's implement pre-activation blocks and compare them with original ResNet blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and explore pre-activation blocks\n",
    "def explore_preact_blocks():\n",
    "    \"\"\"Build and understand pre-activation residual blocks.\"\"\"\n",
    "    \n",
    "    print(\"üî® Building Pre-Activation Blocks...\")\n",
    "    \n",
    "    # Create a pre-activation block\n",
    "    preact_block = PreActBlock(64, 64, stride=1)\n",
    "    \n",
    "    print(\"\\nüìê Pre-Activation Block Structure:\")\n",
    "    print(preact_block)\n",
    "    \n",
    "    # Test forward pass\n",
    "    x = torch.randn(2, 64, 32, 32)\n",
    "    \n",
    "    print(f\"\\nüß™ Testing forward pass:\")\n",
    "    print(f\"Input shape: {list(x.shape)}\")\n",
    "    \n",
    "    # Manual trace\n",
    "    identity = x\n",
    "    \n",
    "    # Pre-activation path\n",
    "    out = preact_block.bn1(x)\n",
    "    print(f\"After BN1: {list(out.shape)}\")\n",
    "    \n",
    "    out = F.relu(out)\n",
    "    print(f\"After ReLU1: {list(out.shape)}\")\n",
    "    \n",
    "    out = preact_block.conv1(out)\n",
    "    print(f\"After Conv1: {list(out.shape)}\")\n",
    "    \n",
    "    out = preact_block.bn2(out)\n",
    "    print(f\"After BN2: {list(out.shape)}\")\n",
    "    \n",
    "    out = F.relu(out)\n",
    "    print(f\"After ReLU2: {list(out.shape)}\")\n",
    "    \n",
    "    out = preact_block.conv2(out)\n",
    "    print(f\"After Conv2: {list(out.shape)}\")\n",
    "    \n",
    "    # Add identity (clean!)\n",
    "    out += identity\n",
    "    print(f\"After adding identity: {list(out.shape)}\")\n",
    "    print(f\"‚úÖ Notice: No final activation! Output preserves full information.\")\n",
    "    \n",
    "    # Compare with actual forward pass\n",
    "    output = preact_block(x)\n",
    "    print(f\"\\nActual forward pass output: {list(output.shape)}\")\n",
    "    \n",
    "    # Test gradient flow\n",
    "    print(\"\\nüåä Testing gradient flow...\")\n",
    "    x_test = torch.randn(1, 64, 32, 32, requires_grad=True)\n",
    "    output = preact_block(x_test)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"Gradient magnitude at input: {x_test.grad.abs().mean().item():.6f}\")\n",
    "    print(\"‚úÖ Gradients flow smoothly through the clean identity path!\")\n",
    "    \n",
    "    return preact_block\n",
    "\n",
    "preact_block = explore_preact_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e6991",
   "metadata": {},
   "source": [
    "## Part 3: Training Ultra-Deep Networks\n",
    "\n",
    "The real power of ResNet V2: training networks with 1000+ layers! Let's test this capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79992e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train networks of different depths\n",
    "def test_depth_scalability():\n",
    "    \"\"\"Test how ResNet V2 handles extreme depth.\"\"\"\n",
    "    \n",
    "    print(\"üèîÔ∏è Testing Depth Scalability with ResNet V2...\")\n",
    "    \n",
    "    # Create networks of increasing depth\n",
    "    # For demo purposes, we'll simulate with different configurations\n",
    "    depths = [20, 50, 110, 200]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\nüî¨ Testing different depths...\")\n",
    "    \n",
    "    for depth in depths:\n",
    "        print(f\"\\nüìä Depth: {depth} layers\")\n",
    "        \n",
    "        # Create a simple ultra-deep network\n",
    "        layers = []\n",
    "        for i in range(depth // 2):  # Each block is ~2 layers\n",
    "            layers.append(PreActBlock(64, 64, stride=1))\n",
    "        \n",
    "        model = nn.Sequential(*layers).to(device)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"  Parameters: {total_params:,}\")\n",
    "        \n",
    "        # Test gradient flow\n",
    "        x = torch.randn(1, 64, 32, 32, requires_grad=True).to(device)\n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Measure gradient health\n",
    "        grad_norm = x.grad.norm().item()\n",
    "        results[depth] = {\n",
    "            'params': total_params,\n",
    "            'grad_norm': grad_norm\n",
    "        }\n",
    "        \n",
    "        print(f\"  Gradient norm at input: {grad_norm:.6f}\")\n",
    "        print(f\"  ‚úÖ Gradients {'healthy' if grad_norm > 0.001 else 'vanishing'}!\")\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    depths_list = list(results.keys())\n",
    "    param_counts = [results[d]['params'] / 1e6 for d in depths_list]\n",
    "    grad_norms = [results[d]['grad_norm'] for d in depths_list]\n",
    "    \n",
    "    # Parameter scaling\n",
    "    ax1.plot(depths_list, param_counts, 'b-o', linewidth=2, markersize=10)\n",
    "    ax1.set_xlabel('Network Depth (layers)', fontsize=12)\n",
    "    ax1.set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "    ax1.set_title('Parameter Count vs Depth', fontsize=14, weight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient health\n",
    "    ax2.plot(depths_list, grad_norms, 'g-s', linewidth=2, markersize=10)\n",
    "    ax2.axhline(y=0.001, color='r', linestyle='--', label='Vanishing threshold')\n",
    "    ax2.set_xlabel('Network Depth (layers)', fontsize=12)\n",
    "    ax2.set_ylabel('Gradient Norm', fontsize=12)\n",
    "    ax2.set_title('Gradient Health vs Depth', fontsize=14, weight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ Key Insight:\")\n",
    "    print(\"  Pre-activation design maintains healthy gradients even at extreme depths!\")\n",
    "    print(\"  Original ResNet would struggle with networks this deep.\")\n",
    "    print(\"  ResNet V2 enables training 1000+ layer networks successfully!\")\n",
    "\n",
    "test_depth_scalability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1120e3",
   "metadata": {},
   "source": [
    "## Part 4: ResNet vs ResNet V2 Comparison\n",
    "\n",
    "Let's directly compare original ResNet with ResNet V2 on the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ba2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ResNet vs ResNet V2\n",
    "def compare_resnet_versions():\n",
    "    \"\"\"Compare training dynamics of ResNet vs ResNet V2.\"\"\"\n",
    "    \n",
    "    print(\"‚öîÔ∏è ResNet vs ResNet V2 Showdown...\")\n",
    "    \n",
    "    # Create synthetic dataset\n",
    "    print(\"\\nüì¶ Creating dataset...\")\n",
    "    X = torch.randn(800, 3, 32, 32)\n",
    "    y = torch.randint(0, 10, (800,))\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Original ResNet Block (post-activation)\n",
    "    class OriginalResBlock(nn.Module):\n",
    "        def __init__(self, channels):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(channels)\n",
    "            self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            identity = x\n",
    "            out = F.relu(self.bn1(self.conv1(x)))\n",
    "            out = self.bn2(self.conv2(out))\n",
    "            out += identity\n",
    "            out = F.relu(out)  # Post-activation!\n",
    "            return out\n",
    "    \n",
    "    # Build comparable networks\n",
    "    class OriginalResNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 64, 3, 1, 1)\n",
    "            self.blocks = nn.Sequential(*[OriginalResBlock(64) for _ in range(8)])\n",
    "            self.fc = nn.Linear(64, 10)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = self.blocks(x)\n",
    "            x = F.adaptive_avg_pool2d(x, 1)\n",
    "            x = torch.flatten(x, 1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    class PreActResNetModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 64, 3, 1, 1)\n",
    "            self.blocks = nn.Sequential(*[PreActBlock(64, 64) for _ in range(8)])\n",
    "            self.fc = nn.Linear(64, 10)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.blocks(x)\n",
    "            x = F.adaptive_avg_pool2d(x, 1)\n",
    "            x = torch.flatten(x, 1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    models = {\n",
    "        'Original ResNet': OriginalResNet().to(device),\n",
    "        'ResNet V2': PreActResNetModel().to(device)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüèãÔ∏è Training {name}...\")\n",
    "        \n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        losses = []\n",
    "        grad_norms = []\n",
    "        \n",
    "        for epoch in range(15):\n",
    "            epoch_loss = 0\n",
    "            epoch_grads = []\n",
    "            \n",
    "            for batch_x, batch_y in dataloader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Measure gradient norm\n",
    "                total_norm = 0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        total_norm += p.grad.data.norm(2).item() ** 2\n",
    "                epoch_grads.append(total_norm ** 0.5)\n",
    "                \n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            avg_grad = np.mean(epoch_grads)\n",
    "            \n",
    "            losses.append(avg_loss)\n",
    "            grad_norms.append(avg_grad)\n",
    "            \n",
    "            if (epoch + 1) % 3 == 0:\n",
    "                print(f\"  Epoch {epoch+1}: Loss={avg_loss:.4f}, Grad={avg_grad:.4f}\")\n",
    "        \n",
    "        results[name] = {\n",
    "            'losses': losses,\n",
    "            'grads': grad_norms\n",
    "        }\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    colors = {'Original ResNet': '#e74c3c', 'ResNet V2': '#2ecc71'}\n",
    "    \n",
    "    # Training loss\n",
    "    for name, data in results.items():\n",
    "        ax1.plot(data['losses'], label=name, color=colors[name], \n",
    "                linewidth=2.5, marker='o', markersize=6)\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss Comparison', fontsize=14, weight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient norms\n",
    "    for name, data in results.items():\n",
    "        ax2.plot(data['grads'], label=name, color=colors[name],\n",
    "                linewidth=2.5, marker='s', markersize=6)\n",
    "    \n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Gradient Norm', fontsize=12)\n",
    "    ax2.set_title('Gradient Flow Comparison', fontsize=14, weight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüèÜ Final Results:\")\n",
    "    for name, data in results.items():\n",
    "        print(f\"  {name}: Loss = {data['losses'][-1]:.4f}\")\n",
    "    \n",
    "    print(\"\\nüí° Key Observations:\")\n",
    "    print(\"  ‚úÖ ResNet V2 typically converges faster\")\n",
    "    print(\"  ‚úÖ More stable gradient flow throughout training\")\n",
    "    print(\"  ‚úÖ Better performance on very deep networks\")\n",
    "    print(\"  ‚úÖ Cleaner optimization landscape\")\n",
    "\n",
    "compare_resnet_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285e90f",
   "metadata": {},
   "source": [
    "## Part 5: Ablation Study - What Makes Pre-Activation Work?\n",
    "\n",
    "Let's test different design choices to understand which components matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47876d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation study on pre-activation design\n",
    "def ablation_study():\n",
    "    \"\"\"Test different architectural variants.\"\"\"\n",
    "    \n",
    "    print(\"üî¨ Ablation Study: What Makes Pre-Activation Work?\")\n",
    "    \n",
    "    # Different block variants\n",
    "    class Variant1(nn.Module):\n",
    "        \"\"\"Original: BN-ReLU-Conv-BN-ReLU-Conv\"\"\"\n",
    "        def __init__(self, channels):\n",
    "            super().__init__()\n",
    "            self.bn1 = nn.BatchNorm2d(channels)\n",
    "            self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(channels)\n",
    "            self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out = self.conv1(F.relu(self.bn1(x)))\n",
    "            out = self.conv2(F.relu(self.bn2(out)))\n",
    "            return out + x\n",
    "    \n",
    "    class Variant2(nn.Module):\n",
    "        \"\"\"ReLU-BN-Conv (wrong order)\"\"\"\n",
    "        def __init__(self, channels):\n",
    "            super().__init__()\n",
    "            self.bn1 = nn.BatchNorm2d(channels)\n",
    "            self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(channels)\n",
    "            self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out = self.conv1(self.bn1(F.relu(x)))\n",
    "            out = self.conv2(self.bn2(F.relu(out)))\n",
    "            return out + x\n",
    "    \n",
    "    class Variant3(nn.Module):\n",
    "        \"\"\"Only BN before conv (no ReLU)\"\"\"\n",
    "        def __init__(self, channels):\n",
    "            super().__init__()\n",
    "            self.bn1 = nn.BatchNorm2d(channels)\n",
    "            self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(channels)\n",
    "            self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out = self.conv1(self.bn1(x))\n",
    "            out = self.conv2(self.bn2(out))\n",
    "            return out + x\n",
    "    \n",
    "    variants = {\n",
    "        'Full Pre-Act (BN-ReLU-Conv)': Variant1,\n",
    "        'Wrong Order (ReLU-BN-Conv)': Variant2,\n",
    "        'Only BN (no ReLU)': Variant3,\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüß™ Testing variants on gradient flow...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, BlockClass in variants.items():\n",
    "        # Create simple network\n",
    "        model = nn.Sequential(*[BlockClass(64) for _ in range(10)]).to(device)\n",
    "        \n",
    "        # Test gradient flow\n",
    "        x = torch.randn(1, 64, 32, 32, requires_grad=True).to(device)\n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_norm = x.grad.norm().item()\n",
    "        results[name] = grad_norm\n",
    "        \n",
    "        print(f\"  {name}: Gradient norm = {grad_norm:.6f}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    variant_names = list(results.keys())\n",
    "    grad_values = list(results.values())\n",
    "    \n",
    "    colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "    bars = ax.bar(variant_names, grad_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax.set_ylabel('Gradient Norm', fontsize=12)\n",
    "    ax.set_title('Ablation Study: Impact of Design Choices', fontsize=14, weight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, grad_values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.4f}',\n",
    "                ha='center', va='bottom', fontsize=11, weight='bold')\n",
    "    \n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Key Findings:\")\n",
    "    print(\"  1. ‚úÖ Full pre-activation (BN-ReLU-Conv) works best\")\n",
    "    print(\"  2. ‚ùå Wrong order (ReLU-BN) hurts gradient flow\")\n",
    "    print(\"  3. ‚ö†Ô∏è  Only BN (no ReLU) works but not optimal\")\n",
    "    print(\"  4. üéØ Both BN and ReLU before conv are important!\")\n",
    "\n",
    "ablation_study()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ffe23",
   "metadata": {},
   "source": [
    "## Part 6: Visualizing Identity Mapping Quality\n",
    "\n",
    "Let's visualize how clean the identity mappings are in ResNet V2 compared to original ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359466a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize identity mapping quality\n",
    "def visualize_identity_mapping():\n",
    "    \"\"\"See how well identity mappings are preserved.\"\"\"\n",
    "    \n",
    "    print(\"üîç Analyzing Identity Mapping Quality...\")\n",
    "    \n",
    "    # Create blocks\n",
    "    original_block = OriginalResBlock(64).to(device).eval()\n",
    "    preact_block = PreActBlock(64, 64).to(device).eval()\n",
    "    \n",
    "    # Test with identity-like input\n",
    "    x = torch.randn(1, 64, 32, 32).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        original_out = original_block(x)\n",
    "        preact_out = preact_block(x)\n",
    "    \n",
    "    # Measure how much output differs from input\n",
    "    original_diff = (original_out - x).abs().mean().item()\n",
    "    preact_diff = (preact_out - x).abs().mean().item()\n",
    "    \n",
    "    print(f\"\\nüìä Identity Mapping Quality:\")\n",
    "    print(f\"  Original ResNet: |output - input| = {original_diff:.6f}\")\n",
    "    print(f\"  ResNet V2: |output - input| = {preact_diff:.6f}\")\n",
    "    \n",
    "    # Test with multiple random inputs\n",
    "    num_tests = 100\n",
    "    original_diffs = []\n",
    "    preact_diffs = []\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        x = torch.randn(1, 64, 32, 32).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            original_out = original_block(x)\n",
    "            preact_out = preact_block(x)\n",
    "        \n",
    "        original_diffs.append((original_out - x).abs().mean().item())\n",
    "        preact_diffs.append((preact_out - x).abs().mean().item())\n",
    "    \n",
    "    # Visualize distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    ax1.hist(original_diffs, bins=30, alpha=0.6, color='red', label='Original ResNet', edgecolor='black')\n",
    "    ax1.hist(preact_diffs, bins=30, alpha=0.6, color='green', label='ResNet V2', edgecolor='black')\n",
    "    ax1.set_xlabel('|Output - Input|', fontsize=12)\n",
    "    ax1.set_ylabel('Frequency', fontsize=12)\n",
    "    ax1.set_title('Identity Mapping Deviation', fontsize=14, weight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    ax2.boxplot([original_diffs, preact_diffs], labels=['Original\\nResNet', 'ResNet V2'],\n",
    "                patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "    ax2.set_ylabel('|Output - Input|', fontsize=12)\n",
    "    ax2.set_title('Identity Preservation Comparison', fontsize=14, weight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìà Statistics:\")\n",
    "    print(f\"  Original ResNet - Mean: {np.mean(original_diffs):.6f}, Std: {np.std(original_diffs):.6f}\")\n",
    "    print(f\"  ResNet V2 - Mean: {np.mean(preact_diffs):.6f}, Std: {np.std(preact_diffs):.6f}\")\n",
    "    \n",
    "    print(\"\\nüí° Insight:\")\n",
    "    print(\"  ResNet V2 better preserves input information through identity mapping!\")\n",
    "\n",
    "# Define OriginalResBlock for this demo\n",
    "class OriginalResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "visualize_identity_mapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef23ae",
   "metadata": {},
   "source": [
    "## Part 7: Your Turn to Experiment!\n",
    "\n",
    "Now it's your turn! Try different modifications and experiments with ResNet V2.\n",
    "\n",
    "### Suggested Experiments:\n",
    "\n",
    "1. **Ultra-Deep Networks**: Build a 500+ layer network and test training\n",
    "2. **Hybrid Designs**: Mix pre-activation and post-activation blocks\n",
    "3. **Different Normalizations**: Try Group Norm, Layer Norm instead of Batch Norm\n",
    "4. **Activation Functions**: Test different activations (GELU, Swish, etc.)\n",
    "5. **Skip Connection Variants**: Try different skip connection patterns\n",
    "\n",
    "Use the cell below for your experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment cell\n",
    "def my_resnetv2_experiment():\n",
    "    \"\"\"Design your own ResNet V2 experiment!\"\"\"\n",
    "    \n",
    "    print(\"üî¨ Your Custom ResNet V2 Experiment\")\n",
    "    \n",
    "    # TODO: Design your experiment here!\n",
    "    # Ideas:\n",
    "    # - Test extremely deep networks (1000+ layers)\n",
    "    # - Compare different normalization techniques\n",
    "    # - Experiment with activation functions\n",
    "    # - Test gradient flow at various depths\n",
    "    \n",
    "    # Example: Test ultra-deep network\n",
    "    print(\"\\nüèîÔ∏è Testing ultra-deep ResNet V2...\")\n",
    "    \n",
    "    depths = [100, 200, 500, 1000]\n",
    "    \n",
    "    for depth in depths:\n",
    "        # Create ultra-deep network\n",
    "        blocks = [PreActBlock(64, 64) for _ in range(depth // 2)]\n",
    "        model = nn.Sequential(*blocks).to(device)\n",
    "        \n",
    "        # Test gradient flow\n",
    "        x = torch.randn(1, 64, 32, 32, requires_grad=True).to(device)\n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_norm = x.grad.norm().item()\n",
    "        \n",
    "        print(f\"  {depth} layers: Gradient norm = {grad_norm:.6f}\")\n",
    "        print(f\"    {'‚úÖ Healthy!' if grad_norm > 0.001 else '‚ùå Vanishing'}\")\n",
    "    \n",
    "    print(\"\\nüí° Your turn: Modify this cell to explore ResNet V2!\")\n",
    "\n",
    "# Run your experiment\n",
    "my_resnetv2_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02818391",
   "metadata": {},
   "source": [
    "## Conclusions and Takeaways\n",
    "\n",
    "üéâ **Congratulations!** You've mastered ResNet V2 and the power of pre-activation!\n",
    "\n",
    "### Key Insights Discovered:\n",
    "\n",
    "1. **Pre-Activation Design**: Moving BN and ReLU before convolution creates clean identity paths\n",
    "2. **Perfect Identity Mapping**: `x` flows directly to output without transformation\n",
    "3. **Ultra-Deep Networks**: Enables training 1000+ layer networks successfully\n",
    "4. **Better Optimization**: Cleaner gradients lead to faster, more stable training\n",
    "5. **Simple Change, Big Impact**: Just rearranging layers dramatically improves performance\n",
    "\n",
    "### Why Pre-Activation Matters:\n",
    "\n",
    "- **Clean Gradients**: Direct path for backward propagation\n",
    "- **Information Flow**: Input information preserved throughout network\n",
    "- **Optimization Landscape**: Smoother loss surface, easier to optimize\n",
    "- **Scalability**: Handles extreme depth that original ResNet cannot\n",
    "\n",
    "### The Magic Formula:\n",
    "\n",
    "**ResNet V2 Block**:\n",
    "```python\n",
    "output = Conv(ReLU(BN(Conv(ReLU(BN(x)))))) + x\n",
    "```\n",
    "\n",
    "**Key**: Identity `x` is added AFTER all transformations, with NO final activation!\n",
    "\n",
    "### Modern Impact:\n",
    "\n",
    "ResNet V2's pre-activation design influenced:\n",
    "- üß† Transformer architectures (Pre-LayerNorm)\n",
    "- üé® Diffusion models (U-Nets with pre-activation)\n",
    "- üéØ EfficientNets (optimized block designs)\n",
    "- üöÄ Vision Transformers (residual connections everywhere)\n",
    "\n",
    "### The Research Lesson:\n",
    "\n",
    "Sometimes the best improvements come from **simplifying and perfecting** existing ideas rather than adding complexity. ResNet V2 shows that careful architectural choices matter immensely!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Explore Wide ResNets**: Increasing width vs depth\n",
    "2. **Try ResNeXt**: Grouped convolutions + residuals\n",
    "3. **Study DenseNet**: Connecting all layers\n",
    "4. **Apply to Projects**: Use pre-activation in your own architectures\n",
    "\n",
    "The pre-activation principle: **prepare, transform, connect** - is now a fundamental pattern in deep learning architecture design! üéØüß†‚ú®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
