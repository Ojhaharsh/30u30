{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420cc59d",
   "metadata": {},
   "source": [
    "# Day 9: ResNet - Deep Residual Learning üèóÔ∏è\n",
    "\n",
    "Welcome to Day 9 of 30 Papers in 30 Days!\n",
    "\n",
    "Today we're exploring **ResNet (Residual Networks)** - the architecture that solved the degradation problem and enabled training of networks with 100+ layers. In 2015, ResNet didn't just win ImageNet - it proved that deeper networks could actually be better when built correctly.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **The Degradation Problem**: Why deeper networks performed worse\n",
    "2. **Skip Connections**: The elegant solution that changed everything\n",
    "3. **Residual Learning**: Learning the difference instead of the mapping\n",
    "4. **Implementation**: Building ResNet blocks from scratch\n",
    "5. **Gradient Flow**: How skip connections solve vanishing gradients\n",
    "6. **Modern Impact**: Why every architecture now uses skip connections\n",
    "\n",
    "## The Big Idea (in 30 seconds)\n",
    "\n",
    "**Problem**: Deeper networks should be better, but they performed WORSE (even on training data!)\n",
    "\n",
    "**ResNet Solution**: Add skip connections - `output = F(x) + x`\n",
    "\n",
    "**Result**: Networks can now be 100+ layers deep and actually improve with depth!\n",
    "\n",
    "**Magic**: Skip connections create \"information highways\" that preserve gradient flow\n",
    "\n",
    "Let's dive into the skip connection revolution! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd404bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import our ResNet implementation\n",
    "from implementation import ResNet18, ResNet50, BasicBlock, BottleneckBlock\n",
    "from visualization import ResNetVisualizer\n",
    "from train_minimal import ResNetTrainer, create_synthetic_dataset\n",
    "\n",
    "# Set up device and seeds\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"üî• Using device: {device}\")\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"üéØ Ready to explore residual learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcea455",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Degradation Problem\n",
    "\n",
    "Before ResNet, researchers discovered something puzzling: deeper networks performed WORSE than shallow ones, even on training data! This wasn't overfitting - it was a fundamental optimization problem.\n",
    "\n",
    "Let's demonstrate this degradation problem ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the degradation problem\n",
    "def demonstrate_degradation_problem():\n",
    "    \"\"\"Show that deeper plain networks perform worse.\"\"\"\n",
    "    \n",
    "    print(\"üî¨ Demonstrating the Degradation Problem...\")\n",
    "    \n",
    "    # Create plain networks (no skip connections) of different depths\n",
    "    class PlainNet(nn.Module):\n",
    "        def __init__(self, depth, num_classes=10):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            in_channels = 3\n",
    "            \n",
    "            for i in range(depth):\n",
    "                out_channels = 64 if i < depth//2 else 128\n",
    "                layers.append(nn.Conv2d(in_channels, out_channels, 3, 1, 1))\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = out_channels\n",
    "            \n",
    "            layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "            self.features = nn.Sequential(*layers)\n",
    "            self.fc = nn.Linear(128, num_classes)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    # Test different depths\n",
    "    depths = [10, 20, 30, 40]\n",
    "    results = {}\n",
    "    \n",
    "    # Create simple dataset\n",
    "    print(\"\\nüì¶ Creating synthetic dataset...\")\n",
    "    X = torch.randn(1000, 3, 32, 32)\n",
    "    y = torch.randint(0, 10, (1000,))\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for depth in depths:\n",
    "        print(f\"\\nüèãÔ∏è Training {depth}-layer plain network...\")\n",
    "        model = PlainNet(depth=depth).to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train for a few epochs\n",
    "        losses = []\n",
    "        for epoch in range(5):\n",
    "            epoch_loss = 0\n",
    "            for batch_x, batch_y in dataloader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            losses.append(avg_loss)\n",
    "        \n",
    "        results[depth] = losses\n",
    "        print(f\"  Final loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for depth, losses in results.items():\n",
    "        plt.plot(losses, label=f'{depth} layers', linewidth=2, marker='o')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Degradation Problem: Deeper Networks Perform Worse!')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Key Insight:\")\n",
    "    print(\"  Notice how deeper networks have HIGHER training loss!\")\n",
    "    print(\"  This isn't overfitting - it's an optimization problem\")\n",
    "    print(\"  Skip connections (ResNet) solve this!\")\n",
    "\n",
    "demonstrate_degradation_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822dcc9",
   "metadata": {},
   "source": [
    "## Part 2: The ResNet Solution - Skip Connections\n",
    "\n",
    "ResNet's brilliant insight: instead of learning `H(x)` directly, learn the residual `F(x) = H(x) - x`, then add it back: `H(x) = F(x) + x`\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "**Identity Mapping**: If the optimal function is the identity (do nothing), the network just needs to set `F(x) = 0`, which is much easier than learning `H(x) = x` directly.\n",
    "\n",
    "**Gradient Flow**: Skip connections create a direct path for gradients to flow backward, preventing vanishing gradients.\n",
    "\n",
    "Let's build and visualize residual blocks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore ResNet building blocks\n",
    "def explore_residual_blocks():\n",
    "    \"\"\"Understand how residual blocks work.\"\"\"\n",
    "    \n",
    "    print(\"üèóÔ∏è Exploring Residual Blocks...\")\n",
    "    \n",
    "    # Create a basic residual block\n",
    "    block = BasicBlock(inplanes=64, planes=64, stride=1)\n",
    "    \n",
    "    print(\"\\nüìê Basic Residual Block Structure:\")\n",
    "    print(block)\n",
    "    \n",
    "    # Forward pass to see dimensions\n",
    "    input_tensor = torch.randn(1, 64, 32, 32)\n",
    "    \n",
    "    print(f\"\\nInput shape: {list(input_tensor.shape)}\")\n",
    "    \n",
    "    # Manually trace through the block\n",
    "    identity = input_tensor\n",
    "    \n",
    "    # First conv path\n",
    "    out = block.conv1(input_tensor)\n",
    "    out = block.bn1(out)\n",
    "    out = F.relu(out)\n",
    "    print(f\"After conv1-bn1-relu: {list(out.shape)}\")\n",
    "    \n",
    "    # Second conv path\n",
    "    out = block.conv2(out)\n",
    "    out = block.bn2(out)\n",
    "    print(f\"After conv2-bn2: {list(out.shape)}\")\n",
    "    \n",
    "    # Add skip connection\n",
    "    out += identity\n",
    "    print(f\"After adding skip connection: {list(out.shape)}\")\n",
    "    \n",
    "    # Final activation\n",
    "    out = F.relu(out)\n",
    "    print(f\"Final output: {list(out.shape)}\")\n",
    "    \n",
    "    # Visualize the block\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Block diagram\n",
    "    ax1.text(0.5, 0.9, 'Input x', ha='center', fontsize=12, weight='bold')\n",
    "    ax1.arrow(0.5, 0.85, 0, -0.1, head_width=0.05, head_length=0.03, fc='black')\n",
    "    \n",
    "    # Main path\n",
    "    ax1.add_patch(plt.Rectangle((0.3, 0.55), 0.4, 0.15, fill=True, facecolor='lightblue', edgecolor='black'))\n",
    "    ax1.text(0.5, 0.625, 'Conv-BN-ReLU', ha='center', fontsize=10)\n",
    "    ax1.arrow(0.5, 0.55, 0, -0.05, head_width=0.05, head_length=0.02, fc='black')\n",
    "    \n",
    "    ax1.add_patch(plt.Rectangle((0.3, 0.35), 0.4, 0.15, fill=True, facecolor='lightblue', edgecolor='black'))\n",
    "    ax1.text(0.5, 0.425, 'Conv-BN', ha='center', fontsize=10)\n",
    "    ax1.arrow(0.5, 0.35, 0, -0.05, head_width=0.05, head_length=0.02, fc='black')\n",
    "    \n",
    "    # Skip connection\n",
    "    ax1.arrow(0.15, 0.9, 0, -0.55, head_width=0.05, head_length=0.02, fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "    ax1.text(0.08, 0.65, 'Skip', ha='center', fontsize=10, color='red', weight='bold')\n",
    "    \n",
    "    # Addition\n",
    "    ax1.add_patch(plt.Circle((0.5, 0.25), 0.05, fill=True, facecolor='yellow', edgecolor='black'))\n",
    "    ax1.text(0.5, 0.25, '+', ha='center', va='center', fontsize=14, weight='bold')\n",
    "    ax1.arrow(0.5, 0.2, 0, -0.05, head_width=0.05, head_length=0.02, fc='black')\n",
    "    \n",
    "    # Final ReLU\n",
    "    ax1.add_patch(plt.Rectangle((0.35, 0.05), 0.3, 0.1, fill=True, facecolor='lightgreen', edgecolor='black'))\n",
    "    ax1.text(0.5, 0.1, 'ReLU', ha='center', fontsize=10)\n",
    "    \n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Basic Residual Block Architecture')\n",
    "    \n",
    "    # Gradient flow diagram\n",
    "    ax2.text(0.5, 0.1, 'Output', ha='center', fontsize=12, weight='bold')\n",
    "    ax2.arrow(0.5, 0.15, 0, 0.1, head_width=0.05, head_length=0.03, fc='blue')\n",
    "    \n",
    "    ax2.add_patch(plt.Rectangle((0.35, 0.3), 0.3, 0.1, fill=True, facecolor='lightgreen', edgecolor='black'))\n",
    "    ax2.text(0.5, 0.35, 'ReLU', ha='center', fontsize=10)\n",
    "    ax2.arrow(0.5, 0.4, 0, 0.05, head_width=0.05, head_length=0.02, fc='blue')\n",
    "    \n",
    "    # Gradient paths\n",
    "    ax2.arrow(0.5, 0.5, 0, 0.15, head_width=0.05, head_length=0.02, fc='blue')\n",
    "    ax2.text(0.55, 0.575, 'Residual\\nGradient', ha='left', fontsize=9, color='blue')\n",
    "    \n",
    "    ax2.arrow(0.15, 0.5, 0, 0.35, head_width=0.05, head_length=0.02, fc='red', linestyle='--', linewidth=2)\n",
    "    ax2.text(0.05, 0.675, 'Direct\\nGradient', ha='center', fontsize=9, color='red', weight='bold')\n",
    "    \n",
    "    ax2.text(0.5, 0.9, 'Input', ha='center', fontsize=12, weight='bold')\n",
    "    \n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Gradient Flow (Backward Pass)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    print(\"  ‚úÖ Skip connection creates a 'gradient highway'\")\n",
    "    print(\"  ‚úÖ Residual function F(x) only needs to learn small adjustments\")\n",
    "    print(\"  ‚úÖ Identity mapping is trivial: just set F(x) = 0\")\n",
    "    print(\"  ‚úÖ Gradients flow backward through both paths\")\n",
    "\n",
    "explore_residual_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d687592",
   "metadata": {},
   "source": [
    "## Part 3: Building Complete ResNet Architectures\n",
    "\n",
    "Now let's build complete ResNet models and understand how they scale to different depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different ResNet architectures\n",
    "def compare_resnet_architectures():\n",
    "    \"\"\"Compare different ResNet variants.\"\"\"\n",
    "    \n",
    "    print(\"üèõÔ∏è Comparing ResNet Architectures...\")\n",
    "    \n",
    "    models = {\n",
    "        'ResNet-18': ResNet18(num_classes=10),\n",
    "        'ResNet-50': ResNet50(num_classes=10),\n",
    "    }\n",
    "    \n",
    "    # Analyze each model\n",
    "    print(\"\\nüìä Architecture Comparison:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  Model size: {total_params * 4 / 1024**2:.1f} MB (float32)\")\n",
    "        \n",
    "        # Count residual blocks\n",
    "        num_blocks = 0\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, (BasicBlock, BottleneckBlock)):\n",
    "                num_blocks += 1\n",
    "        print(f\"  Residual blocks: {num_blocks}\")\n",
    "    \n",
    "    # Visualize depth comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    model_names = list(models.keys())\n",
    "    param_counts = [sum(p.numel() for p in m.parameters()) / 1e6 for m in models.values()]\n",
    "    \n",
    "    colors = ['#3498db', '#e74c3c']\n",
    "    bars = ax.bar(model_names, param_counts, color=colors, alpha=0.7)\n",
    "    \n",
    "    ax.set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "    ax.set_title('ResNet Architecture Comparison', fontsize=14, weight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, param_counts):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{count:.1f}M',\n",
    "                ha='center', va='bottom', fontsize=11, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test forward pass\n",
    "    print(\"\\nüß™ Testing Forward Pass:\")\n",
    "    test_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(test_input)\n",
    "        \n",
    "        print(f\"  {name}: Input {list(test_input.shape)} ‚Üí Output {list(output.shape)}\")\n",
    "\n",
    "compare_resnet_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e53a2d",
   "metadata": {},
   "source": [
    "## Part 4: Training ResNet and Observing Gradient Flow\n",
    "\n",
    "Let's train a ResNet and monitor how gradients flow through the network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84288857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ResNet and monitor gradient flow\n",
    "def train_and_monitor_resnet():\n",
    "    \"\"\"Train ResNet and visualize gradient flow.\"\"\"\n",
    "    \n",
    "    print(\"üéì Training ResNet with Gradient Monitoring...\")\n",
    "    \n",
    "    # Create smaller ResNet for faster training\n",
    "    model = ResNet18(num_classes=10).to(device)\n",
    "    \n",
    "    # Create synthetic dataset\n",
    "    print(\"\\nüì¶ Creating dataset...\")\n",
    "    train_dataset, test_dataset = create_synthetic_dataset(\n",
    "        num_classes=10,\n",
    "        samples_per_class=100,\n",
    "        image_size=224\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "    # Storage for metrics\n",
    "    train_losses = []\n",
    "    gradient_norms = []\n",
    "    \n",
    "    print(\"\\nüöÄ Training for 5 epochs...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        epoch_loss = 0\n",
    "        epoch_grad_norms = []\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Collect gradient norms\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            epoch_grad_norms.append(total_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        avg_grad_norm = np.mean(epoch_grad_norms)\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        gradient_norms.append(avg_grad_norm)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/5: Loss = {avg_loss:.4f}, Grad Norm = {avg_grad_norm:.4f}\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    # Training loss\n",
    "    ax1.plot(epochs, train_losses, 'b-o', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "    ax1.set_title('ResNet Training Loss', fontsize=14, weight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient norms\n",
    "    ax2.plot(epochs, gradient_norms, 'r-s', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Gradient Norm', fontsize=12)\n",
    "    ax2.set_title('Gradient Flow Health', fontsize=14, weight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "    print(f\"Final loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"Gradient norms remained healthy throughout training!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "trained_resnet = train_and_monitor_resnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f52b0",
   "metadata": {},
   "source": [
    "## Part 5: Visualizing What ResNet Learns\n",
    "\n",
    "Let's visualize the features and activations in our trained ResNet to understand what each layer learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ResNet features\n",
    "def visualize_resnet_features(model):\n",
    "    \"\"\"Visualize what ResNet layers learn.\"\"\"\n",
    "    \n",
    "    print(\"üëÅÔ∏è Visualizing ResNet Features...\")\n",
    "    \n",
    "    # Create a test image\n",
    "    test_image = torch.randn(1, 3, 224, 224)\n",
    "    \n",
    "    # Add some structure\n",
    "    test_image[0, 0, 80:140, 80:140] = 2.0  # Red square\n",
    "    test_image[0, 1, 100:160, 100:160] = 2.0  # Green square\n",
    "    test_image[0, 2, 90:150, 120:180] = 2.0  # Blue rectangle\n",
    "    test_image = torch.clamp(test_image, 0, 1)\n",
    "    \n",
    "    # Show test image\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.imshow(test_image[0].permute(1, 2, 0))\n",
    "    ax.set_title('Test Image', fontsize=14, weight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract features at different layers\n",
    "    model = model.cpu().eval()\n",
    "    activations = {}\n",
    "    \n",
    "    def get_activation(name):\n",
    "        def hook(module, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    model.layer1[0].register_forward_hook(get_activation('layer1'))\n",
    "    model.layer2[0].register_forward_hook(get_activation('layer2'))\n",
    "    model.layer3[0].register_forward_hook(get_activation('layer3'))\n",
    "    model.layer4[0].register_forward_hook(get_activation('layer4'))\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(test_image)\n",
    "    \n",
    "    # Visualize feature maps at different layers\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    for idx, (layer_name, activation) in enumerate(activations.items()):\n",
    "        # Select first 8 channels\n",
    "        num_channels = min(8, activation.shape[1])\n",
    "        \n",
    "        for i in range(num_channels):\n",
    "            if i < 4:\n",
    "                ax = axes[0, i] if idx == 0 else axes[0, i]\n",
    "            else:\n",
    "                ax = axes[1, i-4] if idx == 0 else axes[1, i-4]\n",
    "            \n",
    "            if idx == 0:  # Only plot for first layer to keep it simple\n",
    "                feature_map = activation[0, i].numpy()\n",
    "                im = ax.imshow(feature_map, cmap='viridis')\n",
    "                ax.set_title(f'{layer_name} Ch{i}', fontsize=10)\n",
    "                ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Feature Maps at Different ResNet Layers', fontsize=16, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Observations:\")\n",
    "    print(\"  ‚Ä¢ Early layers (layer1): Detect edges and simple patterns\")\n",
    "    print(\"  ‚Ä¢ Middle layers (layer2-3): Detect textures and shapes\")\n",
    "    print(\"  ‚Ä¢ Deep layers (layer4): Detect high-level features\")\n",
    "    print(\"  ‚Ä¢ Skip connections preserve information across all layers!\")\n",
    "\n",
    "visualize_resnet_features(trained_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd195151",
   "metadata": {},
   "source": [
    "## Part 6: ResNet vs Plain Network Comparison\n",
    "\n",
    "Let's directly compare ResNet with skip connections against a plain network without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb030dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ResNet vs Plain Network\n",
    "def compare_resnet_vs_plain():\n",
    "    \"\"\"Compare learning dynamics of ResNet vs plain network.\"\"\"\n",
    "    \n",
    "    print(\"‚öîÔ∏è ResNet vs Plain Network Showdown...\")\n",
    "    \n",
    "    # Create both architectures\n",
    "    class PlainDeepNet(nn.Module):\n",
    "        def __init__(self, num_classes=10):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.maxpool = nn.MaxPool2d(3, 2, 1)\n",
    "            \n",
    "            # Plain layers (no skip connections)\n",
    "            self.plain_layers = nn.Sequential(\n",
    "                nn.Conv2d(64, 64, 3, 1, 1, bias=False), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, 3, 1, 1, bias=False), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, 3, 2, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, 3, 1, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            )\n",
    "            \n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(128, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            x = self.maxpool(x)\n",
    "            x = self.plain_layers(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    # Create models\n",
    "    resnet_model = ResNet18(num_classes=10).to(device)\n",
    "    plain_model = PlainDeepNet(num_classes=10).to(device)\n",
    "    \n",
    "    # Simple dataset\n",
    "    X = torch.randn(500, 3, 224, 224)\n",
    "    y = torch.randint(0, 10, (500,))\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train both models\n",
    "    models = {'ResNet-18': resnet_model, 'Plain Network': plain_model}\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüèãÔ∏è Training {name}...\")\n",
    "        \n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        losses = []\n",
    "        grad_norms = []\n",
    "        \n",
    "        for epoch in range(10):\n",
    "            epoch_loss = 0\n",
    "            epoch_grads = []\n",
    "            \n",
    "            for batch_x, batch_y in dataloader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient norm\n",
    "                total_norm = 0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        total_norm += p.grad.data.norm(2).item() ** 2\n",
    "                epoch_grads.append(total_norm ** 0.5)\n",
    "                \n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            avg_grad = np.mean(epoch_grads)\n",
    "            \n",
    "            losses.append(avg_loss)\n",
    "            grad_norms.append(avg_grad)\n",
    "            \n",
    "            if (epoch + 1) % 2 == 0:\n",
    "                print(f\"  Epoch {epoch+1}: Loss = {avg_loss:.4f}, Grad Norm = {avg_grad:.4f}\")\n",
    "        \n",
    "        results[name] = {'losses': losses, 'grads': grad_norms}\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    colors = {'ResNet-18': '#2ecc71', 'Plain Network': '#e74c3c'}\n",
    "    \n",
    "    # Loss comparison\n",
    "    for name, data in results.items():\n",
    "        ax1.plot(data['losses'], label=name, linewidth=2, \n",
    "                color=colors[name], marker='o', markersize=6)\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss: ResNet vs Plain Network', fontsize=14, weight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient comparison\n",
    "    for name, data in results.items():\n",
    "        ax2.plot(data['grads'], label=name, linewidth=2,\n",
    "                color=colors[name], marker='s', markersize=6)\n",
    "    \n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Gradient Norm', fontsize=12)\n",
    "    ax2.set_title('Gradient Flow: ResNet vs Plain Network', fontsize=14, weight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüèÜ Results:\")\n",
    "    print(f\"  ResNet-18 Final Loss: {results['ResNet-18']['losses'][-1]:.4f}\")\n",
    "    print(f\"  Plain Network Final Loss: {results['Plain Network']['losses'][-1]:.4f}\")\n",
    "    print(\"\\nüí° Key Takeaway:\")\n",
    "    print(\"  Skip connections enable better optimization and gradient flow!\")\n",
    "    print(\"  ResNet trains more stably and achieves better performance!\")\n",
    "\n",
    "compare_resnet_vs_plain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf352842",
   "metadata": {},
   "source": [
    "## Part 7: Your Turn to Experiment!\n",
    "\n",
    "Now it's your turn to explore ResNet! Try different modifications and experiments.\n",
    "\n",
    "### Suggested Experiments:\n",
    "\n",
    "1. **Depth Scaling**: Test ResNet-34, ResNet-101 and compare performance\n",
    "2. **Skip Connection Ablation**: Remove skip connections and see what breaks\n",
    "3. **Different Blocks**: Compare BasicBlock vs BottleneckBlock\n",
    "4. **Activation Functions**: Try different activations in residual blocks\n",
    "5. **Width vs Depth**: Make networks wider vs deeper\n",
    "\n",
    "Use the cells below for your experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9b074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment cell\n",
    "def my_resnet_experiment():\n",
    "    \"\"\"Design your own ResNet experiment!\"\"\"\n",
    "    \n",
    "    print(\"üî¨ Your Custom ResNet Experiment\")\n",
    "    \n",
    "    # TODO: Design your experiment here!\n",
    "    # Ideas:\n",
    "    # - Test very deep networks (200+ layers)\n",
    "    # - Try different skip connection patterns\n",
    "    # - Experiment with block designs\n",
    "    # - Compare gradient flow at different depths\n",
    "    \n",
    "    # Example: Test depth scaling\n",
    "    depths = [18, 34, 50]\n",
    "    \n",
    "    print(\"\\nüìä Testing different ResNet depths...\")\n",
    "    \n",
    "    for depth in depths:\n",
    "        if depth == 18:\n",
    "            model = ResNet18(num_classes=10)\n",
    "        elif depth == 50:\n",
    "            model = ResNet50(num_classes=10)\n",
    "        else:\n",
    "            print(f\"  ResNet-{depth} not implemented in this example\")\n",
    "            continue\n",
    "        \n",
    "        param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "        print(f\"  ResNet-{depth}: {param_count:.1f}M parameters\")\n",
    "    \n",
    "    print(\"\\nüí° Your turn: Modify this cell to create your own experiments!\")\n",
    "\n",
    "# Run your experiment\n",
    "my_resnet_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46754fb9",
   "metadata": {},
   "source": [
    "## Conclusions and Takeaways\n",
    "\n",
    "üéâ **Congratulations!** You've completed an in-depth exploration of ResNet and residual learning.\n",
    "\n",
    "### Key Insights Discovered:\n",
    "\n",
    "1. **Degradation Problem**: Deeper plain networks performed worse (not overfitting!)\n",
    "2. **Skip Connections**: `output = F(x) + x` creates information highways\n",
    "3. **Residual Learning**: Learning adjustments is easier than learning mappings\n",
    "4. **Gradient Flow**: Skip connections solve vanishing gradients\n",
    "5. **Scalability**: Networks can now be 100+ layers deep\n",
    "6. **Universal Pattern**: Every modern architecture uses skip connections\n",
    "\n",
    "### Why ResNet Changed Everything:\n",
    "\n",
    "- **Enabled Depth**: Proved that deeper networks ARE better when built correctly\n",
    "- **Solved Optimization**: Skip connections address both vanishing gradients and degradation\n",
    "- **Universal Principle**: Inspired skip connections in transformers, GANs, and more\n",
    "- **Practical Impact**: Powers most production computer vision systems today\n",
    "\n",
    "### Modern Applications:\n",
    "\n",
    "Every time you use:\n",
    "- üì± Phone camera features (object detection, portrait mode)\n",
    "- üöó Autonomous vehicles (scene understanding)\n",
    "- üè• Medical imaging (diagnosis assistance)\n",
    "- üéÆ Video games (real-time graphics enhancement)\n",
    "- üì∫ Content recommendation (image understanding)\n",
    "\n",
    "You're benefiting from ResNet's skip connection innovation!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Explore ResNet V2**: Pre-activation improves on the original\n",
    "2. **Try ResNeXt**: Grouped convolutions for efficiency\n",
    "3. **Build DenseNet**: Even more connections!\n",
    "4. **Apply to Tasks**: Use ResNet backbones for segmentation, detection\n",
    "\n",
    "The skip connection revolution shows that sometimes the simplest ideas - adding just one connection - can unlock entirely new possibilities! üöÄüß†‚ú®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
