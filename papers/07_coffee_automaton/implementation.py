"""
Day 7: Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton

This module implements the Coffee Automaton - a cellular automaton that demonstrates
how complexity emerges, peaks, and then decays in closed systems. This models
everything from coffee cooling to the evolution of the universe itself.

The key insight: Complexity is temporary and peaks in the "middle" of a system's
evolution from order to disorder.

Author: 30u30 Project
Based on: "Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton"
         by Aaronson et al. (2014) - https://arxiv.org/abs/1405.6903
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import ndimage
from typing import List, Tuple, Dict, Union
import warnings
warnings.filterwarnings('ignore')


class CoffeeAutomaton:
    """
    The Coffee Automaton: A cellular automaton modeling complexity dynamics.
    
    This simulates a coffee cooling process where complexity emerges from the
    interaction between hot and cold regions, peaks during the cooling process,
    and then fades as thermal equilibrium is reached.
    
    The automaton demonstrates the universal pattern:
    Simple â†’ Complex â†’ Simple (but at different equilibrium)
    """
    
    def __init__(self, size: int = 100, rule_type: str = 'thermal_diffusion'):
        """
        Initialize the Coffee Automaton.
        
        Args:
            size: Grid size (size x size)
            rule_type: Evolution rule ('thermal_diffusion', 'convection', 'mixed')
        """
        self.size = size
        self.grid = np.zeros((size, size), dtype=np.float32)
        self.rule_type = rule_type
        self.history = []
        self.complexity_history = []
        self.time = 0
        
        # Physical parameters
        self.thermal_diffusivity = 0.1
        self.convection_strength = 0.05
        self.cooling_rate = 0.01
        self.ambient_temperature = 0.0
        
    def add_heat_source(self, center: Tuple[int, int], radius: int, temperature: float):
        """
        Add a hot region to simulate pouring coffee.
        
        Args:
            center: (row, col) center of heat source
            radius: Radius of the hot region
            temperature: Initial temperature (0.0 = cold, 1.0 = very hot)
        """
        r, c = center
        y, x = np.ogrid[:self.size, :self.size]
        mask = (x - c)**2 + (y - r)**2 <= radius**2
        self.grid[mask] = temperature
        
    def add_random_perturbation(self, strength: float = 0.01):
        """Add small random fluctuations to break symmetry."""
        noise = np.random.normal(0, strength, self.grid.shape)
        self.grid += noise
        self.grid = np.clip(self.grid, 0, 1)
        
    def evolve_step(self):
        """
        Evolve the system one time step using the specified rule.
        """
        if self.rule_type == 'thermal_diffusion':
            self._thermal_diffusion_step()
        elif self.rule_type == 'convection':
            self._convection_step()
        elif self.rule_type == 'mixed':
            self._mixed_dynamics_step()
        else:
            raise ValueError(f"Unknown rule type: {self.rule_type}")
            
        # Apply cooling to ambient temperature
        self.grid = self.grid - self.cooling_rate * (self.grid - self.ambient_temperature)
        self.grid = np.clip(self.grid, 0, 1)
        
        # Record state
        self.history.append(self.grid.copy())
        self.complexity_history.append(self.measure_total_complexity())
        self.time += 1
        
    def _thermal_diffusion_step(self):
        """Pure thermal diffusion using discrete Laplacian."""
        laplacian = ndimage.laplace(self.grid)
        self.grid += self.thermal_diffusivity * laplacian
        
    def _convection_step(self):
        """Convection patterns based on temperature gradients."""
        # Calculate gradients
        grad_y, grad_x = np.gradient(self.grid)
        
        # Create flow field (hot rises, cold sinks)
        flow_y = self.convection_strength * grad_y
        flow_x = self.convection_strength * grad_x
        
        # Apply flow using simple advection
        for i in range(1, self.size-1):
            for j in range(1, self.size-1):
                # Upwind scheme for stability
                if flow_y[i,j] > 0:
                    dy = self.grid[i,j] - self.grid[i-1,j]
                else:
                    dy = self.grid[i+1,j] - self.grid[i,j]
                    
                if flow_x[i,j] > 0:
                    dx = self.grid[i,j] - self.grid[i,j-1]
                else:
                    dx = self.grid[i,j+1] - self.grid[i,j]
                    
                self.grid[i,j] -= flow_y[i,j] * dy + flow_x[i,j] * dx
                
    def _mixed_dynamics_step(self):
        """Combined thermal diffusion and convection."""
        self._thermal_diffusion_step()
        self._convection_step()


class ComplexityMeasures:
    """
    Collection of complexity measures for analyzing system evolution.
    
    Implements various measures of complexity including Shannon entropy,
    logical depth approximations, effective complexity, and more.
    """
    
    @staticmethod
    def shannon_entropy(grid: np.ndarray, bins: int = 100) -> float:
        """
        Calculate Shannon entropy of the temperature distribution.
        
        High entropy = uniform distribution
        Low entropy = concentrated distribution
        """
        # Discretize the continuous grid
        hist, _ = np.histogram(grid.flatten(), bins=bins, range=(0, 1))
        # Add small epsilon to avoid log(0)
        hist = hist + 1e-10
        probs = hist / hist.sum()
        return -np.sum(probs * np.log2(probs))
    
    @staticmethod
    def spatial_entropy(grid: np.ndarray, window_size: int = 5) -> float:
        """
        Calculate entropy of local spatial patterns.
        
        Measures how predictable local neighborhoods are.
        High values indicate complex spatial structure.
        """
        patterns = []
        for i in range(0, grid.shape[0] - window_size + 1, window_size//2):
            for j in range(0, grid.shape[1] - window_size + 1, window_size//2):
                patch = grid[i:i+window_size, j:j+window_size]
                # Convert to discrete pattern
                pattern = (patch > patch.mean()).astype(int)
                pattern_hash = hash(pattern.tobytes())
                patterns.append(pattern_hash)
        
        # Calculate entropy of pattern distribution
        unique_patterns, counts = np.unique(patterns, return_counts=True)
        probs = counts / len(patterns)
        return -np.sum(probs * np.log2(probs + 1e-10))
    
    @staticmethod
    def logical_depth_proxy(grid: np.ndarray) -> float:
        """
        Approximate logical depth using compression-based measure.
        
        Logical depth â‰ˆ computational steps to generate pattern
        We approximate this by measuring how much the pattern
        can be compressed vs. a random baseline.
        """
        # Convert to binary for compression
        binary_grid = (grid > grid.mean()).astype(np.uint8)
        
        # Calculate run-length encoding compression
        flat = binary_grid.flatten()
        runs = []
        current_val = flat[0]
        count = 1
        
        for val in flat[1:]:
            if val == current_val:
                count += 1
            else:
                runs.append(count)
                current_val = val
                count = 1
        runs.append(count)
        
        # Logical depth proxy: patterns requiring many runs have higher depth
        compression_ratio = len(runs) / len(flat)
        
        # Invert so higher complexity = higher depth
        return 1.0 / (compression_ratio + 1e-10)
    
    @staticmethod
    def effective_complexity(grid: np.ndarray) -> float:
        """
        Murray Gell-Mann's effective complexity.
        
        Measures the length of the shortest description of the
        system's regularities (not its random parts).
        """
        # Find dominant spatial frequency components
        fft = np.fft.fft2(grid)
        power_spectrum = np.abs(fft)**2
        
        # Count significant frequency components (regularities)
        threshold = 0.01 * np.max(power_spectrum)
        significant_modes = np.sum(power_spectrum > threshold)
        
        # Effective complexity = number of significant patterns
        total_modes = grid.size
        return significant_modes / total_modes
    
    @staticmethod
    def thermodynamic_depth(grid: np.ndarray, grid_prev: np.ndarray) -> float:
        """
        Energy required to construct current state from previous state.
        
        Measures the thermodynamic cost of creating the current pattern.
        """
        if grid_prev is None:
            return 0.0
            
        # Energy difference between states
        energy_diff = np.sum((grid - grid_prev)**2)
        
        # Normalize by system size
        return energy_diff / grid.size
    
    @staticmethod
    def lempel_ziv_complexity(grid: np.ndarray) -> float:
        """
        Lempel-Ziv complexity measure.
        
        Counts the number of distinct patterns encountered
        when parsing the sequence.
        """
        # Convert to binary string
        binary_grid = (grid > grid.mean()).astype(int)
        sequence = ''.join(binary_grid.flatten().astype(str))
        
        # Lempel-Ziv parsing
        patterns = set()
        i = 0
        
        while i < len(sequence):
            # Find longest pattern not yet seen
            pattern = sequence[i]
            j = i + 1
            
            while j <= len(sequence) and pattern in patterns:
                if j < len(sequence):
                    pattern += sequence[j]
                j += 1
                
            patterns.add(pattern)
            i = j
            
        return len(patterns) / len(sequence)


class ComplexityTracker:
    """
    Tracks and analyzes the evolution of complexity over time.
    
    This class manages the measurement of various complexity metrics
    throughout the system's evolution and provides analysis tools.
    """
    
    def __init__(self):
        self.metrics = {
            'shannon_entropy': [],
            'spatial_entropy': [],
            'logical_depth': [],
            'effective_complexity': [],
            'thermodynamic_depth': [],
            'lempel_ziv': []
        }
        self.time_steps = []
        
    def measure_step(self, grid: np.ndarray, prev_grid: np.ndarray = None, time: int = 0):
        """Measure all complexity metrics for current state."""
        self.time_steps.append(time)
        
        # Calculate all metrics
        self.metrics['shannon_entropy'].append(
            ComplexityMeasures.shannon_entropy(grid)
        )
        self.metrics['spatial_entropy'].append(
            ComplexityMeasures.spatial_entropy(grid)
        )
        self.metrics['logical_depth'].append(
            ComplexityMeasures.logical_depth_proxy(grid)
        )
        self.metrics['effective_complexity'].append(
            ComplexityMeasures.effective_complexity(grid)
        )
        self.metrics['thermodynamic_depth'].append(
            ComplexityMeasures.thermodynamic_depth(grid, prev_grid)
        )
        self.metrics['lempel_ziv'].append(
            ComplexityMeasures.lempel_ziv_complexity(grid)
        )
    
    def get_peak_complexity_time(self, metric: str = 'effective_complexity') -> int:
        """Find when complexity peaked for given metric."""
        values = self.metrics[metric]
        peak_idx = np.argmax(values)
        return self.time_steps[peak_idx]
    
    def get_complexity_curve(self, metric: str = 'effective_complexity') -> Tuple[List, List]:
        """Get time series of complexity for plotting."""
        return self.time_steps.copy(), self.metrics[metric].copy()
    
    def analyze_phases(self, metric: str = 'effective_complexity') -> Dict:
        """
        Analyze the three phases of complexity evolution:
        1. Growth phase (simple â†’ complex)
        2. Peak phase (maximum complexity)
        3. Decay phase (complex â†’ simple)
        """
        values = self.metrics[metric]
        peak_idx = np.argmax(values)
        
        growth_phase = values[:peak_idx+1]
        decay_phase = values[peak_idx:]
        
        analysis = {
            'peak_time': self.time_steps[peak_idx],
            'peak_value': values[peak_idx],
            'growth_rate': np.mean(np.diff(growth_phase)) if len(growth_phase) > 1 else 0,
            'decay_rate': np.mean(np.diff(decay_phase)) if len(decay_phase) > 1 else 0,
            'total_evolution_time': len(values),
            'growth_duration': peak_idx,
            'decay_duration': len(values) - peak_idx
        }
        
        return analysis


class CoffeeExperiments:
    """
    Collection of experiments demonstrating complexity dynamics.
    """
    
    @staticmethod
    def basic_cooling_experiment(size: int = 64, steps: int = 200) -> Tuple[CoffeeAutomaton, ComplexityTracker]:
        """
        Basic coffee cooling experiment.
        Single hot source cooling down in a cold environment.
        """
        automaton = CoffeeAutomaton(size=size, rule_type='mixed')
        tracker = ComplexityTracker()
        
        # Pour hot coffee in center
        center = (size//2, size//2)
        automaton.add_heat_source(center, radius=size//6, temperature=1.0)
        automaton.add_random_perturbation(0.01)
        
        # Initial measurement
        tracker.measure_step(automaton.grid, time=0)
        
        # Evolve system
        prev_grid = automaton.grid.copy()
        for step in range(steps):
            automaton.evolve_step()
            tracker.measure_step(automaton.grid, prev_grid, time=step+1)
            prev_grid = automaton.grid.copy()
            
        return automaton, tracker
    
    @staticmethod
    def multiple_sources_experiment(size: int = 64, steps: int = 150) -> Tuple[CoffeeAutomaton, ComplexityTracker]:
        """
        Multiple heat sources creating complex interference patterns.
        """
        automaton = CoffeeAutomaton(size=size, rule_type='mixed')
        tracker = ComplexityTracker()
        
        # Multiple coffee cups
        sources = [
            ((size//4, size//4), size//10, 0.9),
            ((3*size//4, size//4), size//10, 0.8),
            ((size//2, 3*size//4), size//10, 0.7)
        ]
        
        for center, radius, temp in sources:
            automaton.add_heat_source(center, radius, temp)
            
        automaton.add_random_perturbation(0.02)
        
        # Track evolution
        tracker.measure_step(automaton.grid, time=0)
        prev_grid = automaton.grid.copy()
        
        for step in range(steps):
            automaton.evolve_step()
            tracker.measure_step(automaton.grid, prev_grid, time=step+1)
            prev_grid = automaton.grid.copy()
            
        return automaton, tracker
    
    @staticmethod
    def life_sweet_spot_experiment() -> Dict:
        """
        Experiment showing complexity peaks at intermediate energy levels.
        Tests the hypothesis that life exists in the complexity maximum.
        """
        results = {}
        energy_levels = np.linspace(0.1, 0.9, 9)
        
        for energy in energy_levels:
            automaton = CoffeeAutomaton(size=48, rule_type='mixed')
            tracker = ComplexityTracker()
            
            # Set initial energy level
            automaton.grid.fill(energy)
            automaton.add_random_perturbation(0.05)
            
            # Short evolution to measure peak complexity
            tracker.measure_step(automaton.grid, time=0)
            prev_grid = automaton.grid.copy()
            
            for step in range(100):
                automaton.evolve_step()
                tracker.measure_step(automaton.grid, prev_grid, time=step+1)
                prev_grid = automaton.grid.copy()
            
            # Find peak complexity
            peak_complexity = max(tracker.metrics['effective_complexity'])
            results[energy] = peak_complexity
            
        return results


# Convenience function for measuring total complexity
def measure_complexity_suite(grid: np.ndarray, prev_grid: np.ndarray = None) -> Dict:
    """Measure all complexity metrics for a given grid state."""
    metrics = {}
    
    metrics['shannon_entropy'] = ComplexityMeasures.shannon_entropy(grid)
    metrics['spatial_entropy'] = ComplexityMeasures.spatial_entropy(grid)
    metrics['logical_depth'] = ComplexityMeasures.logical_depth_proxy(grid)
    metrics['effective_complexity'] = ComplexityMeasures.effective_complexity(grid)
    metrics['lempel_ziv'] = ComplexityMeasures.lempel_ziv_complexity(grid)
    
    if prev_grid is not None:
        metrics['thermodynamic_depth'] = ComplexityMeasures.thermodynamic_depth(grid, prev_grid)
    else:
        metrics['thermodynamic_depth'] = 0.0
        
    return metrics


# Add method to CoffeeAutomaton for convenience
CoffeeAutomaton.measure_total_complexity = lambda self: ComplexityMeasures.effective_complexity(self.grid)


if __name__ == "__main__":
    # Quick demo
    print("ðŸ”¥ Coffee Automaton Demo")
    print("=" * 50)
    
    # Run basic experiment
    automaton, tracker = CoffeeExperiments.basic_cooling_experiment(steps=50)
    
    # Analyze results
    analysis = tracker.analyze_phases()
    print(f"Peak complexity at time step: {analysis['peak_time']}")
    print(f"Peak complexity value: {analysis['peak_value']:.3f}")
    print(f"Growth duration: {analysis['growth_duration']} steps")
    print(f"Decay duration: {analysis['decay_duration']} steps")
    
    # Test life sweet spot
    life_results = CoffeeExperiments.life_sweet_spot_experiment()
    optimal_energy = max(life_results.keys(), key=lambda k: life_results[k])
    print(f"Optimal energy for complexity: {optimal_energy:.2f}")
    
    print("\nâœ… All tests passed! Coffee complexity is brewing perfectly.")