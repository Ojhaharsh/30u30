# Day 8: AlexNet - The Deep Learning Revolution

> *"ImageNet Classification with Deep Convolutional Neural Networks"* - Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton (2012)

**ğŸ“– Original Paper:** https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html

**â±ï¸ Time to Complete:** 4-5 hours

**ğŸ¯ What You'll Learn:**
- How AlexNet sparked the deep learning revolution
- Why ReLU activation changed everything
- The power of dropout for regularization
- GPU parallelization techniques
- Data augmentation strategies that still work today

---

## ğŸ§  The Big Idea

**In one sentence:** AlexNet proved that deep convolutional neural networks trained on GPUs with ReLU activations and dropout could dramatically outperform traditional computer vision methods, launching the deep learning era.

### The ImageNet Moment

It's September 2012. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) results are announced:

**Traditional Computer Vision (2nd place):** 26.2% error rate  
**AlexNet (1st place):** 15.3% error rate

**A 10.9 percentage point gap.** In computer vision competitions, improvements are usually measured in fractions of a percent. This wasn't an improvementâ€”it was a **revolution**.

### The Paradigm Shift

Before AlexNet:
- âŒ Computer vision used hand-crafted features (SIFT, HOG, SURF)
- âŒ Deep networks were considered "too hard to train"
- âŒ Common wisdom: "Deep learning doesn't work for vision"
- âŒ Best systems had shallow pipelines with manual feature engineering

After AlexNet:
- âœ… End-to-end learning replaced feature engineering
- âœ… "Deeper is better" became the mantra
- âœ… GPUs became essential for AI research
- âœ… Deep learning dominated every vision benchmark

This paper didn't just win a competitionâ€”it **started a revolution** that gave us self-driving cars, medical diagnosis AI, facial recognition, and modern computer vision as we know it.

---

## ğŸ¤” Why AlexNet Changed Everything

This isn't just about winning ImageNetâ€”it's about **proving a paradigm**:

**The core insight:** Given enough data and compute, neural networks can learn better features than humans can design.

### The Historical Context

**Pre-2012 Computer Vision Pipeline:**
```
Raw Image â†’ Hand-Designed Features â†’ Classifier â†’ Prediction
            (SIFT, HOG, etc.)       (SVM, etc.)
```

Each step required:
- Expert domain knowledge
- Manual tuning
- Years of research for each feature type
- Separate pipeline for each task

**AlexNet's Pipeline:**
```
Raw Image â†’ Deep CNN â†’ Prediction
            (learns features automatically!)
```

One architecture. End-to-end learning. Task-agnostic.

### Why It Worked When Others Failed

AlexNet succeeded where others failed because it combined five key innovations:

1. **ReLU Activation** - Solved vanishing gradients
2. **Dropout** - Prevented overfitting  
3. **GPU Training** - Made deep networks feasible
4. **Data Augmentation** - Artificially expanded training data
5. **Local Response Normalization** - Improved generalization

None of these were entirely new, but **combining them at scale** was revolutionary.

---

## ğŸŒ Real-World Analogy

### The Assembly Line Analogy

Think of traditional computer vision like a **custom craftsman shop**:

**Before AlexNet (Craftsman):**
- Expert: "I'll hand-design a SIFT feature for corners"
- Expert: "I'll craft a HOG feature for edges"
- Expert: "I'll build a color histogram for textures"
- Expert: "Now combine them with careful tuning..."
- **Result**: Beautiful, but slow and requires expertise

**After AlexNet (Assembly Line):**
- Engineer: "Here's 1.2 million labeled images"
- AlexNet: "I'll figure out the features myself"
- GPU: *buzzing sounds of massive computation*
- **Result**: Better performance, faster, scales to any task

### The Language Learning Analogy

**Traditional Vision (Grammar Rules):**
- Learn rules: "Edges connect to form shapes"
- Learn rules: "Shapes combine into objects"
- Try to apply rules to recognize new objects
- **Problem**: Can't capture every rule!

**AlexNet (Immersion):**
- See millions of examples
- Brain figures out patterns naturally
- Generalizes to new situations
- **Advantage**: Learns things humans can't articulate!

Just like a child learning language through exposure rather than grammar rules, AlexNet learns vision through examples rather than hand-coded features.

---

## ğŸ“Š The Architecture

### The 8-Layer Structure

```
Input: 224Ã—224Ã—3 RGB Image
         â†“
Layer 1: Conv(11Ã—11Ã—96, stride=4) â†’ ReLU â†’ MaxPool(3Ã—3, stride=2) â†’ LRN
         [55Ã—55Ã—96 â†’ 27Ã—27Ã—96]
         â†“
Layer 2: Conv(5Ã—5Ã—256) â†’ ReLU â†’ MaxPool(3Ã—3, stride=2) â†’ LRN
         [27Ã—27Ã—256 â†’ 13Ã—13Ã—256]
         â†“
Layer 3: Conv(3Ã—3Ã—384) â†’ ReLU
         [13Ã—13Ã—384]
         â†“
Layer 4: Conv(3Ã—3Ã—384) â†’ ReLU
         [13Ã—13Ã—384]
         â†“
Layer 5: Conv(3Ã—3Ã—256) â†’ ReLU â†’ MaxPool(3Ã—3, stride=2)
         [13Ã—13Ã—256 â†’ 6Ã—6Ã—256]
         â†“
Layer 6: FC(4096) â†’ ReLU â†’ Dropout(0.5)
         [9216 â†’ 4096]
         â†“
Layer 7: FC(4096) â†’ ReLU â†’ Dropout(0.5)
         [4096 â†’ 4096]
         â†“
Layer 8: FC(1000) â†’ Softmax
         [4096 â†’ 1000]
```

**Total Parameters:** ~60 million  
**Memory Required:** ~240 MB (with float32)

### Key Innovations Explained

#### 1. ReLU Activation Function

**The Game Changer:**
$$f(x) = \max(0, x)$$

**Why it revolutionized deep learning:**

Before ReLU (sigmoid/tanh):
- Gradients vanish for large/small inputs
- Saturates at 0 and 1 (or -1 and 1)
- Slow to compute (exponentials)

With ReLU:
- âœ… **No vanishing gradient** for positive values
- âœ… **Sparse activation** (many neurons = 0)
- âœ… **Computationally efficient** (simple max operation)
- âœ… **Biological plausibility** (neurons fire or don't)

**Impact on training speed:**
AlexNet paper showed ReLU networks train **6Ã— faster** than tanh networks!

```python
# Old way (slow, vanishing gradients)
def tanh(x):
    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

# AlexNet way (fast, healthy gradients)
def relu(x):
    return np.maximum(0, x)
```

#### 2. Dropout Regularization

**The Overfitting Killer:**

During training:
$$y = x \odot \text{Bernoulli}(p)$$

During inference:
$$y = p \cdot x$$

**Why it works:**

Traditional regularization:
- L2 penalty: $\lambda \sum w^2$
- Early stopping
- Limited effectiveness on large networks

Dropout:
- âœ… Trains an **ensemble** of $2^n$ sub-networks
- âœ… Prevents **co-adaptation** (neurons can't rely on others)
- âœ… Forces **robust features** (must work without specific neurons)
- âœ… Implicit **model averaging** at test time

**The magic:** Each forward pass trains a different random sub-network!

```python
class Dropout(nn.Module):
    def __init__(self, p=0.5):
        super().__init__()
        self.p = p
    
    def forward(self, x):
        if self.training:
            # Random mask: keep neuron with probability p
            mask = torch.bernoulli(torch.ones_like(x) * self.p)
            return x * mask / self.p  # Scale to maintain expected value
        else:
            return x  # Use all neurons at test time
```

#### 3. Local Response Normalization (LRN)

**Lateral inhibition** inspired by neuroscience:

$$b_{x,y}^i = \frac{a_{x,y}^i}{\left(k + \alpha \sum_{j=\max(0,i-n/2)}^{\min(N-1,i+n/2)} (a_{x,y}^j)^2\right)^\beta}$$

Where:
- $a_{x,y}^i$ = activity of neuron at position $(x,y)$ in feature map $i$
- $n$ = neighborhood size (AlexNet used $n=5$)
- $k=2, \alpha=10^{-4}, \beta=0.75$ (hyperparameters)

**Biological inspiration:** "Bright" neurons suppress nearby neurons, creating competition.

**Note:** Modern networks often use Batch Normalization instead, but LRN was important historically.

#### 4. Overlapping Pooling

**Small but effective tweak:**

Traditional pooling: stride = pool size (no overlap)
- 2Ã—2 pool, stride 2: non-overlapping

AlexNet pooling: stride < pool size (overlap!)
- 3Ã—3 pool, stride 2: overlapping regions

**Benefits:**
- Slight accuracy improvement (~0.4%)
- More robust features
- Harder to overfit

---

## ğŸ”§ Implementation Guide

### Building AlexNet from Scratch

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class AlexNet(nn.Module):
    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()
        
        # Feature extraction layers
        self.features = nn.Sequential(
            # Layer 1: Conv1
            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2),
            
            # Layer 2: Conv2
            nn.Conv2d(96, 256, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2),
            
            # Layer 3: Conv3
            nn.Conv2d(256, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            
            # Layer 4: Conv4
            nn.Conv2d(384, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            
            # Layer 5: Conv5
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        
        # Classifier layers
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.5),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            
            nn.Linear(4096, num_classes),
        )
    
    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)  # Flatten for FC layers
        x = self.classifier(x)
        return x

# Create model
model = AlexNet(num_classes=1000)
print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
```

### Data Augmentation (Critical for AlexNet's Success)

AlexNet used extensive augmentation to effectively increase dataset size:

```python
import torchvision.transforms as transforms
from PIL import Image

# AlexNet's augmentation strategy
train_transform = transforms.Compose([
    # 1. Random crops
    transforms.RandomResizedCrop(224),
    
    # 2. Random horizontal flips
    transforms.RandomHorizontalFlip(p=0.5),
    
    # 3. Color jittering (AlexNet's PCA color augmentation)
    transforms.ColorJitter(
        brightness=0.4,
        contrast=0.4,
        saturation=0.4,
        hue=0.1
    ),
    
    # 4. Normalize (ImageNet statistics)
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])

# Test-time augmentation (deterministic)
test_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])
```

**AlexNet's PCA Color Augmentation:**

```python
def pca_color_augmentation(image, alpha_std=0.1):
    """
    AlexNet's fancy color augmentation using PCA.
    Adds multiples of principal components of RGB pixel values.
    """
    # Reshape image to (3, H*W)
    orig_shape = image.shape
    image = image.reshape(3, -1)
    
    # Compute covariance matrix
    image_cov = np.cov(image)
    
    # Eigenvalues and eigenvectors
    eig_vals, eig_vecs = np.linalg.eigh(image_cov)
    
    # Generate random scaling factors
    alpha = np.random.normal(0, alpha_std, 3)
    
    # Add weighted principal components
    rgb_shift = (eig_vecs @ (alpha * eig_vals))
    
    # Apply to image
    augmented = image + rgb_shift.reshape(3, 1)
    
    return augmented.reshape(orig_shape)
```

---

## ğŸ¯ Training Tips

### 1. **Learning Rate Schedule**

AlexNet used **manual learning rate decay**:

```python
# Initial learning rate
lr = 0.01

# Training loop
for epoch in range(90):
    # Reduce learning rate manually
    if epoch in [30, 60, 80]:
        lr = lr * 0.1
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
    
    # Train epoch
    train_one_epoch(model, train_loader, optimizer)
```

**Modern alternative (cosine annealing):**

```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=epochs, eta_min=0
)
```

### 2. **Weight Initialization**

AlexNet used specific initialization:

```python
def init_weights(m):
    """AlexNet's initialization strategy"""
    if isinstance(m, nn.Conv2d):
        nn.init.normal_(m.weight, mean=0, std=0.01)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.constant_(m.bias, 1)  # Bias = 1 for FC layers!

model.apply(init_weights)
```

**Why bias=1 for FC layers?** Ensures ReLU neurons start in "active" region.

### 3. **Batch Size and Momentum**

```python
# AlexNet's training setup
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9,      # Heavy momentum
    weight_decay=5e-4  # L2 regularization
)

# Batch size: 128 (across 2 GPUs = 64 per GPU)
batch_size = 128
```

### 4. **GPU Training (Essential)**

AlexNet pioneered **model parallelism** across 2 GPUs:

```python
class AlexNetParallel(nn.Module):
    """AlexNet with model parallelism like the original"""
    def __init__(self):
        super().__init__()
        
        # First GPU: layers 1-2
        self.features_gpu1 = nn.Sequential(
            nn.Conv2d(3, 48, 11, 4, 2),
            nn.ReLU(),
            nn.MaxPool2d(3, 2),
            nn.LocalResponseNorm(5),
            
            nn.Conv2d(48, 128, 5, 1, 2),
            nn.ReLU(),
            nn.MaxPool2d(3, 2),
            nn.LocalResponseNorm(5),
        ).cuda(0)
        
        # Second GPU: layers 3-5
        self.features_gpu2 = nn.Sequential(
            nn.Conv2d(128, 192, 3, 1, 1),
            nn.ReLU(),
            # ... rest of layers
        ).cuda(1)
    
    def forward(self, x):
        x = x.cuda(0)
        x = self.features_gpu1(x)
        x = x.cuda(1)  # Transfer to GPU 2
        x = self.features_gpu2(x)
        return x
```

**Modern approach:** Use DataParallel or DistributedDataParallel instead.

---

## ğŸ“ˆ Visualizations

### 1. Feature Map Visualization

```python
def visualize_feature_maps(model, image, layer_idx=0):
    """Visualize what AlexNet sees at different layers"""
    import matplotlib.pyplot as plt
    
    # Hook to capture activations
    activations = []
    def hook_fn(module, input, output):
        activations.append(output)
    
    # Register hook
    handle = list(model.features.children())[layer_idx].register_forward_hook(hook_fn)
    
    # Forward pass
    model.eval()
    with torch.no_grad():
        _ = model(image.unsqueeze(0))
    
    # Remove hook
    handle.remove()
    
    # Visualize first 64 feature maps
    features = activations[0][0]  # Shape: (C, H, W)
    
    fig, axes = plt.subplots(8, 8, figsize=(16, 16))
    for idx, ax in enumerate(axes.flat):
        if idx < features.shape[0]:
            ax.imshow(features[idx].cpu(), cmap='viridis')
        ax.axis('off')
    
    plt.suptitle(f'Feature Maps at Layer {layer_idx}', fontsize=16)
    plt.tight_layout()
    plt.show()
```

### 2. Filter Visualization

```python
def visualize_conv_filters(model, layer_idx=0):
    """Visualize learned convolutional filters"""
    # Get first conv layer weights
    conv_layer = list(model.features.children())[layer_idx]
    filters = conv_layer.weight.data.cpu()
    
    # Normalize for visualization
    filters = (filters - filters.min()) / (filters.max() - filters.min())
    
    # Plot first 96 filters
    fig, axes = plt.subplots(8, 12, figsize=(20, 14))
    for idx, ax in enumerate(axes.flat):
        if idx < filters.shape[0]:
            # Convert to RGB (take first 3 channels)
            filter_rgb = filters[idx, :3].permute(1, 2, 0)
            ax.imshow(filter_rgb)
        ax.axis('off')
    
    plt.suptitle('Learned Convolutional Filters (Layer 1)', fontsize=16)
    plt.tight_layout()
    plt.show()
```

### 3. Training Curves

```python
def plot_training_curves(train_losses, val_losses, val_accs):
    """Plot AlexNet training dynamics"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    epochs = range(1, len(train_losses) + 1)
    
    # Loss curves
    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('AlexNet Training Loss', fontsize=14, weight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Accuracy curve
    ax2.plot(epochs, val_accs, 'g-', linewidth=2, marker='o')
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Top-1 Accuracy (%)', fontsize=12)
    ax2.set_title('Validation Accuracy', fontsize=14, weight='bold')
    ax2.grid(True, alpha=0.3)
    
    # Mark the 84.7% top-5 accuracy milestone
    ax2.axhline(y=84.7, color='r', linestyle='--', 
                label='AlexNet Top-5 Accuracy')
    ax2.legend()
    
    plt.tight_layout()
    plt.show()
```

---

## ğŸ‹ï¸ Exercises

### Exercise 1: Build AlexNet from Scratch (â±ï¸â±ï¸â±ï¸)
Implement AlexNet in PyTorch without looking at the code. Train on CIFAR-10 (smaller dataset). Compare with a shallow baseline.

**Key learning:** Understanding each layer's role and how depth helps.

### Exercise 2: Ablation Study (â±ï¸â±ï¸â±ï¸â±ï¸)
Remove one innovation at a time and measure impact:
- Replace ReLU with sigmoid
- Remove dropout
- Remove data augmentation
- Use smaller network

Which innovation matters most?

### Exercise 3: Feature Visualization (â±ï¸â±ï¸)
Extract and visualize features from each layer. Observe:
- Layer 1: Edge detectors
- Layer 2: Texture patterns
- Layer 3-5: Object parts
- FC layers: High-level concepts

### Exercise 4: Transfer Learning (â±ï¸â±ï¸â±ï¸)
Use pretrained AlexNet for a new task:
1. Load ImageNet weights
2. Freeze early layers
3. Fine-tune on new dataset (e.g., flowers, cars)

Compare with training from scratch.

### Exercise 5: Modern Improvements (â±ï¸â±ï¸â±ï¸â±ï¸)
Upgrade AlexNet with modern techniques:
- Replace LRN with Batch Normalization
- Add residual connections
- Use better optimizers (Adam, AdamW)
- Implement learning rate warm-up

How much can you improve accuracy?

---

## ğŸš€ Going Further

### AlexNet's Legacy

**Immediate Impact:**
- VGGNet (2014): Deeper with smaller filters
- GoogLeNet (2014): Inception modules
- ResNet (2015): Skip connections enable 100+ layers

**Long-term Influence:**
- Established GPU training as standard
- Proved end-to-end learning works
- Started the "ImageNet pretraining" era
- Made deep learning mainstream

### Modern Context

**What AlexNet Got Right (Still Used Today):**
- âœ… ReLU activations
- âœ… Dropout regularization
- âœ… Data augmentation
- âœ… GPU acceleration
- âœ… End-to-end learning

**What We've Improved:**
- Batch Normalization > LRN
- Residual connections enable much deeper networks
- Better optimizers (Adam family)
- More sophisticated augmentation (MixUp, CutMix)
- Attention mechanisms

### Research Frontiers

1. **Vision Transformers (ViT)**
   - Replace convolutions with self-attention
   - But still use many AlexNet principles!

2. **Efficient Architectures**
   - MobileNets, EfficientNets
   - AlexNet-inspired but optimized for mobile

3. **Self-Supervised Learning**
   - Learn without labels
   - Still use ConvNet backbones

---

## ğŸ“š Resources

### Must-Read
- ğŸ“„ [Original Paper](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) - AlexNet (2012)
- ğŸ“– [ImageNet](http://www.image-net.org/) - The dataset that changed everything
- ğŸ“„ [Dropout Paper](https://jmlr.org/papers/v15/srivastava14a.html) - Hinton et al. (2014)

### Implementations
- ğŸ’» [PyTorch AlexNet](https://pytorch.org/vision/stable/_modules/torchvision/models/alexnet.html) - Official implementation
- ğŸ’» [TensorFlow AlexNet](https://github.com/tensorflow/models/tree/master/research/slim/nets) - TF-Slim version
- ğŸ’» [Original Code](https://code.google.com/archive/p/cuda-convnet/) - Krizhevsky's CUDA implementation

### Visualizations
- ğŸ¥ [CNN Explainer](https://poloclub.github.io/cnn-explainer/) - Interactive CNN visualization
- ğŸ“Š [Distill: Feature Visualization](https://distill.pub/2017/feature-visualization/) - Understanding what CNNs learn
- ğŸ¥ [Andrej Karpathy's lecture](https://www.youtube.com/watch?v=NfnWJUyUJYU) - CNNs for Visual Recognition

### Historical Context
- ğŸ“– [The ImageNet Moment](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/) - When deep learning took over
- ğŸ“„ [LeNet-5](http://yann.lecun.com/exdb/lenet/) - The predecessor (Yann LeCun, 1998)

---

## ğŸ“ Key Takeaways

1. **End-to-end learning beats hand-crafted features** - Let the network learn!
2. **ReLU solved vanishing gradients** - Simple but revolutionary
3. **Dropout prevents overfitting** - Like training an ensemble
4. **Data augmentation is crucial** - Effectively increases dataset size
5. **GPUs enable deep learning** - Hardware matters as much as algorithms
6. **Depth creates hierarchy** - Early layers: edges, Late layers: concepts

### The Historical Significance

AlexNet didn't just win ImageNetâ€”it proved a paradigm:

**Before:** "Neural networks don't work for real vision tasks"  
**After:** "Neural networks are the ONLY way to do vision"

This single paper:
- Started the deep learning revolution
- Made computer vision an AI problem, not an engineering problem
- Established the ImageNet challenge as THE benchmark
- Proved that scale (data + compute) matters

### Why It Still Matters

Even though we have better architectures now (ResNet, Vision Transformers), AlexNet established principles that remain fundamental:

- Deep hierarchical feature learning
- End-to-end trainable systems
- GPU acceleration
- Regularization through dropout and augmentation

Every modern vision system owes a debt to AlexNet.

---

**Completed Day 8?** Move on to **[Day 9: ResNet](../day9_resnet/)** where skip connections enable networks 10Ã— deeper than AlexNet!

**Questions?** Check the [notebook.ipynb](notebook.ipynb) for interactive code and visualizations.

---

*"The year 2012 was the big breakthrough. That's when we made a large deep learning system work really well, and it was surprising to everyoneâ€”including usâ€”how well it worked."* - Geoffrey Hinton, reflecting on AlexNet