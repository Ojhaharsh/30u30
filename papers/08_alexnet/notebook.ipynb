{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7cf82c8",
   "metadata": {},
   "source": [
    "# Day 8: AlexNet - The Deep Learning Revolution üöÄ\n",
    "\n",
    "Welcome to Day 8 of 30 Papers in 30 Days!\n",
    "\n",
    "Today we're exploring **AlexNet** - the deep convolutional neural network that launched the modern deep learning era. In 2012, AlexNet didn't just win the ImageNet competition - it won by such a massive margin that it changed computer vision forever.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **The Revolution**: How AlexNet transformed computer vision overnight\n",
    "2. **Architecture Deep Dive**: Understanding the 8-layer pioneer that started it all\n",
    "3. **Key Innovations**: ReLU, Dropout, Data Augmentation, and GPU acceleration\n",
    "4. **Implementation**: Building AlexNet from scratch and training it\n",
    "5. **Feature Analysis**: Visualizing what AlexNet actually learns\n",
    "6. **Legacy Impact**: How AlexNet's lessons power today's AI systems\n",
    "\n",
    "## The Big Idea (in 30 seconds)\n",
    "\n",
    "Before AlexNet: Computer vision used hand-crafted features + shallow learning üìê\n",
    "After AlexNet: End-to-end learning from raw pixels with deep networks üß†\n",
    "**Result**: Accuracy jumped from ~74% to 84.7% overnight! üìà\n",
    "\n",
    "AlexNet proved that **bigger networks + more data + better hardware = breakthrough performance**\n",
    "\n",
    "Let's dive into this revolutionary architecture! üí´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory to path for imports\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import our AlexNet implementation\n",
    "from implementation import AlexNet, AlexNetFeatureExtractor\n",
    "from visualization import AlexNetVisualizer\n",
    "from train_minimal import AlexNetTrainer, create_synthetic_dataset\n",
    "\n",
    "# Set up device and random seeds\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"üî• Using device: {device}\")\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"üéØ Ready to explore the deep learning revolution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11c865",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the AlexNet Architecture\n",
    "\n",
    "AlexNet was revolutionary because it was the first successful **very deep** convolutional neural network. Let's build it step by step and understand why each component was crucial.\n",
    "\n",
    "### The 8-Layer Architecture\n",
    "\n",
    "AlexNet consists of:\n",
    "- **5 Convolutional layers** (feature extraction)\n",
    "- **3 Fully connected layers** (classification)\n",
    "- **Key innovations**: Large kernels, ReLU activation, Dropout, Local Response Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build and examine AlexNet architecture\n",
    "def explore_alexnet_architecture():\n",
    "    \"\"\"Explore AlexNet layer by layer.\"\"\"\n",
    "    \n",
    "    print(\"üèóÔ∏è  Building AlexNet Architecture...\")\n",
    "    \n",
    "    # Create AlexNet model\n",
    "    model = AlexNet(num_classes=1000)\n",
    "    \n",
    "    print(\"\\nüìê AlexNet Architecture Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Print each layer with output shapes\n",
    "    input_shape = (1, 3, 224, 224)\n",
    "    x = torch.randn(input_shape)\n",
    "    \n",
    "    print(f\"Input: {list(x.shape)} (Batch, Channels, Height, Width)\")\n",
    "    \n",
    "    # Features (Convolutional layers)\n",
    "    print(\"\\nüîç Feature Extraction Layers:\")\n",
    "    for i, layer in enumerate(model.features):\n",
    "        x = layer(x)\n",
    "        if isinstance(layer, (nn.Conv2d, nn.MaxPool2d)):\n",
    "            print(f\"  {layer.__class__.__name__}: {list(x.shape)}\")\n",
    "    \n",
    "    # Classifier (Fully connected layers)\n",
    "    print(\"\\nüéØ Classification Layers:\")\n",
    "    x = torch.flatten(x, 1)\n",
    "    print(f\"  Flatten: {list(x.shape)}\")\n",
    "    \n",
    "    for i, layer in enumerate(model.classifier):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            x = layer(x)\n",
    "            print(f\"  Linear{i//2 + 1}: {list(x.shape)}\")\n",
    "        else:\n",
    "            x = layer(x)\n",
    "    \n",
    "    # Calculate total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nüìä Model Statistics:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Model size: {total_params * 4 / 1024**2:.1f} MB (float32)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Explore the architecture\n",
    "alexnet_model = explore_alexnet_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a0fb7",
   "metadata": {},
   "source": [
    "## Part 2: The Key Innovations\n",
    "\n",
    "AlexNet introduced several techniques that became standard in deep learning. Let's explore each innovation and understand why it was revolutionary.\n",
    "\n",
    "### Innovation 1: ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c57105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ReLU vs traditional activation functions\n",
    "def compare_activation_functions():\n",
    "    \"\"\"Compare different activation functions and their properties.\"\"\"\n",
    "    \n",
    "    print(\"‚ö° Comparing Activation Functions...\")\n",
    "    \n",
    "    # Create test input\n",
    "    x = torch.linspace(-5, 5, 1000)\n",
    "    \n",
    "    # Different activation functions\n",
    "    activations = {\n",
    "        'Sigmoid': torch.sigmoid(x),\n",
    "        'Tanh': torch.tanh(x),\n",
    "        'ReLU': torch.relu(x),\n",
    "        'ReLU6': torch.clamp(torch.relu(x), max=6)  # Variant used in mobile networks\n",
    "    }\n",
    "    \n",
    "    # Plot activations\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = [ax1, ax2, ax3, ax4]\n",
    "    \n",
    "    for i, (name, y) in enumerate(activations.items()):\n",
    "        ax = axes[i]\n",
    "        ax.plot(x.numpy(), y.numpy(), linewidth=2, label=name)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlabel('Input')\n",
    "        ax.set_ylabel('Output')\n",
    "        ax.set_title(f'{name} Activation')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Gradient comparison\n",
    "    print(\"\\nüìä Gradient Properties:\")\n",
    "    test_x = torch.tensor([2.0], requires_grad=True)\n",
    "    \n",
    "    for name in ['Sigmoid', 'Tanh', 'ReLU']:\n",
    "        if test_x.grad is not None:\n",
    "            test_x.grad.zero_()\n",
    "            \n",
    "        if name == 'Sigmoid':\n",
    "            output = torch.sigmoid(test_x)\n",
    "        elif name == 'Tanh':\n",
    "            output = torch.tanh(test_x)\n",
    "        else:  # ReLU\n",
    "            output = torch.relu(test_x)\n",
    "        \n",
    "        output.backward()\n",
    "        gradient = test_x.grad.item()\n",
    "        \n",
    "        print(f\"  {name}: f(2) = {output.item():.3f}, f'(2) = {gradient:.3f}\")\n",
    "    \n",
    "    print(\"\\nüí° Why ReLU was revolutionary:\")\n",
    "    print(\"  ‚úÖ Non-zero gradients for positive inputs (no vanishing gradient)\")\n",
    "    print(\"  ‚úÖ Computationally simple (max(0, x))\")\n",
    "    print(\"  ‚úÖ Sparse activation (many zeros)\")\n",
    "    print(\"  ‚úÖ No saturation for positive values\")\n",
    "\n",
    "compare_activation_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4456e",
   "metadata": {},
   "source": [
    "### Innovation 2: Dropout Regularization\n",
    "\n",
    "Dropout was a game-changing regularization technique that helped AlexNet generalize better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ba52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate dropout's effect on overfitting\n",
    "def demonstrate_dropout():\n",
    "    \"\"\"Show how dropout prevents overfitting.\"\"\"\n",
    "    \n",
    "    print(\"üé≠ Demonstrating Dropout Regularization...\")\n",
    "    \n",
    "    # Create simple dataset for demonstration\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(1000, 50)  # 1000 samples, 50 features\n",
    "    y = (X[:, :5].sum(dim=1) + 0.1 * torch.randn(1000) > 0).long()  # Only first 5 features matter\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_X, train_y = X[:800], y[:800]\n",
    "    test_X, test_y = X[800:], y[800:]\n",
    "    \n",
    "    # Define models with and without dropout\n",
    "    class SimpleNet(nn.Module):\n",
    "        def __init__(self, use_dropout=False, dropout_rate=0.5):\n",
    "            super().__init__()\n",
    "            self.use_dropout = use_dropout\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(50, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate) if use_dropout else nn.Identity(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(), \n",
    "                nn.Dropout(dropout_rate) if use_dropout else nn.Identity(),\n",
    "                nn.Linear(128, 2)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    # Train both models\n",
    "    models = {\n",
    "        'Without Dropout': SimpleNet(use_dropout=False),\n",
    "        'With Dropout': SimpleNet(use_dropout=True, dropout_rate=0.5)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüèÉ Training {name}...\")\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        train_losses, test_accuracies = [], []\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            # Training\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_X)\n",
    "            loss = criterion(outputs, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Testing\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(test_X)\n",
    "                test_acc = (test_outputs.argmax(dim=1) == test_y).float().mean()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            test_accuracies.append(test_acc.item())\n",
    "        \n",
    "        results[name] = {\n",
    "            'train_loss': train_losses,\n",
    "            'test_acc': test_accuracies,\n",
    "            'final_test_acc': test_accuracies[-1]\n",
    "        }\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    for name, metrics in results.items():\n",
    "        epochs = range(len(metrics['train_loss']))\n",
    "        \n",
    "        ax1.plot(epochs, metrics['train_loss'], label=f\"{name}\", linewidth=2)\n",
    "        ax2.plot(epochs, metrics['test_acc'], label=f\"{name}\", linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss')\n",
    "    ax1.set_title('Training Loss Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Test Accuracy')\n",
    "    ax2.set_title('Test Accuracy Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìà Final Results:\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"  {name}: Test Accuracy = {metrics['final_test_acc']:.3f}\")\n",
    "    \n",
    "    print(\"\\nüí° Dropout Benefits:\")\n",
    "    print(\"  ‚úÖ Prevents overfitting by randomly \\\"turning off\\\" neurons\")\n",
    "    print(\"  ‚úÖ Forces network to learn robust features\")\n",
    "    print(\"  ‚úÖ Acts like training an ensemble of networks\")\n",
    "    print(\"  ‚úÖ Only active during training, turned off during inference\")\n",
    "\n",
    "demonstrate_dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba1b50",
   "metadata": {},
   "source": [
    "## Part 3: Training AlexNet from Scratch\n",
    "\n",
    "Let's train a simplified version of AlexNet on a synthetic dataset to see the training process in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f470430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AlexNet on synthetic data\n",
    "def train_alexnet_demo():\n",
    "    \"\"\"Train AlexNet on synthetic ImageNet-like data.\"\"\"\n",
    "    \n",
    "    print(\"üéì Training AlexNet Demo...\")\n",
    "    \n",
    "    # Create synthetic dataset\n",
    "    print(\"üì¶ Creating synthetic dataset...\")\n",
    "    train_dataset, test_dataset = create_synthetic_dataset(\n",
    "        num_classes=10,\n",
    "        samples_per_class=200,\n",
    "        image_size=224\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=32, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Create smaller AlexNet for faster training\n",
    "    model = AlexNet(num_classes=10).to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nüöÄ Starting training...\")\n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    for epoch in range(10):  # Quick demo\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Epoch {epoch+1}, Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2f}%\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\\n        for data, targets in test_loader:\\n            data, targets = data.to(device), targets.to(device)\\n            outputs = model(data)\\n            _, predicted = torch.max(outputs, 1)\\n            test_total += targets.size(0)\\n            test_correct += (predicted == targets).sum().item()\\n    \\n    test_accuracy = 100 * test_correct / test_total\\n    \\n    # Plot training progress\\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\\n    \\n    epochs = range(1, len(train_losses) + 1)\\n    \\n    ax1.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss')\\n    ax1.set_xlabel('Epoch')\\n    ax1.set_ylabel('Loss')\\n    ax1.set_title('Training Loss')\\n    ax1.grid(True, alpha=0.3)\\n    ax1.legend()\\n    \\n    ax2.plot(epochs, train_accuracies, 'r-', linewidth=2, label='Training Accuracy')\\n    ax2.axhline(y=test_accuracy, color='g', linestyle='--', linewidth=2, label=f'Test Accuracy ({test_accuracy:.1f}%)')\\n    ax2.set_xlabel('Epoch')\\n    ax2.set_ylabel('Accuracy (%)')\\n    ax2.set_title('Training Progress')\\n    ax2.grid(True, alpha=0.3)\\n    ax2.legend()\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    print(f\\\"\\\\nüéØ Final Test Accuracy: {test_accuracy:.2f}%\\\")\\n    print(f\\\"üìä Model trained successfully in {len(train_losses)} epochs!\\\")\\n    \\n    return model\\n\\n# Train the model\\ntrained_model = train_alexnet_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f1494",
   "metadata": {},
   "source": [
    "## Part 4: Feature Visualization - What Does AlexNet See?\n",
    "\n",
    "One of the most fascinating aspects of AlexNet is understanding what features it learns. Let's visualize the filters and feature maps to see how it processes images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AlexNet features\n",
    "def visualize_alexnet_features():\n",
    "    \"\"\"Visualize what AlexNet learns to detect.\"\"\"\n",
    "    \n",
    "    print(\"üëÅÔ∏è  Visualizing AlexNet Features...\")\n",
    "    \n",
    "    # Create visualizer\n",
    "    viz = AlexNetVisualizer(trained_model.cpu())  # Move to CPU for visualization\n",
    "    \n",
    "    # Visualize first layer filters\n",
    "    print(\"\\\\nüîç First Layer Filters (Edge and Color Detectors):\")\n",
    "    viz.plot_conv_filters(layer_name='features.0', num_filters=16, figsize=(12, 8))\\n    \\n    # Create a test image\\n    print(\\\"\\\\nüñºÔ∏è  Creating test image...\\\")\\n    test_image = torch.randn(1, 3, 224, 224)\\n    \\n    # Add some structure to make it more interesting\\n    test_image[0, 0, 50:150, 50:150] = 2.0  # Red square\\n    test_image[0, 1, 100:200, 100:200] = 2.0  # Green square\\n    test_image[0, 2, 75:125, 150:200] = 2.0  # Blue rectangle\\n    \\n    # Normalize\\n    test_image = torch.clamp(test_image, 0, 1)\\n    \\n    # Show the test image\\n    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\\n    ax.imshow(test_image[0].permute(1, 2, 0))\\n    ax.set_title('Test Image')\\n    ax.axis('off')\\n    plt.show()\\n    \\n    # Visualize feature maps for different layers\\n    print(\\\"\\\\nüó∫Ô∏è  Feature Maps at Different Depths:\\\")\\n    layer_names = ['features.0', 'features.3', 'features.6']  # Conv1, Conv2, Conv3\\n    layer_titles = ['Layer 1 (Edges/Colors)', 'Layer 2 (Textures)', 'Layer 3 (Patterns)']\\n    \\n    for layer_name, title in zip(layer_names, layer_titles):\\n        print(f\\\"  Plotting {title}...\\\")\\n        viz.plot_feature_maps(test_image, layer_name, title=title, num_maps=12)\\n\\nvisualize_alexnet_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a151da",
   "metadata": {},
   "source": [
    "## Part 5: The AlexNet Impact Analysis\n",
    "\n",
    "Let's analyze how AlexNet's innovations impact performance and understand why it was so revolutionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze AlexNet's impact through ablation studies\n",
    "def alexnet_impact_analysis():\n",
    "    \\\"\\\"\\\"Analyze the impact of each AlexNet innovation.\\\"\\\"\\\"\\n    \\n    print(\\\"üî¨ AlexNet Impact Analysis...\\\")\\n    \\n    # Create different model variants\\n    class AlexNetVariant(nn.Module):\\n        def __init__(self, use_relu=True, use_dropout=True, use_data_aug=True):\\n            super().__init__()\\n            self.use_relu = use_relu\\n            self.use_dropout = use_dropout\\n            \\n            # Feature layers\\n            activation = nn.ReLU(inplace=True) if use_relu else nn.Tanh()\\n            \\n            self.features = nn.Sequential(\\n                nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\\n                activation,\\n                nn.MaxPool2d(kernel_size=3, stride=2),\\n                nn.Conv2d(64, 192, kernel_size=5, padding=2),\\n                activation,\\n                nn.MaxPool2d(kernel_size=3, stride=2),\\n                nn.Conv2d(192, 384, kernel_size=3, padding=1),\\n                activation,\\n                nn.Conv2d(384, 256, kernel_size=3, padding=1),\\n                activation,\\n                nn.Conv2d(256, 256, kernel_size=3, padding=1),\\n                activation,\\n                nn.MaxPool2d(kernel_size=3, stride=2),\\n            )\\n            \\n            # Classifier\\n            dropout = nn.Dropout(0.5) if use_dropout else nn.Identity()\\n            \\n            self.classifier = nn.Sequential(\\n                dropout,\\n                nn.Linear(256 * 6 * 6, 4096),\\n                activation,\\n                dropout,\\n                nn.Linear(4096, 4096),\\n                activation,\\n                nn.Linear(4096, 10),  # 10 classes for our demo\\n            )\\n            \\n        def forward(self, x):\\n            x = self.features(x)\\n            x = torch.flatten(x, 1)\\n            x = self.classifier(x)\\n            return x\\n    \\n    # Test different configurations\\n    configs = [\\n        {'name': 'Full AlexNet', 'use_relu': True, 'use_dropout': True},\\n        {'name': 'No ReLU (Tanh)', 'use_relu': False, 'use_dropout': True},\\n        {'name': 'No Dropout', 'use_relu': True, 'use_dropout': False},\\n        {'name': 'Neither', 'use_relu': False, 'use_dropout': False},\\n    ]\\n    \\n    print(\\\"\\\\nüß™ Testing different configurations...\\\")\\n    \\n    results = {}\\n    \\n    # Create simple dataset for testing\\n    X = torch.randn(1000, 3, 224, 224)\\n    y = torch.randint(0, 10, (1000,))\\n    dataset = torch.utils.data.TensorDataset(X, y)\\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\\n    \\n    for config in configs:\\n        print(f\\\"  Testing {config['name']}...\\\")\\n        \\n        model = AlexNetVariant(\\n            use_relu=config['use_relu'],\\n            use_dropout=config['use_dropout']\\n        ).to(device)\\n        \\n        # Quick training test (just a few batches)\\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\\n        criterion = nn.CrossEntropyLoss()\\n        \\n        model.train()\\n        total_loss = 0\\n        num_batches = 5  # Quick test\\n        \\n        for batch_idx, (data, targets) in enumerate(dataloader):\\n            if batch_idx >= num_batches:\\n                break\\n                \\n            data, targets = data.to(device), targets.to(device)\\n            \\n            optimizer.zero_grad()\\n            outputs = model(data)\\n            loss = criterion(outputs, targets)\\n            loss.backward()\\n            optimizer.step()\\n            \\n            total_loss += loss.item()\\n        \\n        avg_loss = total_loss / num_batches\\n        results[config['name']] = avg_loss\\n    \\n    # Plot results\\n    names = list(results.keys())\\n    losses = list(results.values())\\n    \\n    plt.figure(figsize=(10, 6))\\n    colors = ['green', 'orange', 'red', 'darkred']\\n    bars = plt.bar(names, losses, color=colors)\\n    plt.ylabel('Average Loss (lower is better)')\\n    plt.title('Impact of AlexNet Innovations')\\n    plt.xticks(rotation=45)\\n    \\n    # Add value labels on bars\\n    for bar, loss in zip(bars, losses):\\n        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \\n                f'{loss:.3f}', ha='center', va='bottom')\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    print(\\\"\\\\nüìä Results Summary:\\\")\\n    for name, loss in results.items():\\n        print(f\\\"  {name}: {loss:.3f} (lower is better)\\\")\\n    \\n    print(\\\"\\\\nüí° Key Insights:\\\")\\n    print(\\\"  üéØ ReLU activation significantly improves training\\\")\\n    print(\\\"  üõ°Ô∏è Dropout helps prevent overfitting\\\")\\n    print(\\\"  üöÄ Combined innovations create synergistic effects\\\")\\n    print(\\\"  üìà Each innovation contributes to the overall success\\\")\\n\\nalexnet_impact_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38788dae",
   "metadata": {},
   "source": [
    "## Part 6: AlexNet's Legacy and Modern Applications\n",
    "\n",
    "Let's explore how AlexNet's innovations continue to influence modern deep learning and see some practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3151c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore AlexNet's modern legacy\n",
    "def explore_alexnet_legacy():\n",
    "    \\\"\\\"\\\"Explore how AlexNet influences modern AI.\\\"\\\"\\\"\\n    \\n    print(\\\"üåü Exploring AlexNet's Legacy...\\\")\\n    \\n    # Evolution timeline\\n    evolution = {\\n        '2012 AlexNet': {'layers': 8, 'parameters': '61M', 'top1_error': '37.5%'},\\n        '2014 VGGNet': {'layers': 19, 'parameters': '144M', 'top1_error': '28.1%'},\\n        '2015 ResNet': {'layers': 152, 'parameters': '60M', 'top1_error': '19.4%'},\\n        '2019 EfficientNet': {'layers': 'Variable', 'parameters': '66M', 'top1_error': '15.3%'},\\n        '2021 ViT': {'layers': 'Transformer', 'parameters': '632M', 'top1_error': '16.5%'}\\n    }\\n    \\n    print(\\\"\\\\nüìà Evolution of Image Classification:\\\")\\n    print(\\\"=\\\" * 60)\\n    for model, stats in evolution.items():\\n        print(f\\\"{model:<20} | Layers: {stats['layers']:<12} | Params: {stats['parameters']:<8} | Error: {stats['top1_error']}\\\")\\n    \\n    # AlexNet techniques in modern models\\n    modern_techniques = {\\n        'ReLU Activation': {\\n            'introduced': 'AlexNet (2012)',\\n            'modern_use': 'Universal in deep learning',\\n            'variants': ['LeakyReLU', 'ELU', 'Swish', 'GELU']\\n        },\\n        'Dropout': {\\n            'introduced': 'AlexNet (2012)',\\n            'modern_use': 'Standard regularization technique',\\n            'variants': ['DropBlock', 'DropPath', 'Spatial Dropout']\\n        },\\n        'Data Augmentation': {\\n            'introduced': 'AlexNet (2012)',\\n            'modern_use': 'Essential for training',\\n            'variants': ['AutoAugment', 'RandAugment', 'MixUp', 'CutMix']\\n        },\\n        'GPU Acceleration': {\\n            'introduced': 'AlexNet (2012)', \\n            'modern_use': 'Standard practice',\\n            'variants': ['Multi-GPU', 'TPUs', 'Distributed Training']\\n        }\\n    }\\n    \\n    print(\\\"\\\\nüß¨ AlexNet DNA in Modern AI:\\\")\\n    print(\\\"=\\\" * 60)\\n    for technique, info in modern_techniques.items():\\n        print(f\\\"\\\\n{technique}:\\\")\\n        print(f\\\"  Introduced: {info['introduced']}\\\")\\n        print(f\\\"  Modern Use: {info['modern_use']}\\\")\\n        print(f\\\"  Variants: {', '.join(info['variants'])}\\\")\\n    \\n    # Create a visual comparison\\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\\n    \\n    # Error rate progression\\n    models = list(evolution.keys())\\n    errors = [float(stats['top1_error'].rstrip('%')) for stats in evolution.values()]\\n    years = [2012, 2014, 2015, 2019, 2021]\\n    \\n    ax1.plot(years, errors, 'bo-', linewidth=3, markersize=8)\\n    ax1.set_xlabel('Year')\\n    ax1.set_ylabel('Top-1 Error Rate (%)')\\n    ax1.set_title('ImageNet Error Rate Over Time')\\n    ax1.grid(True, alpha=0.3)\\n    ax1.set_ylim(0, max(errors) * 1.1)\\n    \\n    # Annotate points\\n    for i, (year, error, model) in enumerate(zip(years, errors, models)):\\n        ax1.annotate(model.split()[1], (year, error), \\n                    textcoords=\\\"offset points\\\", xytext=(0,10), ha='center')\\n    \\n    # Parameter count comparison (extract numbers where possible)\\n    param_counts = []\\n    model_names = []\\n    for model, stats in evolution.items():\\n        param_str = stats['parameters']\\n        if param_str.endswith('M') and param_str[:-1].isdigit():\\n            param_counts.append(int(param_str[:-1]))\\n            model_names.append(model.split()[1] if len(model.split()) > 1 else model)\\n    \\n    if param_counts:\\n        ax2.bar(model_names[:len(param_counts)], param_counts, color='skyblue', alpha=0.7)\\n        ax2.set_ylabel('Parameters (Millions)')\\n        ax2.set_title('Model Size Comparison')\\n        ax2.tick_params(axis='x', rotation=45)\\n    \\n    # Innovation timeline\\n    innovations = ['ReLU', 'Dropout', 'Data Aug.', 'GPU Training']\\n    adoption_years = [2012, 2012, 2012, 2012]  # All from AlexNet\\n    modern_impact = [10, 9, 10, 10]  # Impact score out of 10\\n    \\n    ax3.scatter(adoption_years, modern_impact, s=[200]*4, \\n               c=['red', 'blue', 'green', 'orange'], alpha=0.7)\\n    \\n    for i, innovation in enumerate(innovations):\\n        ax3.annotate(innovation, (adoption_years[i], modern_impact[i]),\\n                    textcoords=\\\"offset points\\\", xytext=(5,5), ha='left')\\n    \\n    ax3.set_xlabel('Year Introduced')\\n    ax3.set_ylabel('Modern Impact Score')\\n    ax3.set_title('AlexNet Innovations - Lasting Impact')\\n    ax3.grid(True, alpha=0.3)\\n    ax3.set_ylim(8, 11)\\n    \\n    # Modern applications\\n    applications = ['Social Media\\\\nPhoto Tagging', 'Medical\\\\nImaging', 'Autonomous\\\\nVehicles', \\n                   'Security\\\\nSystems', 'E-commerce\\\\nVisual Search']\\n    impact_scores = [9.5, 8.7, 9.2, 8.9, 8.5]\\n    \\n    bars = ax4.barh(applications, impact_scores, color='lightcoral', alpha=0.7)\\n    ax4.set_xlabel('Impact Score')\\n    ax4.set_title('AlexNet-Inspired Applications Today')\\n    ax4.set_xlim(0, 10)\\n    \\n    # Add value labels\\n    for bar, score in zip(bars, impact_scores):\\n        ax4.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \\n                f'{score}', ha='left', va='center')\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    print(\\\"\\\\nüöÄ AlexNet's Lasting Impact:\\\")\\n    print(\\\"  üèÜ Launched the deep learning revolution\\\")\\n    print(\\\"  üß† Proved end-to-end learning works\\\")\\n    print(\\\"  üìä Established scaling laws (bigger = better)\\\")\\n    print(\\\"  üîß Introduced techniques still used today\\\")\\n    print(\\\"  üåç Enabled countless AI applications\\\")\\n    print(\\\"  üí° Inspired a generation of researchers\\\")\\n\\nexplore_alexnet_legacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843fc592",
   "metadata": {},
   "source": [
    "## Part 7: Your Turn to Experiment!\n",
    "\n",
    "Now it's your turn to experiment with AlexNet! Try different modifications and see how they affect performance.\n",
    "\n",
    "### Suggested Experiments:\n",
    "\n",
    "1. **Architecture Modifications**: Try different filter sizes, add/remove layers\n",
    "2. **Activation Functions**: Compare ReLU variants (LeakyReLU, ELU, Swish)\n",
    "3. **Regularization**: Experiment with different dropout rates\n",
    "4. **Data Augmentation**: Add new augmentation techniques\n",
    "5. **Optimization**: Try different optimizers and learning schedules\n",
    "\n",
    "Use the cells below to conduct your own experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment cell 1: Architecture modifications\n",
    "def experiment_architecture():\n",
    "    \\\"\\\"\\\"Experiment with AlexNet architecture modifications.\\\"\\\"\\\"\n",
    "    \\n    print(\\\"üî¨ Architecture Modification Experiment\\\")\\n    \\n    # TODO: Design your architecture experiment here!\\n    # Ideas:\\n    # - Try smaller/larger kernel sizes\\n    # - Add/remove layers\\n    # - Change the number of filters\\n    # - Experiment with different pooling strategies\\n    \\n    # Example: Smaller AlexNet\\n    class MiniAlexNet(nn.Module):\\n        def __init__(self, num_classes=10):\\n            super().__init__()\\n            self.features = nn.Sequential(\\n                nn.Conv2d(3, 32, 7, 2, 2),  # Smaller kernels and filters\\n                nn.ReLU(inplace=True),\\n                nn.MaxPool2d(3, 2),\\n                nn.Conv2d(32, 64, 5, 1, 2),\\n                nn.ReLU(inplace=True),\\n                nn.MaxPool2d(3, 2),\\n                nn.Conv2d(64, 128, 3, 1, 1),\\n                nn.ReLU(inplace=True),\\n                nn.AdaptiveAvgPool2d((6, 6))  # Adaptive pooling\\n            )\\n            self.classifier = nn.Sequential(\\n                nn.Dropout(0.5),\\n                nn.Linear(128 * 6 * 6, 512),  # Smaller fully connected\\n                nn.ReLU(inplace=True),\\n                nn.Dropout(0.5),\\n                nn.Linear(512, num_classes)\\n            )\\n        \\n        def forward(self, x):\\n            x = self.features(x)\\n            x = torch.flatten(x, 1)\\n            x = self.classifier(x)\\n            return x\\n    \\n    # Create and analyze the mini model\\n    mini_model = MiniAlexNet(num_classes=10)\\n    \\n    # Count parameters\\n    total_params = sum(p.numel() for p in mini_model.parameters())\\n    original_params = sum(p.numel() for p in alexnet_model.parameters())\\n    \\n    print(f\\\"\\\\nüìä Parameter Comparison:\\\")\\n    print(f\\\"  Original AlexNet: {original_params:,} parameters\\\")\\n    print(f\\\"  Mini AlexNet: {total_params:,} parameters\\\")\\n    print(f\\\"  Reduction: {original_params/total_params:.1f}x smaller\\\")\\n    \\n    # Test forward pass\\n    test_input = torch.randn(1, 3, 224, 224)\\n    with torch.no_grad():\\n        output = mini_model(test_input)\\n        print(f\\\"\\\\n‚úÖ Mini AlexNet output shape: {output.shape}\\\")\\n    \\n    print(\\\"\\\\nüéØ Experiment Ideas:\\\")\\n    print(\\\"  - Compare training speed vs accuracy\\\")\\n    print(\\\"  - Test on different image sizes\\\")\\n    print(\\\"  - Try different activation functions\\\")\\n\\n# Run your architecture experiment\\nexperiment_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ff2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment cell 2: Advanced techniques\n",
    "def experiment_advanced_techniques():\n",
    "    \\\"\\\"\\\"Experiment with advanced training techniques.\\\"\\\"\\\"\n",
    "    \\n    print(\\\"üöÄ Advanced Techniques Experiment\\\")\\n    \\n    # Experiment with different activation functions\\n    activations = {\\n        'ReLU': nn.ReLU(),\\n        'LeakyReLU': nn.LeakyReLU(0.01),\\n        'ELU': nn.ELU(),\\n        'GELU': nn.GELU()\\n    }\\n    \\n    print(\\\"\\\\n‚ö° Comparing Activation Functions:\\\")\\n    \\n    # Test different activations\\n    x = torch.linspace(-3, 3, 1000)\\n    \\n    plt.figure(figsize=(12, 8))\\n    \\n    for i, (name, activation) in enumerate(activations.items()):\\n        plt.subplot(2, 2, i+1)\\n        \\n        with torch.no_grad():\\n            y = activation(x)\\n        \\n        plt.plot(x.numpy(), y.numpy(), linewidth=2, label=name)\\n        plt.grid(True, alpha=0.3)\\n        plt.xlabel('Input')\\n        plt.ylabel('Output')\\n        plt.title(f'{name} Activation')\\n        plt.legend()\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    # Learning rate scheduling experiment\\n    print(\\\"\\\\nüìà Learning Rate Scheduling:\\\")\\n    \\n    # Simulate different LR schedules\\n    epochs = list(range(100))\\n    \\n    # Different schedules\\n    constant_lr = [0.01] * 100\\n    step_lr = [0.01 if epoch < 30 else 0.001 if epoch < 60 else 0.0001 for epoch in epochs]\\n    cosine_lr = [0.01 * (np.cos(np.pi * epoch / 100) + 1) / 2 for epoch in epochs]\\n    exponential_lr = [0.01 * (0.95 ** epoch) for epoch in epochs]\\n    \\n    plt.figure(figsize=(10, 6))\\n    plt.plot(epochs, constant_lr, label='Constant', linewidth=2)\\n    plt.plot(epochs, step_lr, label='Step Decay', linewidth=2)\\n    plt.plot(epochs, cosine_lr, label='Cosine Annealing', linewidth=2)\\n    plt.plot(epochs, exponential_lr, label='Exponential Decay', linewidth=2)\\n    \\n    plt.xlabel('Epoch')\\n    plt.ylabel('Learning Rate')\\n    plt.title('Learning Rate Schedules Comparison')\\n    plt.legend()\\n    plt.grid(True, alpha=0.3)\\n    plt.yscale('log')\\n    plt.show()\\n    \\n    print(\\\"\\\\nüí° Advanced Technique Ideas:\\\")\\n    print(\\\"  üîÑ Cyclical learning rates\\\")\\n    print(\\\"  üéØ Label smoothing\\\")\\n    print(\\\"  üåÄ Mixup data augmentation\\\")\\n    print(\\\"  ‚öñÔ∏è Batch normalization\\\")\\n    print(\\\"  üîß Weight initialization strategies\\\")\\n\\n# Run advanced techniques experiment\\nexperiment_advanced_techniques()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce005f0",
   "metadata": {},
   "source": [
    "## Conclusions and Takeaways\n",
    "\n",
    "üéâ **Congratulations!** You've completed an in-depth exploration of AlexNet and the deep learning revolution it sparked.\n",
    "\n",
    "### Key Insights Discovered:\n",
    "\n",
    "1. **Scale Matters**: Bigger networks + more data + better hardware = breakthrough performance\n",
    "2. **Simple Works**: ReLU is simpler than sigmoid but much more effective\n",
    "3. **Regularization is Crucial**: Dropout prevents overfitting in large networks\n",
    "4. **End-to-End Learning**: Let the network learn features rather than hand-crafting them\n",
    "5. **Hardware Enables Progress**: GPUs made deep learning practically feasible\n",
    "6. **Competition Drives Innovation**: ImageNet challenge focused the field\n",
    "\n",
    "### Why AlexNet Changed Everything:\n",
    "\n",
    "- **Proof of Concept**: Showed that deep learning could work at scale\n",
    "- **Performance Breakthrough**: 84.7% vs 74% accuracy - an impossible-to-ignore improvement\n",
    "- **Technique Validation**: Proved ReLU, dropout, and data augmentation work\n",
    "- **Industry Catalyst**: Triggered massive investment in AI research\n",
    "- **Academic Shift**: Changed focus from feature engineering to architecture engineering\n",
    "\n",
    "### Modern Applications:\n",
    "\n",
    "Every time you use photo tagging, medical imaging, autonomous vehicles, or visual search, you're benefiting from AlexNet's innovations. The techniques it pioneered power:\n",
    "\n",
    "- **Social Media**: Automatic photo tagging and content moderation\n",
    "- **Healthcare**: Medical image analysis and diagnosis assistance  \n",
    "- **Transportation**: Computer vision for autonomous vehicles\n",
    "- **Security**: Facial recognition and surveillance systems\n",
    "- **Commerce**: Visual search and product recommendations\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Explore Deeper Networks**: ResNet, DenseNet, EfficientNet\n",
    "2. **Try Transfer Learning**: Use pretrained models for your tasks\n",
    "3. **Experiment with Architectures**: Design your own CNN variants\n",
    "4. **Apply to Real Problems**: Use computer vision for practical applications\n",
    "\n",
    "The journey from AlexNet to modern AI shows how a single breakthrough can cascade into transforming entire industries. Every pixel processed by AI today carries forward the legacy of those first 8 layers that dared to dream deep! üöÄüß†‚ú®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
