{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro-cell",
            "metadata": {},
            "source": [
                "# Day 12: Dropout - A Simple Way to Prevent Neural Networks from Overfitting üé≤\n",
                "\n",
                "Welcome to Day 12 of 30 Papers in 30 Days!\n",
                "\n",
                "Today we're exploring **Dropout** - the elegantly simple regularization technique that revolutionized neural network training. It's like training an ensemble of networks for the price of one!\n",
                "\n",
                "## What You'll Learn\n",
                "\n",
                "1. **Why Overfitting Happens**: The curse of too many parameters\n",
                "2. **The Dropout Solution**: Random neuron silencing during training\n",
                "3. **Inverted Dropout**: The practical implementation trick\n",
                "4. **Dropout Variants**: Spatial, DropConnect, AlphaDropout\n",
                "5. **MC Dropout**: Uncertainty estimation for free!\n",
                "6. **Implementation**: Build dropout from scratch\n",
                "\n",
                "## The Big Idea (in 30 seconds)\n",
                "\n",
                "**Problem**: Neural networks overfit. They memorize training data instead of learning patterns.\n",
                "\n",
                "**Solution**: During training, randomly \"drop\" (zero out) neurons with probability p!\n",
                "\n",
                "**Why it works**:\n",
                "- No neuron can rely on any other specific neuron\n",
                "- Forces redundant representations\n",
                "- Like training an exponential ensemble of networks!\n",
                "\n",
                "**Result**: Better generalization, less overfitting!\n",
                "\n",
                "Let's dive in! üöÄ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup and imports\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Add current directory to path\n",
                "sys.path.append('.')\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "np.random.seed(42)\n",
                "\n",
                "# Import our implementations\n",
                "from implementation import Dropout, Dropout2D, NaiveDropout, DropoutNetwork\n",
                "from train_minimal import load_mnist, DropoutMLP, SGD, train_epoch, evaluate\n",
                "from visualization import (\n",
                "    plot_dropout_masks, \n",
                "    plot_training_curves_comparison,\n",
                "    plot_ensemble_interpretation\n",
                ")\n",
                "\n",
                "print(\"‚úÖ All imports successful!\")\n",
                "print(\"üé≤ Ready to explore dropout!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part1-intro",
            "metadata": {},
            "source": [
                "## Part 1: Understanding the Problem - Overfitting\n",
                "\n",
                "Before we solve a problem, let's understand it. Neural networks have many parameters - sometimes millions or billions. With that many knobs to turn, it's easy to perfectly fit the training data while failing on new data.\n",
                "\n",
                "Let's visualize this problem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "overfitting-demo",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate overfitting\n",
                "def demonstrate_overfitting():\n",
                "    \"\"\"Show how a network can overfit to training data.\"\"\"\n",
                "    \n",
                "    print(\"üìä Demonstrating Overfitting...\")\n",
                "    \n",
                "    # Load MNIST data\n",
                "    X_train, y_train, X_test, y_test = load_mnist()\n",
                "    \n",
                "    # Use small subset to make overfitting more obvious\n",
                "    X_train_small = X_train[:1000]\n",
                "    y_train_small = y_train[:1000]\n",
                "    \n",
                "    print(f\"Training samples: {len(X_train_small)}\")\n",
                "    print(f\"Test samples: {len(X_test)}\")\n",
                "    \n",
                "    # Train WITHOUT dropout (prone to overfitting)\n",
                "    print(\"\\nTraining WITHOUT dropout...\")\n",
                "    \n",
                "    model_no_dropout = DropoutMLP(\n",
                "        input_size=784,\n",
                "        hidden_sizes=[512, 256],\n",
                "        output_size=10,\n",
                "        dropout_p=1.0,  # No dropout (keep everything)\n",
                "        input_dropout_p=1.0\n",
                "    )\n",
                "    \n",
                "    optimizer = SGD(model_no_dropout.get_params(), lr=0.01, momentum=0.9)\n",
                "    \n",
                "    train_accs_no_drop = []\n",
                "    test_accs_no_drop = []\n",
                "    \n",
                "    for epoch in range(30):\n",
                "        train_epoch(model_no_dropout, X_train_small, y_train_small, optimizer, batch_size=32)\n",
                "        \n",
                "        train_acc, _ = evaluate(model_no_dropout, X_train_small, y_train_small)\n",
                "        test_acc, _ = evaluate(model_no_dropout, X_test, y_test)\n",
                "        \n",
                "        train_accs_no_drop.append(train_acc)\n",
                "        test_accs_no_drop.append(test_acc)\n",
                "        \n",
                "        if (epoch + 1) % 10 == 0:\n",
                "            print(f\"  Epoch {epoch+1}: Train {train_acc:.3f} | Test {test_acc:.3f} | Gap {train_acc - test_acc:.3f}\")\n",
                "    \n",
                "    # Train WITH dropout\n",
                "    print(\"\\nTraining WITH dropout (p=0.5)...\")\n",
                "    \n",
                "    np.random.seed(42)  # Reset for fair comparison\n",
                "    \n",
                "    model_dropout = DropoutMLP(\n",
                "        input_size=784,\n",
                "        hidden_sizes=[512, 256],\n",
                "        output_size=10,\n",
                "        dropout_p=0.5,  # Keep 50% of neurons\n",
                "        input_dropout_p=0.9\n",
                "    )\n",
                "    \n",
                "    optimizer = SGD(model_dropout.get_params(), lr=0.01, momentum=0.9)\n",
                "    \n",
                "    train_accs_drop = []\n",
                "    test_accs_drop = []\n",
                "    \n",
                "    for epoch in range(30):\n",
                "        train_epoch(model_dropout, X_train_small, y_train_small, optimizer, batch_size=32)\n",
                "        \n",
                "        train_acc, _ = evaluate(model_dropout, X_train_small, y_train_small)\n",
                "        test_acc, _ = evaluate(model_dropout, X_test, y_test)\n",
                "        \n",
                "        train_accs_drop.append(train_acc)\n",
                "        test_accs_drop.append(test_acc)\n",
                "        \n",
                "        if (epoch + 1) % 10 == 0:\n",
                "            print(f\"  Epoch {epoch+1}: Train {train_acc:.3f} | Test {test_acc:.3f} | Gap {train_acc - test_acc:.3f}\")\n",
                "    \n",
                "    # Plot comparison\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # No dropout\n",
                "    ax = axes[0]\n",
                "    ax.plot(train_accs_no_drop, 'b-', label='Train', linewidth=2)\n",
                "    ax.plot(test_accs_no_drop, 'r-', label='Test', linewidth=2)\n",
                "    ax.fill_between(range(30), test_accs_no_drop, train_accs_no_drop, alpha=0.2, color='red')\n",
                "    ax.set_xlabel('Epoch', fontsize=12)\n",
                "    ax.set_ylabel('Accuracy', fontsize=12)\n",
                "    ax.set_title('WITHOUT Dropout: Overfitting! ‚ùå', fontsize=14, weight='bold', color='red')\n",
                "    ax.legend(fontsize=11)\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    ax.text(15, 0.75, f'Gap: {train_accs_no_drop[-1] - test_accs_no_drop[-1]:.2f}', \n",
                "           fontsize=12, ha='center', color='red', weight='bold')\n",
                "    \n",
                "    # With dropout\n",
                "    ax = axes[1]\n",
                "    ax.plot(train_accs_drop, 'b-', label='Train', linewidth=2)\n",
                "    ax.plot(test_accs_drop, 'g-', label='Test', linewidth=2)\n",
                "    ax.fill_between(range(30), test_accs_drop, train_accs_drop, alpha=0.2, color='green')\n",
                "    ax.set_xlabel('Epoch', fontsize=12)\n",
                "    ax.set_ylabel('Accuracy', fontsize=12)\n",
                "    ax.set_title('WITH Dropout: Much Better! ‚úÖ', fontsize=14, weight='bold', color='green')\n",
                "    ax.legend(fontsize=11)\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    ax.text(15, 0.75, f'Gap: {train_accs_drop[-1] - test_accs_drop[-1]:.2f}', \n",
                "           fontsize=12, ha='center', color='green', weight='bold')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\nüí° Key Observation:\")\n",
                "    print(f\"  Without dropout: Train-Test gap = {train_accs_no_drop[-1] - test_accs_no_drop[-1]:.3f}\")\n",
                "    print(f\"  With dropout:    Train-Test gap = {train_accs_drop[-1] - test_accs_drop[-1]:.3f}\")\n",
                "    print(f\"  Dropout reduces overfitting by ~{100*(1 - (train_accs_drop[-1] - test_accs_drop[-1])/(train_accs_no_drop[-1] - test_accs_no_drop[-1])):.0f}%!\")\n",
                "\n",
                "demonstrate_overfitting()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part2-intro",
            "metadata": {},
            "source": [
                "## Part 2: How Dropout Works\n",
                "\n",
                "Dropout is beautifully simple:\n",
                "\n",
                "1. **During Training**: For each forward pass, randomly zero out neurons with probability (1-p)\n",
                "2. **During Inference**: Use all neurons (no dropout)\n",
                "\n",
                "Let's visualize what a dropout mask looks like."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dropout-mask-viz",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize dropout masks\n",
                "def visualize_dropout_mechanism():\n",
                "    \"\"\"Show how dropout masks work.\"\"\"\n",
                "    \n",
                "    print(\"üé≤ Visualizing Dropout Masks...\")\n",
                "    \n",
                "    # Different keep probabilities\n",
                "    keep_probs = [0.9, 0.7, 0.5, 0.3]\n",
                "    \n",
                "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "    \n",
                "    for i, p in enumerate(keep_probs):\n",
                "        # Generate mask for 16x16 layer\n",
                "        mask = (np.random.rand(16, 16) < p).astype(float)\n",
                "        \n",
                "        # Show mask\n",
                "        ax = axes[0, i]\n",
                "        ax.imshow(mask, cmap='RdYlGn', vmin=0, vmax=1)\n",
                "        ax.set_title(f'keep_prob = {p}\\n{int(mask.sum())}/256 active', fontsize=11, weight='bold')\n",
                "        ax.axis('off')\n",
                "        \n",
                "        # Show histogram of multiple samples\n",
                "        ax = axes[1, i]\n",
                "        active_counts = [(np.random.rand(256) < p).sum() for _ in range(1000)]\n",
                "        ax.hist(active_counts, bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
                "        ax.axvline(256*p, color='red', linestyle='--', linewidth=2, label=f'Expected: {256*p:.0f}')\n",
                "        ax.set_xlabel('Active Neurons', fontsize=10)\n",
                "        ax.set_ylabel('Count', fontsize=10)\n",
                "        ax.legend(fontsize=9)\n",
                "        ax.grid(True, alpha=0.3)\n",
                "    \n",
                "    axes[0, 0].set_ylabel('Dropout Mask\\n(Green=Active)', fontsize=11)\n",
                "    axes[1, 0].set_ylabel('Distribution\\n(1000 samples)', fontsize=11)\n",
                "    \n",
                "    plt.suptitle('Dropout Masks at Different Keep Probabilities', fontsize=14, weight='bold', y=1.02)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\nüí° Key Insights:\")\n",
                "    print(\"  ‚Ä¢ Each training step uses a DIFFERENT random mask\")\n",
                "    print(\"  ‚Ä¢ p=0.5 is the standard for hidden layers\")\n",
                "    print(\"  ‚Ä¢ p=0.8-0.9 for input layers (don't drop too much input!)\")\n",
                "    print(\"  ‚Ä¢ p=1.0 for output layers (need all outputs!)\")\n",
                "\n",
                "visualize_dropout_mechanism()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part3-intro",
            "metadata": {},
            "source": [
                "## Part 3: Inverted Dropout - The Practical Trick\n",
                "\n",
                "There are two ways to implement dropout:\n",
                "\n",
                "**Original (Naive) Dropout:**\n",
                "- Training: Apply mask, no scaling\n",
                "- Inference: Scale by p\n",
                "\n",
                "**Inverted Dropout (Standard):**\n",
                "- Training: Apply mask AND scale by 1/p\n",
                "- Inference: No change needed!\n",
                "\n",
                "Inverted dropout is better because inference is simpler (no scaling)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inverted-dropout",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare inverted vs naive dropout\n",
                "def compare_dropout_implementations():\n",
                "    \"\"\"Compare inverted vs naive dropout.\"\"\"\n",
                "    \n",
                "    print(\"üîÑ Comparing Inverted vs Naive Dropout...\")\n",
                "    \n",
                "    # Create sample input\n",
                "    x = np.ones((1, 10))\n",
                "    p = 0.5  # Keep probability\n",
                "    \n",
                "    print(f\"\\nInput: {x[0]}\")\n",
                "    print(f\"Keep probability: {p}\")\n",
                "    \n",
                "    # Set same random seed for comparison\n",
                "    np.random.seed(42)\n",
                "    mask = (np.random.rand(*x.shape) < p).astype(float)\n",
                "    print(f\"\\nMask: {mask[0].astype(int)}\")\n",
                "    \n",
                "    # Naive Dropout\n",
                "    print(\"\\n--- NAIVE DROPOUT ---\")\n",
                "    naive_train = x * mask  # No scaling during training\n",
                "    naive_test = x * p       # Scale during inference\n",
                "    \n",
                "    print(f\"Training output:  {naive_train[0]}\")\n",
                "    print(f\"Inference output: {naive_test[0]}\")\n",
                "    print(f\"Expected value (train): {naive_train.mean():.3f}\")\n",
                "    print(f\"Expected value (test):  {naive_test.mean():.3f}\")\n",
                "    \n",
                "    # Inverted Dropout\n",
                "    print(\"\\n--- INVERTED DROPOUT ---\")\n",
                "    inverted_train = (x * mask) / p  # Scale during training\n",
                "    inverted_test = x                 # No change during inference\n",
                "    \n",
                "    print(f\"Training output:  {inverted_train[0]}\")\n",
                "    print(f\"Inference output: {inverted_test[0]}\")\n",
                "    print(f\"Expected value (train): {inverted_train.mean():.3f}\")\n",
                "    print(f\"Expected value (test):  {inverted_test.mean():.3f}\")\n",
                "    \n",
                "    # Verify expected values match\n",
                "    print(\"\\nüí° Key Insight:\")\n",
                "    print(\"  With inverted dropout, the expected value during training\")\n",
                "    print(\"  equals the value during inference (1.0 in this case).\")\n",
                "    print(\"  This makes inference simpler - no scaling needed!\")\n",
                "    \n",
                "    # Statistical verification\n",
                "    print(\"\\nüìä Statistical Verification (1000 trials):\")\n",
                "    \n",
                "    naive_means = []\n",
                "    inverted_means = []\n",
                "    \n",
                "    for _ in range(1000):\n",
                "        mask = (np.random.rand(*x.shape) < p).astype(float)\n",
                "        naive_means.append((x * mask).mean())\n",
                "        inverted_means.append(((x * mask) / p).mean())\n",
                "    \n",
                "    print(f\"  Naive training mean:    {np.mean(naive_means):.4f} (should be ~0.5)\")\n",
                "    print(f\"  Inverted training mean: {np.mean(inverted_means):.4f} (should be ~1.0)\")\n",
                "\n",
                "compare_dropout_implementations()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part4-intro",
            "metadata": {},
            "source": [
                "## Part 4: Why Dropout Works - The Ensemble Interpretation\n",
                "\n",
                "Dropout can be understood as training an **exponential ensemble** of networks!\n",
                "\n",
                "With n neurons and dropout, there are 2^n possible sub-networks. Each training step trains a different sub-network. At inference, we use the average of all these sub-networks.\n",
                "\n",
                "This is like getting an ensemble for free!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ensemble-viz",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the ensemble interpretation\n",
                "def visualize_ensemble_nature():\n",
                "    \"\"\"Show dropout as implicit ensemble training.\"\"\"\n",
                "    \n",
                "    print(\"üîÄ Dropout as Ensemble Learning...\")\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "    \n",
                "    # Sample sub-networks\n",
                "    n_neurons = 8\n",
                "    p = 0.5\n",
                "    \n",
                "    # Show 3 different \"sub-networks\" (dropout masks)\n",
                "    for i in range(3):\n",
                "        ax = axes[i]\n",
                "        \n",
                "        # Generate random mask\n",
                "        mask = (np.random.rand(1, n_neurons) < p)[0]\n",
                "        \n",
                "        # Visualize as network diagram\n",
                "        # Input layer\n",
                "        for j in range(4):\n",
                "            ax.scatter(0, j, s=300, c='steelblue', zorder=3)\n",
                "        \n",
                "        # Hidden layer (with dropout)\n",
                "        for j in range(n_neurons):\n",
                "            y = j - (n_neurons - 4) / 2\n",
                "            color = 'green' if mask[j] else 'lightgray'\n",
                "            alpha = 1.0 if mask[j] else 0.3\n",
                "            ax.scatter(1, y, s=300, c=color, alpha=alpha, zorder=3, edgecolor='black')\n",
                "            \n",
                "            # Draw connections\n",
                "            for k in range(4):\n",
                "                if mask[j]:\n",
                "                    ax.plot([0, 1], [k, y], 'g-', alpha=0.3, linewidth=1)\n",
                "                else:\n",
                "                    ax.plot([0, 1], [k, y], 'gray', alpha=0.1, linewidth=0.5)\n",
                "        \n",
                "        # Output layer\n",
                "        for j in range(2):\n",
                "            y = j + 0.5\n",
                "            ax.scatter(2, y, s=300, c='orange', zorder=3)\n",
                "            \n",
                "            for k in range(n_neurons):\n",
                "                ky = k - (n_neurons - 4) / 2\n",
                "                if mask[k]:\n",
                "                    ax.plot([1, 2], [ky, y], 'g-', alpha=0.3, linewidth=1)\n",
                "        \n",
                "        ax.set_xlim(-0.5, 2.5)\n",
                "        ax.set_ylim(-3, 6)\n",
                "        ax.set_title(f'Sub-network {i+1}\\n({int(mask.sum())}/{n_neurons} neurons active)', \n",
                "                    fontsize=12, weight='bold')\n",
                "        ax.axis('off')\n",
                "    \n",
                "    plt.suptitle('Each Training Step = Different Sub-Network!', fontsize=14, weight='bold', y=1.02)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Statistics\n",
                "    print(f\"\\nüìä Ensemble Statistics:\")\n",
                "    print(f\"  Hidden layer neurons: {n_neurons}\")\n",
                "    print(f\"  Possible sub-networks: 2^{n_neurons} = {2**n_neurons}\")\n",
                "    print(f\"\\n  For a real network with 512 hidden neurons:\")\n",
                "    print(f\"  Possible sub-networks: 2^512 ‚âà 10^154 (more than atoms in universe!)\")\n",
                "    print(f\"\\nüí° Dropout = Implicit ensemble of exponentially many networks!\")\n",
                "\n",
                "visualize_ensemble_nature()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part5-intro",
            "metadata": {},
            "source": [
                "## Part 5: Finding the Optimal Dropout Rate\n",
                "\n",
                "The dropout rate is a hyperparameter. Too little dropout = overfitting. Too much dropout = underfitting.\n",
                "\n",
                "Let's find the sweet spot!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dropout-rate-sweep",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find optimal dropout rate\n",
                "def find_optimal_dropout_rate():\n",
                "    \"\"\"Sweep dropout rates to find optimal.\"\"\"\n",
                "    \n",
                "    print(\"üîç Finding Optimal Dropout Rate...\")\n",
                "    \n",
                "    # Load data\n",
                "    X_train, y_train, X_test, y_test = load_mnist()\n",
                "    X_train, y_train = X_train[:3000], y_train[:3000]  # Subset for speed\n",
                "    \n",
                "    # Dropout rates to test (keep probability)\n",
                "    dropout_rates = [1.0, 0.9, 0.7, 0.5, 0.3, 0.1]\n",
                "    \n",
                "    results = {}\n",
                "    epochs = 20\n",
                "    \n",
                "    for keep_prob in dropout_rates:\n",
                "        print(f\"\\nTesting keep_prob = {keep_prob}...\")\n",
                "        \n",
                "        np.random.seed(42)\n",
                "        \n",
                "        model = DropoutMLP(\n",
                "            input_size=784,\n",
                "            hidden_sizes=[256, 128],\n",
                "            output_size=10,\n",
                "            dropout_p=keep_prob,\n",
                "            input_dropout_p=1.0 if keep_prob == 1.0 else 0.9\n",
                "        )\n",
                "        \n",
                "        optimizer = SGD(model.get_params(), lr=0.01, momentum=0.9)\n",
                "        \n",
                "        for epoch in range(epochs):\n",
                "            train_epoch(model, X_train, y_train, optimizer, batch_size=64)\n",
                "        \n",
                "        train_acc, _ = evaluate(model, X_train, y_train)\n",
                "        test_acc, _ = evaluate(model, X_test, y_test)\n",
                "        \n",
                "        results[keep_prob] = {\n",
                "            'train_acc': train_acc,\n",
                "            'test_acc': test_acc,\n",
                "            'gap': train_acc - test_acc\n",
                "        }\n",
                "        \n",
                "        print(f\"  Train: {train_acc:.3f} | Test: {test_acc:.3f} | Gap: {train_acc - test_acc:.3f}\")\n",
                "    \n",
                "    # Plot results\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    probs = list(results.keys())\n",
                "    train_accs = [results[p]['train_acc'] for p in probs]\n",
                "    test_accs = [results[p]['test_acc'] for p in probs]\n",
                "    gaps = [results[p]['gap'] for p in probs]\n",
                "    \n",
                "    # Accuracy plot\n",
                "    ax = axes[0]\n",
                "    ax.plot(probs, train_accs, 'b-o', label='Train', linewidth=2, markersize=10)\n",
                "    ax.plot(probs, test_accs, 'g-s', label='Test', linewidth=2, markersize=10)\n",
                "    \n",
                "    best_p = probs[np.argmax(test_accs)]\n",
                "    ax.axvline(best_p, color='green', linestyle='--', alpha=0.7, label=f'Best: {best_p}')\n",
                "    \n",
                "    ax.set_xlabel('Keep Probability (p)', fontsize=12)\n",
                "    ax.set_ylabel('Accuracy', fontsize=12)\n",
                "    ax.set_title('Accuracy vs Dropout Rate', fontsize=14, weight='bold')\n",
                "    ax.legend(fontsize=11)\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    ax.set_xlim(0, 1.1)\n",
                "    \n",
                "    # Gap plot\n",
                "    ax = axes[1]\n",
                "    colors = ['red' if g > 0.1 else 'orange' if g > 0.05 else 'green' for g in gaps]\n",
                "    ax.bar(range(len(probs)), gaps, color=colors, alpha=0.7, edgecolor='black')\n",
                "    ax.set_xticks(range(len(probs)))\n",
                "    ax.set_xticklabels([f'{p:.1f}' for p in probs])\n",
                "    ax.set_xlabel('Keep Probability (p)', fontsize=12)\n",
                "    ax.set_ylabel('Train-Test Gap', fontsize=12)\n",
                "    ax.set_title('Overfitting vs Dropout Rate', fontsize=14, weight='bold')\n",
                "    ax.grid(True, alpha=0.3, axis='y')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\nüí° Best dropout rate: keep_prob = {best_p}\")\n",
                "    print(f\"  Test accuracy: {results[best_p]['test_acc']:.3f}\")\n",
                "    print(f\"  Train-Test gap: {results[best_p]['gap']:.3f}\")\n",
                "    \n",
                "    return results\n",
                "\n",
                "results = find_optimal_dropout_rate()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part6-intro",
            "metadata": {},
            "source": [
                "## Part 6: Spatial Dropout (Dropout2D)\n",
                "\n",
                "For CNNs, standard dropout isn't ideal because nearby pixels are correlated. If you drop one pixel, its neighbors can \"fill in\" the missing information.\n",
                "\n",
                "**Spatial Dropout (Dropout2D)** drops entire feature channels instead!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "spatial-dropout",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate spatial dropout\n",
                "def demonstrate_spatial_dropout():\n",
                "    \"\"\"Compare standard vs spatial dropout.\"\"\"\n",
                "    \n",
                "    print(\"üñºÔ∏è Demonstrating Spatial Dropout...\")\n",
                "    \n",
                "    # Create sample feature map (batch=1, channels=4, height=8, width=8)\n",
                "    np.random.seed(42)\n",
                "    x = np.random.randn(1, 4, 8, 8)\n",
                "    \n",
                "    print(f\"Input shape: {x.shape}\")\n",
                "    print(f\"  Batch: {x.shape[0]}\")\n",
                "    print(f\"  Channels: {x.shape[1]}\")\n",
                "    print(f\"  Height x Width: {x.shape[2]}x{x.shape[3]}\")\n",
                "    \n",
                "    # Standard dropout (flattened)\n",
                "    standard_dropout = Dropout(p=0.5)\n",
                "    x_flat = x.reshape(1, -1)\n",
                "    y_standard = standard_dropout.forward(x_flat).reshape(x.shape)\n",
                "    \n",
                "    # Spatial dropout\n",
                "    spatial_dropout = Dropout2D(p=0.5)\n",
                "    y_spatial = spatial_dropout.forward(x)\n",
                "    \n",
                "    # Visualize\n",
                "    fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
                "    \n",
                "    # Original channels\n",
                "    for c in range(4):\n",
                "        axes[0, c].imshow(x[0, c], cmap='viridis')\n",
                "        axes[0, c].set_title(f'Original Ch {c}', fontsize=10)\n",
                "        axes[0, c].axis('off')\n",
                "    axes[0, 0].set_ylabel('Original', fontsize=12, rotation=0, ha='right', labelpad=40)\n",
                "    \n",
                "    # Standard dropout\n",
                "    for c in range(4):\n",
                "        axes[1, c].imshow(y_standard[0, c], cmap='viridis')\n",
                "        zeros_pct = 100 * np.sum(y_standard[0, c] == 0) / y_standard[0, c].size\n",
                "        axes[1, c].set_title(f'Ch {c}: {zeros_pct:.0f}% zeros', fontsize=10)\n",
                "        axes[1, c].axis('off')\n",
                "    axes[1, 0].set_ylabel('Standard\\nDropout', fontsize=12, rotation=0, ha='right', labelpad=40)\n",
                "    \n",
                "    # Spatial dropout\n",
                "    for c in range(4):\n",
                "        axes[2, c].imshow(y_spatial[0, c], cmap='viridis')\n",
                "        is_dropped = np.all(y_spatial[0, c] == 0)\n",
                "        status = 'DROPPED' if is_dropped else 'KEPT (2x)'\n",
                "        color = 'red' if is_dropped else 'green'\n",
                "        axes[2, c].set_title(f'Ch {c}: {status}', fontsize=10, color=color, weight='bold')\n",
                "        axes[2, c].axis('off')\n",
                "    axes[2, 0].set_ylabel('Spatial\\nDropout', fontsize=12, rotation=0, ha='right', labelpad=40)\n",
                "    \n",
                "    plt.suptitle('Standard Dropout vs Spatial Dropout (Dropout2D)', fontsize=14, weight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\nüí° Key Difference:\")\n",
                "    print(\"  Standard: Scattered zeros across all channels\")\n",
                "    print(\"  Spatial:  Entire channels are ON or OFF\")\n",
                "    print(\"\\n  For CNNs, spatial dropout is better because:\")\n",
                "    print(\"  ‚Ä¢ Nearby pixels are correlated\")\n",
                "    print(\"  ‚Ä¢ Dropping one pixel: neighbors can compensate\")\n",
                "    print(\"  ‚Ä¢ Dropping whole channel: must use different features\")\n",
                "\n",
                "demonstrate_spatial_dropout()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part7-intro",
            "metadata": {},
            "source": [
                "## Part 7: MC Dropout - Uncertainty for Free!\n",
                "\n",
                "One beautiful application of dropout is **Monte Carlo Dropout** (MC Dropout):\n",
                "\n",
                "1. Keep dropout ON during inference\n",
                "2. Run multiple forward passes\n",
                "3. Mean = prediction, Variance = uncertainty!\n",
                "\n",
                "This gives us uncertainty estimates without changing the model!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "mc-dropout",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate MC Dropout\n",
                "def demonstrate_mc_dropout():\n",
                "    \"\"\"Show how MC Dropout provides uncertainty estimates.\"\"\"\n",
                "    \n",
                "    print(\"üéØ Demonstrating MC Dropout for Uncertainty...\")\n",
                "    \n",
                "    # Load data and train model\n",
                "    X_train, y_train, X_test, y_test = load_mnist()\n",
                "    X_train, y_train = X_train[:3000], y_train[:3000]\n",
                "    \n",
                "    np.random.seed(42)\n",
                "    \n",
                "    model = DropoutMLP(\n",
                "        input_size=784,\n",
                "        hidden_sizes=[256, 128],\n",
                "        output_size=10,\n",
                "        dropout_p=0.5\n",
                "    )\n",
                "    \n",
                "    optimizer = SGD(model.get_params(), lr=0.01, momentum=0.9)\n",
                "    \n",
                "    print(\"Training model...\")\n",
                "    for epoch in range(15):\n",
                "        train_epoch(model, X_train, y_train, optimizer, batch_size=64)\n",
                "        if (epoch + 1) % 5 == 0:\n",
                "            train_acc, _ = evaluate(model, X_train, y_train)\n",
                "            print(f\"  Epoch {epoch+1}: Train acc = {train_acc:.3f}\")\n",
                "    \n",
                "    # Standard evaluation\n",
                "    print(\"\\n--- Standard Evaluation ---\")\n",
                "    model.eval()\n",
                "    test_acc, _ = evaluate(model, X_test, y_test)\n",
                "    print(f\"Test accuracy: {test_acc:.3f}\")\n",
                "    \n",
                "    # MC Dropout evaluation\n",
                "    print(\"\\n--- MC Dropout Evaluation ---\")\n",
                "    \n",
                "    n_samples = 100\n",
                "    n_test = 200\n",
                "    \n",
                "    X_subset = X_test[:n_test]\n",
                "    y_subset = y_test[:n_test]\n",
                "    \n",
                "    # Run multiple forward passes with dropout ON\n",
                "    model.train()  # Keep dropout active!\n",
                "    \n",
                "    all_predictions = []\n",
                "    for i in range(n_samples):\n",
                "        preds = model.forward(X_subset)\n",
                "        all_predictions.append(preds)\n",
                "    \n",
                "    all_predictions = np.stack(all_predictions, axis=0)  # (n_samples, n_test, 10)\n",
                "    \n",
                "    # Compute statistics\n",
                "    mean_pred = all_predictions.mean(axis=0)\n",
                "    std_pred = all_predictions.std(axis=0)\n",
                "    \n",
                "    # Get predictions and uncertainty\n",
                "    predicted_classes = mean_pred.argmax(axis=1)\n",
                "    uncertainty = std_pred.sum(axis=1)  # Total variance\n",
                "    \n",
                "    # Check correctness\n",
                "    correct = (predicted_classes == y_subset)\n",
                "    \n",
                "    print(f\"MC Dropout accuracy: {correct.mean():.3f}\")\n",
                "    print(f\"\\nUncertainty Analysis:\")\n",
                "    print(f\"  Correct predictions:   mean uncertainty = {uncertainty[correct].mean():.4f}\")\n",
                "    print(f\"  Incorrect predictions: mean uncertainty = {uncertainty[~correct].mean():.4f}\")\n",
                "    \n",
                "    # Plot\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # Uncertainty distribution\n",
                "    ax = axes[0]\n",
                "    ax.hist(uncertainty[correct], bins=20, alpha=0.7, label='Correct', color='green', density=True)\n",
                "    ax.hist(uncertainty[~correct], bins=20, alpha=0.7, label='Incorrect', color='red', density=True)\n",
                "    ax.set_xlabel('Uncertainty (Total Variance)', fontsize=12)\n",
                "    ax.set_ylabel('Density', fontsize=12)\n",
                "    ax.set_title('Uncertainty: Correct vs Incorrect', fontsize=14, weight='bold')\n",
                "    ax.legend(fontsize=11)\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Rejection curve\n",
                "    ax = axes[1]\n",
                "    \n",
                "    # Sort by uncertainty\n",
                "    order = np.argsort(uncertainty)\n",
                "    \n",
                "    reject_rates = np.linspace(0, 0.5, 20)\n",
                "    accuracies = []\n",
                "    \n",
                "    for reject_rate in reject_rates:\n",
                "        n_keep = int(n_test * (1 - reject_rate))\n",
                "        keep_indices = order[:n_keep]  # Keep least uncertain\n",
                "        acc = correct[keep_indices].mean()\n",
                "        accuracies.append(acc)\n",
                "    \n",
                "    ax.plot(100 * reject_rates, accuracies, 'b-o', linewidth=2, markersize=6)\n",
                "    ax.axhline(test_acc, color='r', linestyle='--', label='No rejection')\n",
                "    ax.set_xlabel('Rejection Rate (%)', fontsize=12)\n",
                "    ax.set_ylabel('Accuracy on Remaining', fontsize=12)\n",
                "    ax.set_title('Accuracy vs Rejection Rate', fontsize=14, weight='bold')\n",
                "    ax.legend(fontsize=11)\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\nüí° Key Insights:\")\n",
                "    ratio = uncertainty[~correct].mean() / uncertainty[correct].mean()\n",
                "    print(f\"  ‚Ä¢ Wrong predictions are {ratio:.1f}x more uncertain\")\n",
                "    print(f\"  ‚Ä¢ Rejecting uncertain samples improves accuracy\")\n",
                "    print(f\"  ‚Ä¢ MC Dropout = Free uncertainty estimation!\")\n",
                "\n",
                "demonstrate_mc_dropout()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part8-intro",
            "metadata": {},
            "source": [
                "## Part 8: Key Takeaways\n",
                "\n",
                "Let's summarize what we've learned about dropout!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "summary",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary\n",
                "print(\"=\"*60)\n",
                "print(\"KEY TAKEAWAYS: DROPOUT\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "print(\"\"\"\n",
                "üéØ WHAT IS DROPOUT?\n",
                "   Randomly zero out neurons during training to prevent overfitting.\n",
                "\n",
                "üîß IMPLEMENTATION:\n",
                "   ‚Ä¢ Training: mask = (rand < p); output = (input * mask) / p\n",
                "   ‚Ä¢ Inference: output = input (no change!)\n",
                "\n",
                "‚öôÔ∏è RECOMMENDED RATES:\n",
                "   ‚Ä¢ Input layer:  p = 0.8-0.9 (keep most input)\n",
                "   ‚Ä¢ Hidden layers: p = 0.5 (standard)\n",
                "   ‚Ä¢ Output layer: p = 1.0 (never dropout!)\n",
                "\n",
                "üß† WHY IT WORKS:\n",
                "   ‚Ä¢ Prevents co-adaptation of neurons\n",
                "   ‚Ä¢ Forces redundant representations\n",
                "   ‚Ä¢ Implicit ensemble of 2^n networks\n",
                "   ‚Ä¢ Adds beneficial noise during training\n",
                "\n",
                "üì¶ VARIANTS:\n",
                "   ‚Ä¢ Dropout:   Standard for dense layers\n",
                "   ‚Ä¢ Dropout2D: Spatial dropout for CNNs\n",
                "   ‚Ä¢ AlphaDropout: For SELU activations\n",
                "   ‚Ä¢ DropConnect: Drop weights, not neurons\n",
                "   ‚Ä¢ DropBlock: Drop contiguous regions\n",
                "\n",
                "üé≤ MC DROPOUT:\n",
                "   ‚Ä¢ Keep dropout ON at inference\n",
                "   ‚Ä¢ Run multiple forward passes\n",
                "   ‚Ä¢ Mean = prediction, Variance = uncertainty\n",
                "\n",
                "‚ö†Ô∏è COMMON MISTAKES:\n",
                "   ‚Ä¢ Forgetting model.eval() during inference\n",
                "   ‚Ä¢ Dropout on output layer\n",
                "   ‚Ä¢ Too aggressive dropout (underfitting)\n",
                "   ‚Ä¢ Using dropout with BatchNorm (tricky!)\n",
                "\n",
                "üöÄ MODERN USAGE:\n",
                "   ‚Ä¢ Still widely used in NLP (transformers)\n",
                "   ‚Ä¢ Less common in vision (BatchNorm preferred)\n",
                "   ‚Ä¢ Essential for uncertainty estimation\n",
                "\"\"\")\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üéâ Congratulations! You've mastered Dropout!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exercises-intro",
            "metadata": {},
            "source": [
                "## Exercises\n",
                "\n",
                "Ready to test your understanding? Head to the `exercises/` directory for 5 progressive challenges:\n",
                "\n",
                "1. **Build Dropout from Scratch** - Implement forward/backward passes\n",
                "2. **Dropout Rate Sweep** - Find the optimal rate empirically\n",
                "3. **Spatial Dropout** - Implement Dropout2D for CNNs\n",
                "4. **MC Dropout** - Build uncertainty estimation\n",
                "5. **Regularization Comparison** - Compare dropout with L2, early stopping\n",
                "\n",
                "Good luck! üöÄ"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}