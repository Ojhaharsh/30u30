{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c046d1",
   "metadata": {},
   "source": [
    "# Day 4: Keeping Neural Networks Simple (MDL)\n",
    "\n",
    "Hinton & van Camp (1993): \"Keeping Neural Networks Simple by Minimizing the Description Length of the Weights\"\n",
    "\n",
    "This paper showed that representing weights as **Gaussian distributions** and minimizing their description length produces the same objective as variational Bayesian inference. The result: networks that find flat, robust minima instead of sharp, brittle ones.\n",
    "\n",
    "## What this notebook covers\n",
    "\n",
    "1. **The gappy sine wave** — training on data with a missing region\n",
    "2. **Training a Bayesian network** — the loss is error + KL divergence (complexity)\n",
    "3. **Uncertainty visualization** — running multiple forward passes to see where the model is confident vs. uncertain\n",
    "4. **Weight distributions** — seeing which weights matter (low sigma) and which can be pruned (high sigma)\n",
    "\n",
    "## The core idea\n",
    "\n",
    "Standard weights are point estimates (w = 5.123). MDL weights are distributions (w ~ N(5.1, 0.2)). If the network works fine despite weight noise, you didn't need the precision — and the saved precision is literally saved bits of description length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from implementation import MDLNetwork\n",
    "from visualization import (plot_loss_dynamics, plot_uncertainty_envelope,\n",
    "                           plot_weight_distributions, plot_snr_analysis,\n",
    "                           analyze_compression_stats)\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c419d4",
   "metadata": {},
   "source": [
    "## 1. The Data: A Gappy Sine Wave\n",
    "\n",
    "To see why Bayesian weights are useful, we need a dataset that exposes the difference between \"confident\" and \"honest\" predictions.\n",
    "\n",
    "We generate a sine wave but **delete the middle part** (|x| < 1). A standard NN will confidently predict through the gap as if it has data there. A Bayesian NN should show high uncertainty in the gap — because it genuinely hasn't seen data there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noisy sine wave with a GAP\n",
    "def generate_gappy_data(n=100):\n",
    "    # Left side (-3 to -1)\n",
    "    X1 = np.random.uniform(-3, -1, n//2)\n",
    "    # Right side (1 to 3)\n",
    "    X2 = np.random.uniform(1, 3, n//2)\n",
    "    # Combine\n",
    "    X = np.concatenate([X1, X2])\n",
    "    # Add noise\n",
    "    y = np.sin(X) + np.random.normal(0, 0.1, n)\n",
    "    return X.reshape(-1, 1), y.reshape(-1, 1)\n",
    "\n",
    "X_train, y_train = generate_gappy_data(100)\n",
    "\n",
    "# Visualization range (including the gap)\n",
    "X_test = np.linspace(-4, 4, 200).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X_train, y_train, c='red', label='Training Data')\n",
    "plt.axvspan(-1, 1, color='gray', alpha=0.2, label='The GAP (No Data)')\n",
    "plt.plot(X_test, np.sin(X_test), 'k--', alpha=0.5, label='True Sine Wave')\n",
    "plt.title(\"The Challenge: Predict what happens in the Gap\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14a7f2",
   "metadata": {},
   "source": [
    "## 2. Training the Bayesian Network\n",
    "\n",
    "The MDL network has **two** loss terms (this is the paper's core equation):\n",
    "\n",
    "1. **Error Cost (NLL):** How well do the predictions fit the data?\n",
    "2. **Complexity Cost (KL):** How many bits to describe the weights?\n",
    "\n",
    "Total loss = Error + beta * KL\n",
    "\n",
    "The `kl_weight` (beta) controls the balance. Too low: overfits (no uncertainty). Too high: underfits (ignores data). The right value gives honest uncertainty where data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e458df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Network\n",
    "net = MDLNetwork(input_size=1, hidden_size=20, output_size=1)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 2000\n",
    "lr = 0.01\n",
    "kl_weight = 0.1  # The \"Simplicity Pressure\"\n",
    "\n",
    "# Storage for plotting\n",
    "history = {'total': [], 'nll': [], 'kl': []}\n",
    "\n",
    "print(\"Training Bayesian Network...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward Pass (This samples random weights!)\n",
    "    # Every time we call this, the network is slightly different.\n",
    "    preds = net.forward(X_train)\n",
    "    \n",
    "    # 2. Data Loss (MSE as proxy for Negative Log Likelihood)\n",
    "    nll = np.mean((preds - y_train)**2)\n",
    "    d_nll = 2 * (preds - y_train) / len(X_train)\n",
    "    \n",
    "    # 3. Complexity Loss (KL Divergence)\n",
    "    kl = net.total_kl() / len(X_train)\n",
    "    \n",
    "    # 4. Total Loss\n",
    "    loss = nll + kl_weight * kl\n",
    "    \n",
    "    # Store history\n",
    "    history['total'].append(loss)\n",
    "    history['nll'].append(nll)\n",
    "    history['kl'].append(kl)\n",
    "    \n",
    "    # 5. Backward Pass\n",
    "    net.backward(d_nll)\n",
    "    \n",
    "    # 6. Update Weights\n",
    "    net.update_weights(lr, kl_weight)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch:4d} | Total: {loss:.4f} | Error: {nll:.4f} | Complexity: {kl:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef8a10e",
   "metadata": {},
   "source": [
    "## 3. Loss Dynamics: Complexity vs. Error\n",
    "\n",
    "Watch how the two loss terms interact during training.\n",
    "The **Error** (NLL) drops quickly as the network fits the data. The **Complexity** (KL) may rise initially as weights move from the prior, then stabilizes as the network finds a compact solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a531767",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_dynamics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a13028",
   "metadata": {},
   "source": [
    "## 4. The Uncertainty Envelope\n",
    "\n",
    "The signature visualization for Bayesian neural networks.\n",
    "\n",
    "We run the network **100 times** on the test data. Because weights are distributions (not fixed numbers), each forward pass samples different weights and produces a slightly different prediction.\n",
    "\n",
    "* **Where we have training data:** predictions cluster tightly (low variance).\n",
    "* **In the gap:** predictions spread out (high variance) — the network honestly reflects that it has no data here.\n",
    "\n",
    "This spread is the model's epistemic uncertainty, and it comes directly from the Gaussian weight distributions in Hinton & van Camp's formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195fa287",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_uncertainty_envelope(net, X_train, y_train, X_test, n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554351f4",
   "metadata": {},
   "source": [
    "## 5. Weight Distributions\n",
    "\n",
    "The paper's title is about \"Minimizing Description Length of the Weights.\" Here's what that looks like in practice.\n",
    "\n",
    "Each weight has a learned sigma (standard deviation):\n",
    "* **Small sigma:** the network needs this weight to be precise — it carries information about the data.\n",
    "* **Large sigma:** this weight can be anything — it carries almost zero bits of information.\n",
    "\n",
    "Weights with large sigma relative to their mean (low signal-to-noise ratio) are effectively \"free\" under the bits-back coding scheme. They've been compressed away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weight_distributions(net, 'layer1')\n",
    "plot_snr_analysis(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b7d4a",
   "metadata": {},
   "source": [
    "## 6. Compression Statistics\n",
    "\n",
    "We can quantify compression by looking at Signal-to-Noise Ratio (SNR = |mu|/sigma).\n",
    "Weights with SNR below a threshold (e.g., 0.5) carry negligible information — they could be pruned to zero without hurting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_compression_stats(net, threshold_snr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1374daff",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "**1. Generalization through compression (paper's core argument):**\n",
    "The KL penalty forces the network to use as few bits as possible to describe its weights. Weights that don't help explain the data get pushed back toward the prior — effectively pruned. What remains is the simplest model that fits the training data, which is exactly the MDL principle.\n",
    "\n",
    "**2. Honest uncertainty (direct consequence of the formulation):**\n",
    "A standard network would draw a confident (and wrong) line through the gap in our data. The MDL network's weight distributions produce spread-out predictions where data is missing — it distinguishes \"I've seen this\" from \"I'm guessing.\"\n",
    "\n",
    "**3. Noise during training is the mechanism, not a trick:**\n",
    "Sampling weights from distributions during training is how the bits-back argument works. The noise in the weights is what allows the coding cost to be reduced (Hinton & van Camp, 1993, Section 2).\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Pointer Networks (Vinyals et al., 2015)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
