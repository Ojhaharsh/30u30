{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c046d1",
   "metadata": {},
   "source": [
    "# Day 4: Keeping Neural Networks Simple (MDL) üß†\n",
    "\n",
    "Welcome to Day 4 of 30 Papers in 30 Days!\n",
    "\n",
    "Today we are tackling a profound idea from Geoffrey Hinton (1993): **\"Keeping Neural Networks Simple by Minimizing the Description Length of the Weights\"**.\n",
    "\n",
    "This paper introduced the idea that **Compression = Generalization**. If you can describe your model with fewer bits, it will understand the world better.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1.  **The Problem**: Why standard networks are \"brittle\" and overconfident.\n",
    "2.  **The Solution**: How adding **noise** to weights forces simplicity.\n",
    "3.  **The Implementation**: Building a **Bayesian Neural Network** from scratch.\n",
    "4.  **The Visualization**: Seeing the famous **\"Uncertainty Envelope\"**.\n",
    "\n",
    "## The Big Idea (in 30 seconds)\n",
    "\n",
    "Imagine you are a teacher grading a student.\n",
    "* **Standard Student (Overfitting):** Memorizes the textbook word-for-word. Gets 100% on the practice test, but fails if you rephrase the question.\n",
    "* **MDL Student (Generalization):** Remembers the *concepts* loosely. Might get a few details wrong, but understands the logic and passes any test.\n",
    "\n",
    "We force the network to be the second student by telling it: **\"You are not allowed to memorize precise weights (e.g., 5.12391). You can only memorize fuzzy ranges (e.g., roughly 5).\"**\n",
    "\n",
    "Let's build it! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "# Import our MDL implementation\n",
    "from implementation import MDLNetwork\n",
    "from visualization import (\n",
    "    plot_uncertainty_envelope,\n",
    "    plot_weight_distributions,\n",
    "    plot_loss_dynamics,\n",
    "    plot_snr_analysis,\n",
    "    analyze_compression_stats\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c419d4",
   "metadata": {},
   "source": [
    "## 1. The Data: A \"Gappy\" Sine Wave üåä\n",
    "\n",
    "To see why MDL is cool, we need a tricky dataset.\n",
    "We will generate a sine wave, but we will **delete the middle part**.\n",
    "\n",
    "* **Standard NN:** Will confidently hallucinate a line through the gap.\n",
    "* **Bayesian NN:** Will say \"I don't know what's in the gap!\" (High Uncertainty).\n",
    "\n",
    "This \"knowing what you don't know\" is crucial for AI safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noisy sine wave with a GAP\n",
    "def generate_gappy_data(n=100):\n",
    "    # Left side (-3 to -1)\n",
    "    X1 = np.random.uniform(-3, -1, n//2)\n",
    "    # Right side (1 to 3)\n",
    "    X2 = np.random.uniform(1, 3, n//2)\n",
    "    # Combine\n",
    "    X = np.concatenate([X1, X2])\n",
    "    # Add noise\n",
    "    y = np.sin(X) + np.random.normal(0, 0.1, n)\n",
    "    return X.reshape(-1, 1), y.reshape(-1, 1)\n",
    "\n",
    "X_train, y_train = generate_gappy_data(100)\n",
    "\n",
    "# Visualization range (including the gap)\n",
    "X_test = np.linspace(-4, 4, 200).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X_train, y_train, c='red', label='Training Data')\n",
    "plt.axvspan(-1, 1, color='gray', alpha=0.2, label='The GAP (No Data)')\n",
    "plt.plot(X_test, np.sin(X_test), 'k--', alpha=0.5, label='True Sine Wave')\n",
    "plt.title(\"The Challenge: Predict what happens in the Gap\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14a7f2",
   "metadata": {},
   "source": [
    "## 2. Training the Bayesian Brain üß†\n",
    "\n",
    "We will now train our `MDLNetwork`. Unlike a normal network, this one has **two** loss terms:\n",
    "\n",
    "1.  **Error Cost (NLL):** \"Did I get the prediction right?\"\n",
    "2.  **Complexity Cost (KL):** \"Did I use simple weights?\"\n",
    "\n",
    "The `kl_weight` parameter controls the balance. \n",
    "* Too low? It overfits (memorizes).\n",
    "* Too high? It underfits (ignores data to be simple).\n",
    "* Just right? **Magic happens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e458df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Network\n",
    "net = MDLNetwork(input_size=1, hidden_size=20, output_size=1)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 2000\n",
    "lr = 0.01\n",
    "kl_weight = 0.1  # The \"Simplicity Pressure\"\n",
    "\n",
    "# Storage for plotting\n",
    "history = {'total': [], 'nll': [], 'kl': []}\n",
    "\n",
    "print(\"Training Bayesian Network...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward Pass (This samples random weights!)\n",
    "    # Every time we call this, the network is slightly different.\n",
    "    preds = net.forward(X_train)\n",
    "    \n",
    "    # 2. Data Loss (MSE as proxy for Negative Log Likelihood)\n",
    "    nll = np.mean((preds - y_train)**2)\n",
    "    d_nll = 2 * (preds - y_train) / len(X_train)\n",
    "    \n",
    "    # 3. Complexity Loss (KL Divergence)\n",
    "    kl = net.total_kl() / len(X_train)\n",
    "    \n",
    "    # 4. Total Loss\n",
    "    loss = nll + kl_weight * kl\n",
    "    \n",
    "    # Store history\n",
    "    history['total'].append(loss)\n",
    "    history['nll'].append(nll)\n",
    "    history['kl'].append(kl)\n",
    "    \n",
    "    # 5. Backward Pass\n",
    "    net.backward(d_nll)\n",
    "    \n",
    "    # 6. Update Weights\n",
    "    net.update_weights(lr, kl_weight)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch:4d} | Total: {loss:.4f} | Error: {nll:.4f} | Complexity: {kl:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef8a10e",
   "metadata": {},
   "source": [
    "## 3. The Battle: Complexity vs. Error ‚öîÔ∏è\n",
    "\n",
    "Let's look at how the network learned. \n",
    "Usually, the **Error** drops quickly, but the **Complexity** (KL) might rise at first as the network learns structure, before stabilizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a531767",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_dynamics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a13028",
   "metadata": {},
   "source": [
    "## 4. The Uncertainty Envelope üìâ\n",
    "\n",
    "This is the most famous visualization in Bayesian Deep Learning.\n",
    "\n",
    "We will run the network **100 times** on the test data.\n",
    "Because the weights are \"fuzzy\" (random distributions), each run produces a slightly different line.\n",
    "\n",
    "* **Where we have data:** All the lines agree (Low Variance).\n",
    "* **In the GAP:** The lines disagree (High Variance).\n",
    "\n",
    "This tube represents what the model **doesn't know**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195fa287",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_uncertainty_envelope(net, X_train, y_train, X_test, n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554351f4",
   "metadata": {},
   "source": [
    "## 5. Peeking Inside the Brain (Weight Distributions) üî¨\n",
    "\n",
    "The paper's title is about \"Minimizing Description Length.\"\n",
    "How do we see that?\n",
    "\n",
    "We look at the **Sigma** (Uncertainty) of the weights.\n",
    "* **Sharp Weights (Low Sigma):** The model says \"This weight MUST be exactly 0.5.\"\n",
    "* **Fuzzy Weights (High Sigma):** The model says \"This weight can be anything around 0.1. It doesn't matter.\"\n",
    "\n",
    "**Key Insight:** The fuzzy weights carry **0 bits of information**. They are effectively compressed away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weight_distributions(net, 'layer1')\n",
    "plot_snr_analysis(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b7d4a",
   "metadata": {},
   "source": [
    "## 6. How much did we compress? üóúÔ∏è\n",
    "\n",
    "Finally, let's calculate the statistics.\n",
    "If a weight has a Signal-to-Noise Ratio (SNR) < 0.5, it is basically noise.\n",
    "We can set it to zero (prune it) without hurting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_compression_stats(net, threshold_snr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1374daff",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways üéØ\n",
    "\n",
    "### 1. Generalization = Compression\n",
    "By punishing the model for having precise weights (the KL term), we forced it to find a **simple solution**. This simple solution generalizes better to the \"Gap\" in our data.\n",
    "\n",
    "### 2. Uncertainty is Useful\n",
    "Unlike a standard Neural Network which would lie confidently in the gap, this model admits \"I don't know.\" This is critical for self-driving cars and medical AI.\n",
    "\n",
    "### 3. Noise is a Feature, not a Bug\n",
    "We injected noise *during* training. This noise prevented the model from memorizing the data. It's like training a runner with a heavy backpack‚Äîwhen you take it off (or average the results), they are stronger.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You have implemented one of the deepest concepts in AI theory.\n",
    "\n",
    "**Next Up:** We move from theory back to architecture with **Pointer Networks**!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
