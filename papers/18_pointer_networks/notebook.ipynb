{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e6db53",
   "metadata": {},
   "source": [
    "# Day 18 Walkthrough: Pointer Networks\n",
    "\n",
    "Day 18 of **30 Papers in 30 Days**.\n",
    "\n",
    "Today: **Pointer Networks** - a specialized Seq2Seq architecture that solves the \"variable output space\" problem. Unlike traditional models that pick from a fixed vocabulary, Pointer Networks learn to \"point\" directly at input elements.\n",
    "\n",
    "### What You'll Learn\n",
    "1. **The Architecture**: Implementing Equation 3 (Additive Attention as a pointer).\n",
    "2. **The Data**: Creating a sorting task with positional targets (Indirection).\n",
    "3. **The Training**: Optimizing selection heads to approximate discrete algorithms.\n",
    "4. **The Visualization**: Interpreting \"Laser Pointer\" attention heatmaps.\n",
    "5. **Inductive Bias**: Why 'selection' is better than 'prediction' for geometric tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3dbf8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bc9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Local implementation imports\n",
    "from implementation import PointerNetwork\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503018f2",
   "metadata": {},
   "source": [
    "## 1. The Theory: Pointing vs. Prediction\n",
    "\n",
    "A standard classifier picks one of K fixed classes. A Pointer Network picks one of N input items (Section 2.3). At step $i$ of decoding, the score for input $j$ is:\n",
    "\n",
    "$$u_{j}^{i} = v^{T} \\tanh(W_{1}e_{j} + W_{2}d_{i})$$\n",
    "\n",
    "The resulting probability distribution $softmax(u^i)$ tells us which input item to select."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4c725",
   "metadata": {},
   "source": [
    "## 2. Dataset: Sorting Indirection\n",
    "\n",
    "Note: The paper's three experiments are Convex Hull, Delaunay Triangulation, and TSP (Sections 3.1-3.3). Sorting is used as a motivating example in the Introduction but was not one of the paper's experimental tasks. We use it here as a simpler demonstration of the pointer mechanism.\n",
    "\n",
    "- Input: `[0.7, 0.2, 0.9]`\n",
    "- Sorted Indices: `[1, 0, 2]` (pointing to 0.2, then 0.7, then 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortingDataset(Dataset):\n",
    "    def __init__(self, num_samples=5000, seq_len=5):\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            nums = np.random.rand(seq_len)\n",
    "            indices = np.argsort(nums)\n",
    "            self.samples.append((nums, indices))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        nums, indices = self.samples[idx]\n",
    "        return torch.tensor(nums).float().unsqueeze(-1), torch.tensor(indices).long()\n",
    "\n",
    "seq_len = 5\n",
    "train_data = SortingDataset(10000, seq_len=seq_len)\n",
    "val_data = SortingDataset(1000, seq_len=seq_len)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "x, y = train_data[0]\n",
    "print(f\"Input values:  {x.squeeze().tolist()}\")\n",
    "print(f\"Target pointers: {y.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a8ab7",
   "metadata": {},
   "source": [
    "## 3. Training the Pointer Network\n",
    "\n",
    "We optimize standard Cross Entropy loss, where the output vocabulary is the input sequence indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointerNetwork(input_size=1, hidden_size=64).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "print(\"Starting training (50 epochs for convergence)...\")\n",
    "for epoch in range(1, 51):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output.view(-1, seq_len), y.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:2d} | Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993210a8",
   "metadata": {},
   "source": [
    "## 4. Visualizing Laser Pointers\n",
    "\n",
    "The attention matrix should look like a permutation matrix (one sharp peak per output step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pointers(idx=0):\n",
    "    model.eval()\n",
    "    x, y = val_data[idx]\n",
    "    x_t = x.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        log_pointers = model(x_t)\n",
    "        pointers = torch.exp(log_pointers).squeeze(0).cpu().numpy()\n",
    "        preds = log_pointers.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(pointers, cmap='Blues', vmin=0, vmax=1)\n",
    "    plt.title(\"Pointer Network Attention Heatmap\")\n",
    "    plt.show()\n",
    "    print(f\"Sorted Indices: {preds}\")\n",
    "    print(f\"Correct Indices: {y.tolist()}\")\n",
    "\n",
    "visualize_pointers(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43ba0b",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "1. **Attention as Output**: Pointer Networks use the attention distribution itself as the output layer, replacing the fixed softmax (Section 2.3).\n",
    "2. **Dynamic Vocabulary**: The model natively supports variable input sizes because the pointers are relative to the input length.\n",
    "3. **Algorithmic Generalization**: The paper shows a single model trained on n=5-50 can generalize to n=500 for Convex Hull (Section 4.2, Table 1).\n",
    "4. **Critical for Combinatorial Tasks**: Standard Seq2Seq methods fail where Pointer Networks succeed by providing the correct inductive bias (pointing vs predicting values).\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Implement Equation 3 in `exercises/exercise_01_pointer_attention.py`.\n",
    "- Learn to mask selections in `exercises/exercise_02_masking.py` so you don't pick the same node twice.\n",
    "- Master geometric formatting and decoding in Exercises 3-5.\n",
    "- Move on to **Day 19: Relational Reasoning** to reason about structured objects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
