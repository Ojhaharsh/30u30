{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 30: Deep Reinforcement Learning from Human Feedback (RLHF)\n",
        "\n",
        "> Christiano et al. (2017) \u2014 [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/abs/1706.03741)\n",
        "\n",
        "This is the algorithm that enabled ChatGPT.\n",
        "\n",
        "## What You'll Learn\n",
        "1. **The Bradley-Terry Model**: How to turn pairwise preferences into a probability.\n",
        "2. **Reward Modeling**: Training a neural network to predict human preferences.\n",
        "3. **Synthetic Oracle**: Simulating human feedback using ground-truth rewards.\n",
        "4. **The RLHF Loop**: Collect -> Label -> Train Reward Model -> Train Policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "We need PyTorch, Gymnasium, and Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The Bradley-Terry Model\n",
        "\n",
        "The core mathematical assumption in RLHF is that the probability of preferring segment $\\sigma^1$ over $\\sigma^2$ depends on their rewards:\n",
        "\n",
        "$$ P[\\sigma^1 \\succ \\sigma^2] = \\frac{\\exp(\\sum r(\\sigma^1))}{\\exp(\\sum r(\\sigma^1)) + \\exp(\\sum r(\\sigma^2))} $$\n",
        "\n",
        "Let's implement this probability function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def preference_probability(r1_sum, r2_sum):\n",
        "    \"\"\"Computes P(1 > 2) given reward sums.\"\"\"\n",
        "    r1_exp = torch.exp(r1_sum)\n",
        "    r2_exp = torch.exp(r2_sum)\n",
        "    return r1_exp / (r1_exp + r2_exp)\n",
        "\n",
        "# Example: Which is better?\n",
        "r1 = torch.tensor(1.0) # Sum reward 1.0\n",
        "r2 = torch.tensor(0.0) # Sum reward 0.0\n",
        "prob = preference_probability(r1, r2)\n",
        "print(f\"P(1 > 2): {prob.item():.4f}\")\n",
        "# Expected: exp(1)/(exp(1)+1) ~= 0.7311"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. The Reward Model\n",
        "\n",
        "We need a neural network that takes an observation (state) and outputs a single scalar reward. This network will be trained to agree with the 'human' preferences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, obs):\n",
        "        return self.net(obs)\n",
        "\n",
        "# Test dimensions\n",
        "rm = RewardModel(obs_dim=4)\n",
        "dummy_obs = torch.randn(1, 4)\n",
        "print(f\"Reward Output: {rm(dummy_obs).item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. The Preference Loss\n",
        "\n",
        "We train the reward model by minimizing the cross-entropy loss between the predicted preference probabilities and the actual human labels.\n",
        "\n",
        "If the human says \"Segment 1 is better\" (label=0), we want to maximize $P(\\sigma^1 \\succ \\sigma^2)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def compute_loss(reward_model, s1, s2, label):\n",
        "    # s1, s2 are batches of observations: (batch, len, obs_dim)\n",
        "    # label is (batch,) where 0 means s1 is better, 1 means s2 is better\n",
        "    \n",
        "    # 1. Get rewards for all steps\n",
        "    r1 = reward_model(s1).squeeze(-1) # (batch, len)\n",
        "    r2 = reward_model(s2).squeeze(-1)\n",
        "    \n",
        "    # 2. Sum rewards\n",
        "    r1_sum = r1.sum(dim=1)\n",
        "    r2_sum = r2.sum(dim=1)\n",
        "    \n",
        "    # 3. Stack as logits\n",
        "    logits = torch.stack([r1_sum, r2_sum], dim=1)\n",
        "    \n",
        "    # 4. Cross Entropy\n",
        "    loss = F.cross_entropy(logits, label)\n",
        "    return loss\n",
        "\n",
        "# Test Loss\n",
        "s1 = torch.randn(2, 5, 4) # Batch 2, Len 5, Dim 4\n",
        "s2 = torch.randn(2, 5, 4)\n",
        "labels = torch.tensor([0, 1]) # First pair 1>2, Second pair 2>1\n",
        "loss = compute_loss(rm, s1, s2, labels)\n",
        "print(f\"Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. The Synthetic Oracle\n",
        "\n",
        "Since we can't ask you to click a button thousands of times, we simulate a 'perfect' human who prefers the segment with higher ground-truth reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SyntheticOracle:\n",
        "    def query(self, r1_sum, r2_sum):\n",
        "        return 0 if r1_sum > r2_sum else 1\n",
        "\n",
        "oracle = SyntheticOracle()\n",
        "print(f\"Oracle says: {oracle.query(10, 5)} (0 means first is better)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Putting It All Together\n",
        "\n",
        "The full loop involves collecting data, getting labels, training the reward model, and then training the policy (PPO) against that reward model.\n",
        "\n",
        "For the full implementation, run `python implementation.py` or `python train_minimal.py`.\n",
        "Here is the high-level logic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"1. Initialize Policy and Reward Model\")\n",
        "print(\"2. Collect trajectories using Policy\")\n",
        "print(\"3. Ask Oracle to label pairs of trajectories\")\n",
        "print(\"4. Update Reward Model to minimize Preference Loss\")\n",
        "print(\"5. Update Policy (PPO) to maximize Reward Model output\")\n",
        "print(\"6. Repeat!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Preference Learning**: We can learn a reward function just by asking \"which is better?\"\n",
        "2. **Bradley-Terry**: The math model that converts scores into preference probabilities.\n",
        "3. **Scalability**: This method scales to tasks where we can't write a reward function (e.g., summarization, driving, behaving helpfully).\n",
        "4. **Alignment**: This is the technique that aligns raw LLMs into helpful assistants like ChatGPT."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}