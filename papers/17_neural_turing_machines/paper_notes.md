# Paper Notes: Neural Turing Machines (NTM)

## ELI5 (Explain Like I'm 5)

### The Scratchpad Metaphor
Imagine you're trying to do a really complex long-division problem in your head. It’s hard because you have to remember the numbers while you're also doing the subtraction and the next division. Your "internal memory" (the hidden state in an RNN) gets overwhelmed.

The Neural Turing Machine (NTM) is like giving your brain a **scratchpad**. Instead of holding everything in your head, you can write intermediate numbers down, look at them when you need them, and erase them when they're no longer useful. 

**Wait, isn't that just an LSTM?**
No. An LSTM's "memory" is a single vector of a fixed size. If you want to remember more, you have to make the whole network bigger. An NTM's memory is a separate "bank" (like a spreadsheet). You can make the spreadsheet bigger without changing how the network itself works.

> **Note:** This analogy is ours, not Graves'. Graves uses the more formal "Turing's Metaphor" (human with pen and paper) described below.

---

## What the Paper Actually Says (Traceability)

The paper **"Neural Turing Machines" (Graves et al., 2014)** introduces a new architecture that couples a neural network "controller" to an external memory bank.

### 1. Turing's Metaphor (Section 1)
Graves starts by referencing Alan Turing's 1948 description of a human "calculating machine" who has a pen, paper, and an "effective procedure." Turing's point was that human "intelligence" often involves a brain interacting with external symbols. The NTM is the neural equivalent of this interaction.

### 2. The Architecture (Section 2)
The system is divided into two parts:
- **The Controller:** Usually an LSTM. It "thinks" and decides what to do.
- **The Heads:** The interface between the Controller and the Memory.

### 3. Differentiable Addressing (Section 3.3.1)
This is the most critical technical contribution. How does a neural network "point" to a specific memory location without breaking the ability to use gradient descent? 
- In a normal computer, an address is a discrete number (e.g., "Address 42"). This is **not** differentiable.
- In an NTM, an address is a **weighting vector** (e.g., "0.9 on Address 42 and 0.1 on Address 43"). This "blurry" focus allows the model to be trained via backpropagation.

**The Four-Step Addressing Pipeline:**
1. **Content Addressing (Eq 5):** The controller emits a key vector and compares it against all memory rows via cosine similarity. [Our Addition: Analogy — like saying "find me something that looks like X."]
2. **Interpolation (Eq 7):** A gate $g_t$ blends the new content weighting with the previous weighting. [Our Addition: Analogy — deciding whether to use the new search result or stay at the previous location.]
3. **Convolutional Shift (Eq 8):** A shift distribution $s_t$ performs circular convolution over the weighting, enabling relative movement. [Our Addition: Analogy — like saying "move one slot to the right."]
4. **Sharpening (Eq 9):** To prevent the focus from becoming *too* blurry over time, the model "squeezes" the weights to pick a clear winner.

### 4. Reading and Writing (Section 3.1 & 3.2)
- **Reading:** Does a weighted sum of memory locations.
- **Writing:** This is a two-step process. First, it **erases** (Eq 3) by multiplying memory by a "forget" vector, then it **adds** (Eq 4) new information. [Our Addition: Analogy] This resembles how conventional RAM performs write operations, but here every step is differentiable.

---

## The Experiments: What the NTM Learned

The paper tests the model on five tasks. These experiments are significant because they show the NTM learning *algorithms*, not just patterns.

### 1. The Copy Task (Section 4.1)
- **Task:** See a sequence, then repeat it back.
- **Result:** The NTM learns to write the sequence into memory sequentially and then read it back. The paper shows it **generalizes** to sequences longer than those seen in training. An LSTM usually fails here because its memory is "fixed."

### 2. Repeat Copy (Section 4.2)
- **Task:** See a sequence and a number $N$, then repeat the sequence $N$ times.
- **Result:** [Our Addition: Analogy] The NTM's behavior resembles a loop — it stores the sequence once and then repeatedly reads it back, keeping a counter in its controller.

### 3. Associative Recall (Section 4.3)
- **Task:** See a list of (key, value) pairs. Then see a key and output the corresponding value.
- **Result:** The NTM learns to use its Content Addressing to "query" the memory.

### 4. Dynamic N-Grams (Section 4.4)
- **Task:** Predict the next bit in a sequence generated by a specific probability table.
- **Result:** The NTM learns to update its own memory to keep track of the count of occurrences (Section 4.4). [Our Addition: Analogy — in effect, it "programs" its own statistics table.]

### 5. Priority Sort (Section 4.5)
- **Task:** Sort a list of vectors by a scalar "priority" value.
- **Result:** The NTM learns to use its memory to store vectors and then use its addressing mechanism to find the one with the highest priority next.

---

## What the Paper Doesn't Cover (Section 5)

- **Alternative Topologies:** The paper focus solely on a 2D memory matrix. It doesn't explore stacks, queues, or other data structures.
- **Large-scale benchmarks:** The experiments are restricted to toy algorithmic tasks. 
- **Training Stability:** Although it mentions gradient clipping, it does not provide a comprehensive analysis of why NTMs can be difficult to train compared to standard LSTMs.

---

## Our Additions (Not from the Paper)

### Practical Implementation Details
While the paper presents Equations 1-9 as a complete system, training them in PyTorch requires subtle numerical stability tricks that aren't explicitly detailed:
1. **Softplus for Positivity:** Parameters like $\beta$ (key strength) and $\gamma$ (sharpening) must be positive. We use `1 + F.softplus(x)` instead of a raw ReLU to prevent dead units.
2. **Epsilon Values:** When normalizing weights (Eq 9), adding a small constant ($1e-8$) is essential to prevent `ZeroDivisionError` when a head is initially uninitialized.
3. **The "Copy" Delimiter:** In Section 4.1, the paper mentions a "delimiter flag." In our implementation, we explicitly use an $(N+1)$-th bit to represent this flag, which is a common practical convention not strictly mandated in the text.

### Evolutionary Context (Where NTM stands now)
The NTM was a direct precursor to the **Differentiable Neural Computer (DNC)** (Nature, 2016). While the NTM used a simple circular shift for location addressing, the DNC added "Dynamic Memory Allocation" (like a real OS) and "Temporal Linkage" (keeping track of the order of writes automatically).
