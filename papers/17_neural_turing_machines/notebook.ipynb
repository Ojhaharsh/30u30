{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Day 17: Neural Turing Machines (NTM)\n",
                "\n",
                "**Paper:** \"Neural Turing Machines\" â€” Graves, Wayne, Danihelka (2014)\n",
                "\n",
                "We explore the internal mechanics of the NTM, specifically how it uses differentiable addressing to read and write to an external memory bank.\n",
                "\n",
                "---\n",
                "\n",
                "## What You'll Learn\n",
                "\n",
                "1. How content-based addressing uses cosine similarity to find relevant memory locations (Eq 5)\n",
                "2. How circular convolution implements location-based shifting (Eq 8)\n",
                "3. How sharpening prevents blurry focus after the shift operation (Eq 9)\n",
                "4. How the write head performs erase-then-add updates to memory (Eq 3, 4)\n",
                "5. Why decoupling memory from computation allows NTMs to learn algorithms like copying and sorting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "from implementation import NTM, NTMMemory\n",
                "\n",
                "# Setup a small NTM for demonstration\n",
                "N, M = 16, 8\n",
                "controller_size = 32\n",
                "ntm = NTM(input_size=5, output_size=5, controller_size=controller_size, N=N, M=M)\n",
                "print(f\"NTM initialized with {N} memory slots and {M} dimensions per slot.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Differentiable Addressing Visualization\n",
                "\n",
                "Standard Turing Machines use discrete pointers. NTMs use \"blurry\" weightings that sum to 1. This allows the model to be trained with gradient descent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example of a weighting vector w\n",
                "torch.manual_seed(42)\n",
                "w = F.softmax(torch.randn(1, N), dim=1)\n",
                "\n",
                "plt.figure(figsize=(10, 2))\n",
                "plt.bar(range(N), w[0].detach().numpy())\n",
                "plt.title(\"Example Weighting (w)\")\n",
                "plt.xlabel(\"Memory Slot\")\n",
                "plt.ylabel(\"Weight Intensity\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Sharpening\n",
                "\n",
                "Over time, the weightings tend to drift and become too uniform. Sharpening forces the head to focus on fewer locations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sharpen(w, gamma):\n",
                "    w_pow = w ** gamma\n",
                "    return w_pow / w_pow.sum()\n",
                "\n",
                "gammas = [1, 2, 5, 20]\n",
                "fig, axes = plt.subplots(1, len(gammas), figsize=(15, 3))\n",
                "\n",
                "for i, g in enumerate(gammas):\n",
                "    w_sh = sharpen(w[0], g)\n",
                "    axes[i].bar(range(N), w_sh.detach().numpy())\n",
                "    axes[i].set_title(f\"Gamma = {g}\")\n",
                "\n",
                "plt.suptitle(\"Effect of Sharpening on Focus\", fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Circular Shift\n",
                "\n",
                "The Convolutional Shift allows the NTM to implement relative movements (e.g., \"move to the next slot\")."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def shift_demo(w, s_val):\n",
                "    # Simple shift simulation\n",
                "    return torch.roll(w, shifts=s_val, dims=0)\n",
                "\n",
                "w_focused = torch.zeros(N)\n",
                "w_focused[4] = 1.0\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(211)\n",
                "plt.bar(range(N), w_focused)\n",
                "plt.title(\"Original Focus at Slot 4\")\n",
                "\n",
                "plt.subplot(212)\n",
                "plt.bar(range(N), shift_demo(w_focused, 2))\n",
                "plt.title(\"Shifted Focus by +2 Slots\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Key Takeaways\n",
                "\n",
                "### What We Learned\n",
                "\n",
                "1. **Content addressing finds relevant memory rows** - Cosine similarity between a search key and each memory row produces a soft weighting over locations\n",
                "\n",
                "2. **Circular convolution enables sequential access** - The shift mechanism lets the controller increment or decrement its focus position, which is necessary for tasks like copying\n",
                "\n",
                "3. **Sharpening counteracts convolution blur** - Raising weights to a power gamma and renormalizing keeps the focus peaked on specific addresses\n",
                "\n",
                "4. **Erase-then-add is order-dependent** - The write head first removes old content (Eq 3) then writes new content (Eq 4); reversing these operations produces different results\n",
                "\n",
                "### What's Next?\n",
                "\n",
                "- **Differentiable Neural Computers** (Graves et al., 2016) - Improved memory addressing with temporal links and allocation mechanisms\n",
                "- **Memory Networks** (Weston et al., 2015) - An alternative approach to external memory for question answering\n",
                "- **Attention mechanisms** generalized many of these ideas into what became the Transformer architecture\n",
                "\n",
                "---\n",
                "\n",
                "*The NTM demonstrated that neural networks can learn simple algorithms by separating storage from computation. The four-stage addressing pipeline (content lookup, interpolation, shift, sharpening) laid groundwork for the attention mechanisms used in modern architectures.*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
