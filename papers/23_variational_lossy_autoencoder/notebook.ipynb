{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7eea8be9",
            "metadata": {},
            "source": [
                "# Day 23: Variational Lossy Autoencoder (VLAE)\n",
                "\n",
                "> Xi Chen et al. (2017) - [Variational Lossy Autoencoder](https://arxiv.org/abs/1611.02731)\n",
                "\n",
                "**Core problem:** Posterior collapse. VAEs with powerful decoders (PixelCNN) ignore the latent code because local pixel dependencies are \"easier\" than global semantic modeling.\n",
                "\n",
                "**VLAE solution:** \n",
                "1. **Cripple the decoder:** Use a limited receptive field to force reliance on global $z$.\n",
                "2. **Boost the prior:** Use autoregressive flows (IAF) to allow for complex, flexible latent distributions.\n",
                "\n",
                "In this notebook, we implement and verify the VLAE architecture on binarized MNIST."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a9550bf2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torchvision import datasets, transforms\n",
                "from torch.utils.data import DataLoader\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "from implementation import VLAE, loss_function\n",
                "from visualization import plot_reconstructions, plot_latent_sampling, plot_bits_heatmap\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "91f160be",
            "metadata": {},
            "source": [
                "## 1. Technical Verification: Autoregressive Masking\n",
                "\n",
                "VLAE relies on strict autoregressive property. We use **MADE** (Masked Autoencoder for Distribution Estimation) for the Flows and **MaskedConv2d** for the PixelCNN decoder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "05872d3b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from implementation import MADE\n",
                "dim = 10\n",
                "model = MADE(dim, 32, dim)\n",
                "x = torch.randn(1, dim, requires_grad=True)\n",
                "out = model(x)\n",
                "\n",
                "# Verification: Output i should NOT depend on input >= i\n",
                "i = 5\n",
                "out[0, i].backward()\n",
                "print(f\"Gradients for dimension {i}:\")\n",
                "print(x.grad.data.numpy())\n",
                "assert torch.all(x.grad.data[0, i:] == 0)\n",
                "print(\"Autoregressive condition MET.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f5c4942d",
            "metadata": {},
            "source": [
                "## 2. Load and Binarize MNIST\n",
                "\n",
                "We use binarized MNIST because PixelCNN is a discrete model ($p(x_i | x_{<i})$)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c2d31137",
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size = 64\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    lambda x: (x > 0.5).float()\n",
                "])\n",
                "\n",
                "train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), \n",
                "                          batch_size=batch_size, shuffle=True)\n",
                "test_loader = DataLoader(datasets.MNIST('./data', train=False, transform=transform), \n",
                "                         batch_size=batch_size, shuffle=False)\n",
                "\n",
                "model = VLAE(input_dim=1, latent_dim=32, n_layers=3, use_flow=True).to(device)\n",
                "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f734aa64",
            "metadata": {},
            "source": [
                "## 3. Training Loop\n",
                "\n",
                "We track both the Reconstruction Loss (BCE) and the KL Divergence (the information usage)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "08f7e69e",
            "metadata": {},
            "outputs": [],
            "source": [
                "epochs = 3 # 10+ is ideal for crisp results\n",
                "model.train()\n",
                "for epoch in range(1, epochs + 1):\n",
                "    total_loss, total_kl, total_recon = 0, 0, 0\n",
                "    for data, _ in train_loader:\n",
                "        data = data.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        logits, mu, logvar, z, log_det = model(data)\n",
                "        loss = loss_function(logits, data, mu, logvar, z, log_det)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            recon = torch.nn.functional.binary_cross_entropy_with_logits(logits, data, reduction='sum')\n",
                "            total_recon += recon.item()\n",
                "            total_kl += (loss.item() - recon.item())\n",
                "            total_loss += loss.item()\n",
                "            \n",
                "    print(f\"Epoch {epoch} | Loss: {total_loss/len(train_loader.dataset):.2f} | R: {total_recon/len(train_loader.dataset):.2f} | KL: {total_kl/len(train_loader.dataset):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "530e07a0",
            "metadata": {},
            "source": [
                "## 4. Visual Analysis\n",
                "\n",
                "### Posterior Information Usage (Bits Heatmap)\n",
                "In a collapsed VAE, this plot would be flat (all zero bits). In VLAE, we expect clear spikes in dimensions that captured global features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "631544dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "test_data, _ = next(iter(test_loader))\n",
                "with torch.no_grad():\n",
                "    _, mu, logvar, _, _ = model(test_data[:100].to(device))\n",
                "    plot_bits_heatmap(mu, logvar)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "328c9d0f",
            "metadata": {},
            "source": [
                "### Reconstructions\n",
                "The decoder should accurately reconstruct local textures while relying on $z$ for global orientation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1537ef69",
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_reconstructions(model, test_data[:8], device)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c71122a6",
            "metadata": {},
            "source": [
                "## 5. Latent Space Traversals\n",
                "\n",
                "Does $z$ capture high-level semantics? We pick an image, encode it, and sweep one dimension of $z$ to see what changes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c7567166",
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_traversal(model, image, dim_idx, device, n_steps=8):\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        mu, logvar = model.encoder(image.to(device))\n",
                "        z = model.reparameterize(mu, logvar)\n",
                "        \n",
                "        traversals = []\n",
                "        for val in np.linspace(-3, 3, n_steps):\n",
                "            z_step = z.clone()\n",
                "            z_step[0, dim_idx] += val\n",
                "            h_latent = z_step.view(1, -1, 1, 1).expand(-1, -1, 28, 28)\n",
                "            \n",
                "            # Fast recon (teacher forcing with zeroes for speed in visualization)\n",
                "            # Correct way is autoregressive, but here we just check logits\n",
                "            h = model.initial_conv(torch.zeros_like(image).to(device))\n",
                "            for layer in model.decoder_layers:\n",
                "                h = layer(h, h_latent)\n",
                "            logits = model.final_conv(h)\n",
                "            traversals.append(torch.sigmoid(logits).cpu().squeeze())\n",
                "            \n",
                "    fig, axes = plt.subplots(1, n_steps, figsize=(15, 2))\n",
                "    for i, img in enumerate(traversals):\n",
                "        axes[i].imshow(img, cmap='gray')\n",
                "        axes[i].axis('off')\n",
                "    plt.show()\n",
                "\n",
                "print(\"Traversal for dimension 0:\")\n",
                "plot_traversal(model, test_data[0:1], dim_idx=0, device=device)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3bfd29d",
            "metadata": {},
            "source": [
                "## 6. Full Sampling (Slow)\n",
                "\n",
                "We generate new digits by sampling $z \\sim p(z)$ and decoding pixel-by-pixel."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "374e608c",
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_latent_sampling(model, device, latent_dim=32, n_samples=9)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d635df0c",
            "metadata": {},
            "source": [
                "### Summary\n",
                "\n",
                "- VLAE forces latent usage by restricting the decoder's visual field.\n",
                "- Normalizing flows (IAF) allow for a high-capacity prior.\n",
                "- The result is a disentangled model where $z$ is semantic and the decoder is structural/textural."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
