{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d695bcb",
   "metadata": {},
   "source": [
    "# Day 1: The Unreasonable Effectiveness of Recurrent Neural Networks\n",
    "\n",
    "**Welcome to the 30u30 challenge!** ðŸš€\n",
    "\n",
    "Today we'll build a **character-level RNN** from scratch and see how simple models can generate surprisingly good text.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. âœ… How RNNs process sequences\n",
    "2. âœ… Forward propagation through time\n",
    "3. âœ… Backpropagation through time (BPTT)\n",
    "4. âœ… Why RNNs can generate coherent text\n",
    "5. âœ… Temperature sampling\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# For pretty plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Setup complete! âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bfac1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding the Problem\n",
    "\n",
    "### The Task: Predict the Next Character\n",
    "\n",
    "Given a sequence of characters, predict what comes next.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input:  \"hello wor\"\n",
    "Output: \"l\" (most likely)\n",
    "```\n",
    "\n",
    "### Why is this hard?\n",
    "\n",
    "- Context matters: \"h\" after \"wor\" vs \"h\" after \"t\"\n",
    "- Long-range dependencies: opening quote â†’ closing quote\n",
    "- Multiple valid continuations\n",
    "\n",
    "### Why RNNs?\n",
    "\n",
    "RNNs have **memory** (hidden state) that remembers previous characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72a71e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Prepare Data\n",
    "\n",
    "Let's start with a tiny dataset so we can see what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny training data\n",
    "data = \"hello hello hello world world world\"\n",
    "\n",
    "# Get unique characters\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Data: {data}\")\n",
    "print(f\"Unique characters: {chars}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create mappings\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"\\nMappings:\")\n",
    "for ch in chars[:5]:\n",
    "    print(f\"  '{ch}' â†’ {char_to_idx[ch]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43cdbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Build the RNN\n",
    "\n",
    "### The Math\n",
    "\n",
    "At each time step $t$:\n",
    "\n",
    "1. **Hidden state update:**\n",
    "   $$h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$\n",
    "\n",
    "2. **Output:**\n",
    "   $$y_t = W_{hy} h_t + b_y$$\n",
    "\n",
    "3. **Probabilities:**\n",
    "   $$p_t = \\text{softmax}(y_t)$$\n",
    "\n",
    "### Visualizing the Flow\n",
    "\n",
    "```\n",
    "Input:  \"h\"  \"e\"  \"l\"  \"l\"  \"o\"\n",
    "         â†“    â†“    â†“    â†“    â†“\n",
    "       [RNN][RNN][RNN][RNN][RNN]\n",
    "         â†“    â†“    â†“    â†“    â†“\n",
    "Output: \"e\"  \"l\"  \"l\"  \"o\"  \" \"\n",
    "```\n",
    "\n",
    "Each RNN cell:\n",
    "- Takes current input + previous hidden state\n",
    "- Produces new hidden state + output\n",
    "- Hidden state = memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e25b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 25  # Size of hidden state vector\n",
    "seq_length = 10   # Number of steps to unroll\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize weights (small random values)\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # Input â†’ Hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden â†’ Hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # Hidden â†’ Output\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((vocab_size, 1))   # Output bias\n",
    "\n",
    "print(f\"Weight shapes:\")\n",
    "print(f\"  Wxh: {Wxh.shape} (hidden_size Ã— vocab_size)\")\n",
    "print(f\"  Whh: {Whh.shape} (hidden_size Ã— hidden_size)\")\n",
    "print(f\"  Why: {Why.shape} (vocab_size Ã— hidden_size)\")\n",
    "print(f\"  bh:  {bh.shape}\")\n",
    "print(f\"  by:  {by.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d58fbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Forward Pass\n",
    "\n",
    "Let's process one sequence and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, targets, h_prev):\n",
    "    \"\"\"\n",
    "    Forward pass through RNN.\n",
    "    \n",
    "    Args:\n",
    "        inputs: List of character indices (length = seq_length)\n",
    "        targets: List of target character indices\n",
    "        h_prev: Previous hidden state (hidden_size Ã— 1)\n",
    "        \n",
    "    Returns:\n",
    "        loss: Cross-entropy loss\n",
    "        h_last: Final hidden state\n",
    "        cache: Values for backward pass\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(h_prev)\n",
    "    loss = 0\n",
    "    \n",
    "    # Forward through time\n",
    "    for t in range(len(inputs)):\n",
    "        # 1. One-hot encode input\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        \n",
    "        # 2. Hidden state\n",
    "        hs[t] = np.tanh(Wxh @ xs[t] + Whh @ hs[t-1] + bh)\n",
    "        \n",
    "        # 3. Output\n",
    "        ys[t] = Why @ hs[t] + by\n",
    "        \n",
    "        # 4. Softmax (numerically stable)\n",
    "        ps[t] = np.exp(ys[t] - np.max(ys[t])) / np.sum(np.exp(ys[t] - np.max(ys[t])))\n",
    "        \n",
    "        # 5. Loss (cross-entropy)\n",
    "        loss += -np.log(ps[t][targets[t], 0])\n",
    "    \n",
    "    return loss, hs[len(inputs)-1], (xs, hs, ys, ps)\n",
    "\n",
    "# Test with first 10 characters\n",
    "test_input = [char_to_idx[ch] for ch in data[:seq_length]]\n",
    "test_target = [char_to_idx[ch] for ch in data[1:seq_length+1]]\n",
    "h0 = np.zeros((hidden_size, 1))\n",
    "\n",
    "loss, h_final, cache = forward_pass(test_input, test_target, h0)\n",
    "\n",
    "print(f\"Input sequence: '{data[:seq_length]}'\")\n",
    "print(f\"Target sequence: '{data[1:seq_length+1]}'\")\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Final hidden state shape: {h_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff453c",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "1. Each character was converted to a one-hot vector\n",
    "2. RNN processed them one by one, updating hidden state\n",
    "3. At each step, RNN predicted next character\n",
    "4. Loss = how wrong the predictions were\n",
    "\n",
    "**Initial loss is high** because weights are random! ðŸŽ²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d23cceb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Sampling (Before Training)\n",
    "\n",
    "Let's see what the model generates with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_idx, n, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text by sampling from the model.\n",
    "    \n",
    "    Args:\n",
    "        h: Initial hidden state\n",
    "        seed_idx: Starting character index\n",
    "        n: Number of characters to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        \n",
    "    Returns:\n",
    "        indices: List of generated character indices\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_idx] = 1\n",
    "    indices = []\n",
    "    \n",
    "    for t in range(n):\n",
    "        h = np.tanh(Wxh @ x + Whh @ h + bh)\n",
    "        y = Why @ h + by\n",
    "        \n",
    "        # Apply temperature\n",
    "        y = y / temperature\n",
    "        p = np.exp(y - np.max(y)) / np.sum(np.exp(y - np.max(y)))\n",
    "        \n",
    "        # Sample from distribution\n",
    "        idx = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        # Update input for next step\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        indices.append(idx)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# Generate before training\n",
    "h = np.zeros((hidden_size, 1))\n",
    "seed = char_to_idx['h']\n",
    "sample_indices = sample(h, seed, 50)\n",
    "sample_text = ''.join([idx_to_char[i] for i in sample_indices])\n",
    "\n",
    "print(\"Generated text (untrained):\")\n",
    "print(f\"'{sample_text}'\")\n",
    "print(\"\\n(Looks like gibberish, right? That's expected!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e0791",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Backward Pass (BPTT)\n",
    "\n",
    "Now we need to compute gradients to update weights.\n",
    "\n",
    "**Backpropagation Through Time** = apply chain rule backwards through the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fe8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(inputs, targets, cache):\n",
    "    \"\"\"\n",
    "    Backward pass: compute gradients via BPTT.\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = cache\n",
    "    \n",
    "    # Initialize gradients\n",
    "    dWxh = np.zeros_like(Wxh)\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    dWhy = np.zeros_like(Why)\n",
    "    dbh = np.zeros_like(bh)\n",
    "    dby = np.zeros_like(by)\n",
    "    dh_next = np.zeros_like(hs[0])\n",
    "    \n",
    "    # Backward through time\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Gradient of loss w.r.t. output\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1  # Softmax + cross-entropy gradient\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dWhy += dy @ hs[t].T\n",
    "        dby += dy\n",
    "        \n",
    "        # Backprop to hidden state\n",
    "        dh = Why.T @ dy + dh_next\n",
    "        \n",
    "        # Backprop through tanh\n",
    "        dh_raw = (1 - hs[t] ** 2) * dh\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dbh += dh_raw\n",
    "        dWxh += dh_raw @ xs[t].T\n",
    "        dWhh += dh_raw @ hs[t-1].T\n",
    "        \n",
    "        # Gradient for next iteration\n",
    "        dh_next = Whh.T @ dh_raw\n",
    "    \n",
    "    # Clip gradients to prevent explosion\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "# Test backward pass\n",
    "grads = backward_pass(test_input, test_target, cache)\n",
    "print(\"Gradients computed successfully! âœ…\")\n",
    "print(f\"Gradient shapes: {[g.shape for g in grads]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d627f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Training Loop\n",
    "\n",
    "Now let's train! We'll use **Adagrad** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f103e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to indices\n",
    "data_indices = [char_to_idx[ch] for ch in data]\n",
    "data_size = len(data_indices)\n",
    "\n",
    "# Adagrad memory\n",
    "mWxh = np.zeros_like(Wxh)\n",
    "mWhh = np.zeros_like(Whh)\n",
    "mWhy = np.zeros_like(Why)\n",
    "mbh = np.zeros_like(bh)\n",
    "mby = np.zeros_like(by)\n",
    "\n",
    "# Training\n",
    "losses = []\n",
    "smooth_loss = -np.log(1.0/vocab_size) * seq_length\n",
    "p = 0  # Data pointer\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "\n",
    "for iteration in range(1000):\n",
    "    # Reset if at end of data\n",
    "    if p + seq_length + 1 >= data_size or iteration == 0:\n",
    "        h_prev = np.zeros((hidden_size, 1))\n",
    "        p = 0\n",
    "    \n",
    "    # Get batch\n",
    "    inputs = data_indices[p:p+seq_length]\n",
    "    targets = data_indices[p+1:p+seq_length+1]\n",
    "    \n",
    "    # Forward pass\n",
    "    loss, h_prev, cache = forward_pass(inputs, targets, h_prev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    losses.append(smooth_loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    dWxh, dWhh, dWhy, dbh, dby = backward_pass(inputs, targets, cache)\n",
    "    \n",
    "    # Adagrad update\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                   [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                   [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param -= learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "    \n",
    "    p += seq_length\n",
    "    \n",
    "    # Print progress\n",
    "    if iteration % 100 == 0:\n",
    "        print(f\"Iteration {iteration}, Loss: {smooth_loss:.4f}\")\n",
    "        \n",
    "        # Generate sample\n",
    "        sample_indices = sample(h_prev, inputs[0], 50, temperature=0.8)\n",
    "        txt = ''.join(idx_to_char[i] for i in sample_indices)\n",
    "        print(f\"  Sample: '{txt}'\\n\")\n",
    "\n",
    "print(\"Training complete! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade8213",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Started at: {losses[0]:.4f}\")\n",
    "print(f\"Improvement: {losses[0] - losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36630d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Play with Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Effect of Temperature on Sampling\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "temperatures = [0.2, 0.5, 0.8, 1.0, 1.5, 2.0]\n",
    "h = np.zeros((hidden_size, 1))\n",
    "seed = char_to_idx['h']\n",
    "\n",
    "for temp in temperatures:\n",
    "    sample_indices = sample(h, seed, 100, temperature=temp)\n",
    "    txt = ''.join(idx_to_char[i] for i in sample_indices)\n",
    "    print(f\"Temperature = {temp}:\")\n",
    "    print(f\"  {txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd4624",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- **Low temperature (0.2)**: Very conservative, repetitive\n",
    "- **Medium temperature (0.8)**: Balanced creativity\n",
    "- **High temperature (2.0)**: More random, less coherent\n",
    "\n",
    "**Why?** Temperature scales the logits before softmax:\n",
    "- Low T â†’ sharper distribution â†’ always pick most likely\n",
    "- High T â†’ flatter distribution â†’ more randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa026f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: What Did We Learn?\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **RNNs have memory** through hidden states\n",
    "2. **They can learn patterns** from data\n",
    "3. **Temperature controls creativity** vs coherence\n",
    "4. **Gradient clipping is crucial** to prevent explosion\n",
    "5. **Simple models can do surprising things**\n",
    "\n",
    "### Why \"Unreasonable Effectiveness\"?\n",
    "\n",
    "With just a few thousand parameters, we can:\n",
    "- Generate Shakespeare\n",
    "- Write code\n",
    "- Compose music\n",
    "\n",
    "The model **discovers structure** in the data:\n",
    "- Words\n",
    "- Grammar\n",
    "- Style\n",
    "\n",
    "All from predicting one character at a time!\n",
    "\n",
    "### Connection to Modern AI\n",
    "\n",
    "This is the foundation of:\n",
    "- **LSTMs** (Day 2) â†’ better memory\n",
    "- **Transformers** (Day 13+) â†’ attention mechanism\n",
    "- **GPT** â†’ scale this up massively\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Try larger datasets**: Shakespeare, Wikipedia, code\n",
    "2. **Tune hyperparameters**: Hidden size, learning rate\n",
    "3. **Complete the exercises** in `/exercises`\n",
    "4. **Move to Day 2**: LSTMs!\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- ðŸ“– [Original blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- ðŸ“Š [Visualizing RNNs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- ðŸ’» [Implementation code](./implementation.py)\n",
    "- ðŸŽ¯ [Exercises](./exercises/)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed Day 1. ðŸŽ‰\n",
    "\n",
    "Share your progress with **#30u30**!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
