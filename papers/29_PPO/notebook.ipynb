{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 29: Proximal Policy Optimization Algorithms\n",
    "\n",
    "> Schulman, Wolski, Dhariwal, Radford, Klimov -- OpenAI (2017)\n",
    "> https://arxiv.org/abs/1707.06347\n",
    "\n",
    "What you will build in this notebook:\n",
    "1. The probability ratio r_t = pi_new / pi_old\n",
    "2. The clipped surrogate objective (Equation 7)\n",
    "3. Generalized Advantage Estimation (GAE)\n",
    "4. The full PPO loss (Equation 9)\n",
    "5. A complete PPO training loop on CartPole-v1\n",
    "6. Visualization of clip fraction and entropy over training\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print('Setup complete.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Probability Ratio\n",
    "\n",
    "The ratio r_t(theta) = pi_theta(a_t|s_t) / pi_theta_old(a_t|s_t) measures\n",
    "how much the new policy differs from the old policy for a specific action.\n",
    "\n",
    "- r_t = 1: policies are identical for this action\n",
    "- r_t > 1: new policy assigns higher probability to this action\n",
    "- r_t < 1: new policy assigns lower probability to this action\n",
    "\n",
    "Computed in log space for numerical stability: r_t = exp(log_new - log_old)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrate the probability ratio\n",
    "log_probs_old = torch.tensor([-1.0, -0.5, -2.0, -0.3])\n",
    "log_probs_new = torch.tensor([-0.8, -0.5, -2.5, -0.1])  # slightly different policy\n",
    "\n",
    "ratio = torch.exp(log_probs_new - log_probs_old)\n",
    "print('Old log probs:', log_probs_old.numpy())\n",
    "print('New log probs:', log_probs_new.numpy())\n",
    "print('Ratio r_t:    ', ratio.numpy().round(3))\n",
    "print()\n",
    "print('ratio > 1: new policy assigns MORE probability to this action')\n",
    "print('ratio < 1: new policy assigns LESS probability to this action')\n",
    "print('ratio = 1: policies are identical for this action')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Clipped Surrogate Objective (Equation 7)\n",
    "\n",
    "The core contribution of the paper. The min() makes this a pessimistic\n",
    "lower bound: the objective stops improving once the ratio moves beyond\n",
    "[1-epsilon, 1+epsilon].\n",
    "\n",
    "L_CLIP = E[min(r_t * A_t, clip(r_t, 1-eps, 1+eps) * A_t)]\n",
    "\n",
    "The paper uses epsilon=0.2 as the default (Section 3)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def clipped_surrogate_loss(log_probs_new, log_probs_old, advantages, epsilon=0.2):\n",
    "    \"\"\"Equation 7 from Schulman et al. (2017).\"\"\"\n",
    "    ratio = torch.exp(log_probs_new - log_probs_old)\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    return -torch.min(surr1, surr2).mean()\n",
    "\n",
    "# Visualize: what happens to the objective as the ratio changes?\n",
    "ratios = torch.linspace(0.5, 1.5, 100)\n",
    "advantage_pos = torch.tensor(1.0)   # positive advantage (good action)\n",
    "advantage_neg = torch.tensor(-1.0)  # negative advantage (bad action)\n",
    "epsilon = 0.2\n",
    "\n",
    "obj_pos = torch.min(ratios * advantage_pos, torch.clamp(ratios, 1-epsilon, 1+epsilon) * advantage_pos)\n",
    "obj_neg = torch.min(ratios * advantage_neg, torch.clamp(ratios, 1-epsilon, 1+epsilon) * advantage_neg)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(ratios.numpy(), obj_pos.numpy(), 'steelblue', linewidth=2)\n",
    "axes[0].axvline(1-epsilon, color='gray', linestyle='--', alpha=0.7, label=f'1-eps={1-epsilon}')\n",
    "axes[0].axvline(1+epsilon, color='gray', linestyle='--', alpha=0.7, label=f'1+eps={1+epsilon}')\n",
    "axes[0].set_title('Positive Advantage (A_t > 0)')\n",
    "axes[0].set_xlabel('Probability ratio r_t')\n",
    "axes[0].set_ylabel('Objective contribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(ratios.numpy(), obj_neg.numpy(), 'coral', linewidth=2)\n",
    "axes[1].axvline(1-epsilon, color='gray', linestyle='--', alpha=0.7, label=f'1-eps={1-epsilon}')\n",
    "axes[1].axvline(1+epsilon, color='gray', linestyle='--', alpha=0.7, label=f'1+eps={1+epsilon}')\n",
    "axes[1].set_title('Negative Advantage (A_t < 0)')\n",
    "axes[1].set_xlabel('Probability ratio r_t')\n",
    "axes[1].set_ylabel('Objective contribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('PPO Clipped Surrogate Objective (Equation 7)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('The objective is flat outside [1-eps, 1+eps]: no incentive to move further.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generalized Advantage Estimation (GAE)\n",
    "\n",
    "PPO uses GAE (Schulman et al. 2015b, cited in Section 4) to estimate advantages.\n",
    "\n",
    "A_t = sum_{l=0}^{inf} (gamma * lambda)^l * delta_{t+l}\n",
    "delta_t = r_t + gamma * V(s_{t+1}) * (1 - done_t) - V(s_t)\n",
    "\n",
    "lambda controls bias vs. variance:\n",
    "- lambda=0: one-step TD (low variance, high bias)\n",
    "- lambda=1: Monte Carlo (high variance, low bias)\n",
    "- lambda=0.95: paper default, good balance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.99, lambda_=0.95):\n",
    "    \"\"\"GAE backward recurrence. values has shape (T+1,) including bootstrap.\"\"\"\n",
    "    T = len(rewards)\n",
    "    advantages = np.zeros(T)\n",
    "    gae = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma * values[t+1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lambda_ * (1 - dones[t]) * gae\n",
    "        advantages[t] = gae\n",
    "    return advantages\n",
    "\n",
    "# Demonstrate lambda effect\n",
    "T = 20\n",
    "rewards = np.ones(T) * 1.0\n",
    "values = np.ones(T + 1) * 5.0  # overestimated baseline\n",
    "dones = np.zeros(T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "for lam, color in [(0.0, 'steelblue'), (0.5, 'mediumseagreen'), (0.95, 'coral'), (1.0, 'mediumpurple')]:\n",
    "    adv = compute_gae(rewards, values, dones, gamma=0.99, lambda_=lam)\n",
    "    ax.plot(adv, label=f'lambda={lam}', color=color, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Timestep t')\n",
    "ax.set_ylabel('Advantage A_t')\n",
    "ax.set_title('GAE Advantages for Different Lambda Values')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('lambda=0: only uses immediate TD error (low variance)')\n",
    "print('lambda=1: accumulates all future TD errors (high variance)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Full PPO Loss (Equation 9)\n",
    "\n",
    "The complete objective combines three terms (Equation 9 from the paper):\n",
    "\n",
    "L = L_CLIP - c1 * L_VF + c2 * S[pi]\n",
    "\n",
    "- L_CLIP: clipped policy loss (Section 3)\n",
    "- L_VF = (V(s) - V_target)^2: value function MSE\n",
    "- S[pi]: entropy bonus (encourages exploration)\n",
    "- c1=1.0, c2=0.01 for Atari; c2=0 for MuJoCo (Table 3 in paper)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def ppo_full_loss(log_probs_new, log_probs_old, advantages, values, returns, entropy,\n",
    "                  epsilon=0.2, c1=1.0, c2=0.01):\n",
    "    \"\"\"Full PPO objective, Equation 9 from Schulman et al. (2017).\"\"\"\n",
    "    # Normalize advantages within minibatch (standard practice, not in paper equations)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # L_CLIP: clipped surrogate (Equation 7)\n",
    "    ratio = torch.exp(log_probs_new - log_probs_old)\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "    # L_VF: value function MSE\n",
    "    value_loss = F.mse_loss(values, returns)\n",
    "\n",
    "    # S[pi]: entropy bonus\n",
    "    entropy_loss = -entropy.mean()\n",
    "\n",
    "    # Combined: minimize policy_loss + c1*value_loss + c2*entropy_loss\n",
    "    total = policy_loss + c1 * value_loss + c2 * entropy_loss\n",
    "\n",
    "    clip_fraction = ((ratio - 1.0).abs() > epsilon).float().mean().item()\n",
    "    return total, {'policy': policy_loss.item(), 'value': value_loss.item(),\n",
    "                   'entropy': -entropy_loss.item(), 'clip_frac': clip_fraction}\n",
    "\n",
    "# Quick sanity check\n",
    "batch = 64\n",
    "log_new = torch.randn(batch)\n",
    "log_old = log_new.detach() + torch.randn(batch) * 0.1  # slightly different\n",
    "adv = torch.randn(batch)\n",
    "vals = torch.randn(batch)\n",
    "rets = torch.randn(batch)\n",
    "ent = torch.rand(batch) * 0.5 + 0.1\n",
    "\n",
    "loss, info = ppo_full_loss(log_new, log_old, adv, vals, rets, ent)\n",
    "print('PPO full loss components:')\n",
    "for k, v in info.items():\n",
    "    print(f'  {k}: {v:.4f}')\n",
    "print(f'  total loss: {loss.item():.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training PPO on CartPole-v1\n",
    "\n",
    "CartPole-v1 is considered solved when the average reward over 100\n",
    "consecutive episodes exceeds 475.\n",
    "\n",
    "We use the PPOAgent from implementation.py, which follows Algorithm 1\n",
    "from the paper."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from implementation import PPOAgent\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n\n",
    "\n",
    "agent = PPOAgent(\n",
    "    obs_dim=obs_dim, act_dim=act_dim,\n",
    "    T=512, K=4, M=64,\n",
    "    epsilon=0.2, gamma=0.99, lambda_=0.95,\n",
    "    lr=3e-4, c1=1.0, c2=0.01,\n",
    ")\n",
    "\n",
    "rewards_history = []\n",
    "stats_history = []\n",
    "\n",
    "for i in range(60):\n",
    "    mean_reward = agent.collect_rollout(env)\n",
    "    stats = agent.update()\n",
    "    rewards_history.append(mean_reward)\n",
    "    stats_history.append(stats)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'Iter {i+1:3d} | reward={mean_reward:6.1f} | '\n",
    "              f'clip_frac={stats[\"clip_fraction\"]:.3f} | '\n",
    "              f'entropy={stats[\"entropy\"]:.4f}')\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Diagnostics\n",
    "\n",
    "The clip fraction is a key diagnostic for PPO:\n",
    "- Too low (< 5%): epsilon is too small, updates are too conservative\n",
    "- Too high (> 50%): epsilon is too large, updates are too aggressive\n",
    "- Healthy range: 5-30%\n",
    "\n",
    "Entropy should decrease over training as the policy becomes more deterministic."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Reward curve\n",
    "axes[0].plot(rewards_history, alpha=0.5, color='steelblue')\n",
    "if len(rewards_history) >= 10:\n",
    "    rolling = np.convolve(rewards_history, np.ones(10)/10, mode='valid')\n",
    "    axes[0].plot(np.arange(9, len(rewards_history)), rolling, 'steelblue', linewidth=2)\n",
    "axes[0].set_title('Reward over Training')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Mean Episode Reward')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Clip fraction\n",
    "clip_fracs = [s['clip_fraction'] for s in stats_history]\n",
    "axes[1].plot(clip_fracs, color='coral')\n",
    "axes[1].axhline(0.05, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(0.30, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_title('Clip Fraction')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Fraction of steps clipped')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Entropy\n",
    "entropies = [s['entropy'] for s in stats_history]\n",
    "axes[2].plot(entropies, color='mediumseagreen')\n",
    "axes[2].set_title('Policy Entropy')\n",
    "axes[2].set_xlabel('Iteration')\n",
    "axes[2].set_ylabel('Entropy (nats)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('PPO Training Diagnostics: CartPole-v1', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **The clip is a pessimistic lower bound.** The min() in L_CLIP takes the\n",
    "   worse of clipped and unclipped. The agent never benefits from moving the\n",
    "   ratio beyond [1-eps, 1+eps]. (Section 3 of the paper.)\n",
    "\n",
    "2. **Multiple epochs on the same data is the efficiency gain.** Standard PG\n",
    "   discards data after one step. PPO reuses each batch for K epochs.\n",
    "   (Algorithm 1, Section 3.)\n",
    "\n",
    "3. **GAE lambda controls bias-variance tradeoff.** lambda=0 is one-step TD\n",
    "   (low variance, high bias). lambda=1 is Monte Carlo (high variance, low bias).\n",
    "   The paper uses lambda=0.95 throughout.\n",
    "\n",
    "4. **PPO is the algorithm inside RLHF.** InstructGPT and ChatGPT use PPO to\n",
    "   fine-tune language models on human preference signals. Day 30 covers this\n",
    "   directly.\n",
    "\n",
    "   Note: The RLHF connection is our retrospective addition, not from the 2017 paper.\n",
    "\n",
    "**Next:** [Day 30 - Deep Reinforcement Learning from Human Feedback](../30_RLHF/)\n"
   ]
  }
 ]
}