{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 25: The Physics of Scale\n",
    "## Interactive Analysis: Scaling Laws for Neural Language Models\n",
    "\n",
    "This notebook provides a first-principles walkthrough of the scaling laws that define modern AI, following the empirical rigor established in the original paper. \n",
    "\n",
    "### Learning Objectives:\n",
    "1. **Derive the 6N Rule**: Calculate FLOPs for arbitrary scale.\n",
    "2. **The 12Ld^2 Rule**: Verify parameter counting on a scratch-built Transformer.\n",
    "3. **The Kaplan Sweep**: Sweep model sizes and witness the log-log straight line.\n",
    "4. **Non-Linear Fitting**: Fit the irreducible loss $L_\\infty$ using `MasterFitter`.\n",
    "5. **Compute Optimization**: Find the optimal frontier for a fixed budget.\n",
    "6. **The Predictor**: Predict GPT-3 performance from 1M parameter data.\n",
    "7. **Explaining the Chinchilla Gap**: Identify why Kaplan's curves were slightly biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementation import KaplanTransformer, MasterFitter, ComputeEconomy\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Arithmetic of a Transformer\n",
    "Kaplan et al. (Section 2.1) claim that the number of non-embedding parameters $N$ for a Transformer is roughly $12Ld_{model}^2$.\n",
    "Let's prove this by building a block and counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_parameters(d_model=256, n_layers=4):\n",
    "    model = KaplanTransformer(vocab_size=100, d_model=d_model, n_heads=4, n_layers=n_layers)\n",
    "    \n",
    "    actual_n = model.count_parameters(mode=\"Kaplan\")\n",
    "    theoretical_n = 12 * n_layers * (d_model**2)\n",
    "    \n",
    "    print(f\"\\033[1mConfiguration: d_model={d_model}, L={n_layers}\\033[0m\")\n",
    "    print(f\"Actual N (Kaplan): {actual_n:,}\")\n",
    "    print(f\"Theoretical 12Ld^2: {theoretical_n:,}\")\n",
    "    print(f\"Precision: {100 - abs(actual_n - theoretical_n)/theoretical_n*100:.2f}%\")\n",
    "\n",
    "analyze_parameters(128, 2)\n",
    "analyze_parameters(512, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Economy: The 6N Rule\n",
    "The total compute $C$ for training is $6N$ FLOPS per token. \n",
    "Let's see how many PF-days it would take to train models of various sizes on 300 Billion tokens (the GPT-3 budget)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = [1e6, 1e8, 1e9, 1.75e11] # 1M to GPT-3\n",
    "tokens = 300e9\n",
    "\n",
    "for n in n_params:\n",
    "    c_pf = ComputeEconomy.calculate_c_pfdays(n, tokens)\n",
    "    print(f\"N: {n:10.0e} | Compute: {c_pf:10.2e} PF-days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Power Law: Log-Log Linearity\n",
    "We will now simulate a scaling sweep and use `MasterFitter` to recover the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with noise and irreducible loss\n",
    "ns_empirical = np.logspace(5, 8, 8)\n",
    "l_inf_true = 1.7\n",
    "alpha_true = 0.076\n",
    "nc_true = 8.8e13\n",
    "\n",
    "ls_empirical = l_inf_true + (nc_true / ns_empirical)**alpha_true + np.random.normal(0, 0.01, 8)\n",
    "\n",
    "# Fit\n",
    "fitter = MasterFitter(ns_empirical, ls_empirical)\n",
    "fitter.fit()\n",
    "\n",
    "print(f\"Recovered alpha: {fitter.alpha:.4f} (Target: {alpha_true})\")\n",
    "print(f\"Recovered L_inf: {fitter.l_inf:.4f} (Target: {l_inf_true})\")\n",
    "\n",
    "# Plot\n",
    "n_plot = np.logspace(4, 12, 100)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(ns_empirical, ls_empirical, label=\"Empirical Pts\")\n",
    "plt.plot(n_plot, fitter.predict(n_plot), 'r--', label=\"Fitted Law\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(\"The Universal Law of Model Size\")\n",
    "plt.xlabel(\"Parameters (N)\")\n",
    "plt.ylabel(\"Loss (L)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute-Optimal Frontier\n",
    "If you have 1 PF-day of budget, what $N$ and $D$ should you pick? \n",
    "Kaplan (2020) vs Chinchilla (2022) differ here. Let's visualize the trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 1e-2 # 0.01 PF-days\n",
    "n_candidates = np.logspace(6, 9, 100)\n",
    "\n",
    "# C = 6 * N * D => D = C / (6 * N)\n",
    "def get_d(n, c_pfdays):\n",
    "    flops = c_pfdays * 1e15 * 60 * 60 * 24\n",
    "    return flops / (6 * n)\n",
    "\n",
    "losses_cap = [fitter.predict(n) for n in n_candidates]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_candidates, losses_cap, label=\"Loss at Fixed Budget\")\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Model Size (N)\")\n",
    "plt.ylabel(\"Estimated Loss\")\n",
    "plt.title(\"Finding the Optimum: Kaplan Frontier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Insights\n",
    "1. **Scale is predictable**: Within 100k to 1.5B parameters, laws are nearly perfect.\n",
    "2. **Irreducible Loss**: No matter how big the model, $L_\\infty$ remains (entropy of the source).\n",
    "3. **Compute is the currency**: All scaling factors eventually map back to FLOPs.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
