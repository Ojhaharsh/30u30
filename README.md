# 30u30: Master Ilya's 30 Foundational AI Papers in 30 Days

[![License: CC BY-NC-ND 4.0](https://img.shields.io/badge/License-CC%20BY--NC--ND%204.0-lightgrey.svg?style=for-the-badge)](https://creativecommons.org/licenses/by-nc-nd/4.0/)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/downloads/)
[![GitHub stars](https://img.shields.io/github/stars/Ojhaharsh/30u30?style=for-the-badge&logo=github)](https://github.com/Ojhaharsh/30u30)
[![Website](https://img.shields.io/badge/Website-30u30-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ojhaharsh.github.io/30u30/)

> *"If you really learn all of these, you'll know 90% of what matters today"* - Ilya Sutskever

> Complete implementations â€¢ Interactive notebooks â€¢ Beginner-friendly â€¢ 100% Free

**30u30 is an open-source study guide that takes you through the 30 foundational AI papers recommended by Ilya Sutskever â€” one paper per day.** 
Each paper comes with a full implementation from scratch, detailed notes, interactive exercises with solutions, and visualizations.  
Think of it as [Rustlings](https://github.com/rust-lang/rustlings), but for deep learning fundamentals. You read the paper, understand the math, and build it yourself.

---

## ğŸš€ Units 1-4 Complete! Now in Unit 5: Generative Models & Scaling

### âœ… Unit 1: The Foundations (Days 1-7) - COMPLETE!

**Day 1: "The Unreasonable Effectiveness of Recurrent Neural Networks"**
- Character-level RNN from scratch (pure NumPy)  
- Interactive Jupyter notebook with visualizations  
- 5 progressive exercises + solutions  
**[Start Day 1 â†’](papers/01_Unreasonable_Effectiveness/)**

**Day 2: "Understanding LSTM Networks"**
- Complete LSTM with 4 gates from scratch
- Gate activation analysis & visualizations
- 5 exercises including LSTM vs GRU comparison  
**[Start Day 2 â†’](papers/02_Understanding_LSTM/)**

**Day 3: "RNN Regularization"**
- Dropout, Layer Norm, Weight Decay, Early Stopping
- Complete regularization pipeline from scratch
- 5 exercises on preventing overfitting  
**[Start Day 3 â†’](papers/03_RNN_Regularization/)**

**Day 4: "Minimizing Description Length"**
- Bayesian / Noisy-Weight networks, MDL intuition
- Uncertainty envelopes, compression analysis, pareto frontier
- 5 exercises that demonstrate gaps, beta tuning, MC inference  
**[Start Day 4 â†’](papers/04_Minimizing_Description_Length/)**

**Day 5: "MDL Principle Tutorial"**
- Two-Part Codes, Prequential MDL, NML Complexity
- MDL vs AIC vs BIC comparison, compression analysis
- 5 exercises: from basic MDL to model selection showdown  
**[Start Day 5 â†’](papers/05_MDL_Principle/)**

**Day 6: "The First Law of Complexodynamics"**
- Information equilibration, channel capacity, evolutionary dynamics
- Complete complexity evolution simulator with 7 visualizations
- 5 exercises: from Shannon entropy to real genome analysis  
**[Start Day 6 â†’](papers/06_Complexodynamics/)**

**Day 7: "The Coffee Automaton"**
- Cellular automaton complexity, heat diffusion, emergent behavior
- Chaos theory, Lyapunov exponents, information flow analysis
- 5 exercises: edge of chaos, pattern classification, neural network initialization  
**[Start Day 7 â†’](papers/07_coffee_automaton/)**

### âœ… Unit 2: Deep Learning Explosion (Vision & Architectures) - COMPLETE!

**Day 8: "ImageNet Classification with Deep CNNs (AlexNet)"**
- The paper that sparked the deep learning revolution
- GPU-accelerated training, ReLU activations, dropout regularization
- 5 exercises: GPU impact analysis, activation functions, data augmentation  
**[Start Day 8 â†’](papers/08_alexnet/)**

**Day 9: "Deep Residual Learning for Image Recognition (ResNet)"**
- Skip connections that enable 100+ layer networks
- Identity mappings, residual blocks, gradient highway
- 5 exercises: vanishing gradients, skip connection ablation, depth analysis  
**[Start Day 9 â†’](papers/09_resnet/)**

**Day 10: "Identity Mappings in Deep Residual Networks (ResNet v2)"**
- Pre-activation design: BN â†’ ReLU â†’ Conv
- Why order matters for 1000+ layer networks
- 5 exercises: pre vs post activation, information flow, extreme depth  
**[Start Day 10 â†’](papers/10_resnet_v2/)**

**Day 11: "Multi-Scale Context Aggregation by Dilated Convolutions"**
- Exponentially expanding receptive fields without pooling
- Dense prediction, semantic segmentation, WaveNet foundations
- 5 exercises: receptive field analysis, dilation patterns, context modules  
**[Start Day 11 â†’](papers/11_dilated_convolutions/)**

**Day 12: "Dropout: A Simple Way to Prevent Overfitting"**
- The standard regularization technique for neural networks
- Inverted dropout, MC Dropout for uncertainty, ensemble interpretation
- 5 exercises: implement dropout, rate sweep, spatial dropout, MC uncertainty  
**[Start Day 12 â†’](papers/12_dropout/)**

### âœ… Unit 3: The Transformer Era (Days 13-16) - COMPLETE!

**Day 13: "Attention Is All You Need"**
- The paper that revolutionized NLP and beyond - the Transformer
- Self-attention, multi-head attention, positional encoding
- 5 exercises: from scaled dot-product to full Transformer + interactive visualization  
**[Start Day 13 â†’](papers/13_attention/)**

**Day 14: "The Annotated Transformer"**
- Code-level understanding of the Transformer - from math to PyTorch
- Production-quality implementation with all training infrastructure
- 5 exercises: attention, multi-head, encoder, training, inference  
**[Start Day 14 â†’](papers/14_annotated_transformer/)**

**Day 15: "Neural Machine Translation by Jointly Learning to Align and Translate"**
- The original attention mechanism - before Transformers existed!
- Bahdanau (additive) attention, bidirectional encoder, alignment visualization
- 5 exercises: attention from scratch, encoder-decoder, beam search, visualization  
**[Start Day 15 â†’](papers/15_bahdanau_attention/)**

**Day 16: "Order Matters: Sequence to Sequence for Sets"**
- Pointer Networks - process sets, output sequences by pointing!
- Order-invariant encoding, Read-Process-Write framework
- 5 exercises: pointer attention, set encoder, sorting, convex hull, TSP  
**[Start Day 16 â†’](papers/16_order_matters/)**

### âœ… Unit 4: Specialized Architectures (Days 17-22) - COMPLETE!

**Day 17: "Neural Turing Machines"**
- Differentiable external memory for neural networks
- Addressing mechanics: Content-based, Interpolation, Shift, Sharpening
- 5 exercises: addressing logic, circular convolution, memory updates  
**[Start Day 17 â†’](papers/17_neural_turing_machines/)**

**Day 18: "Pointer Networks"**
- Networks that can "point" to their input (essential for combinatorial problems)
- Laser pointer attention, sampling without replacement, combinatorial optimization
- 5 exercises: pointer attention, convex hull formatting, TSP cost analysis  
**[Start Day 18 â†’](papers/18_pointer_networks/)**

**Day 19: "Relational Reasoning"**
- Pairwise object processing for VQA and physical reasoning
- g_theta and f_phi modules, set-based inductive bias
- 5 exercises: pair generation, sort-of-CLEVR logic, masking  
**[Start Day 19 â†’](papers/19_relational_reasoning/)**

**Day 20: "Relational Recurrent Neural Networks"**
- Multi-head dot-product attention inside a recurrent cell (MHDPA)
- Relational memory core: memory slots interact via self-attention at each timestep
- 5 exercises: memory attention, slot interactions, sequence modeling  
**[Start Day 20 â†’](papers/20_Relational_RNNs/)**

**Day 21: "Neural Message Passing for Quantum Chemistry"**
- Unifying framework for graph neural networks: message, update, readout
- Edge networks, GRU update, Set2Set readout, QM9 benchmark
- 5 exercises: message functions, graph construction, property prediction  
**[Start Day 21 â†’](papers/21_Neural_Message_Passing/)**

**Day 22: "Deep Speech 2: End-to-End Speech Recognition"**
- End-to-end speech recognition replacing traditional ASR pipelines
- Conv + bidirectional GRU + CTC loss, sequence-wise BatchNorm, SortaGrad
- 5 exercises: spectrogram features, CTC decoding, RNN BatchNorm, curriculum learning, full pipeline  
**[Start Day 22 â†’](papers/22_deep_speech_2/)**

### ğŸ§  Unit 5: Generative Models & Scaling (Days 23-27) - STARTING NOW!

**Day 23: "Variational Lossy Autoencoder"**
- Curing posterior collapse in VAEs with powerful decoders
- Restricted receptive field (PixelCNN) + Inverse Autoregressive Flows (IAF)
- 5 exercises: from masked convolutions to full flow priors   
**[Start Day 23 â†’](papers/23_variational_lossy_autoencoder/)**

**Day 24: "GPipe: Efficient Training of Giant Neural Networks"**
- Pipeline parallelism + micro-batching + activation checkpointing
- Training giant 6B+ parameter models on limited hardware
- 5 exercises: from micro-batching to full pipeline integration  
**[Start Day 24 â†’](papers/24_gpipe/)**

**Day 25: "Scaling Laws for Neural Language Models"**
- The power-law relationships between model size, compute, and performance
- Scaling compute budget vs. model size vs. dataset size
- 5 exercises: scaling law calculations, compute-optimal training, dataset scaling  
**[Start Day 25 â†’](papers/25_scaling_laws/)**

**Day 26: "Kolmogorov Complexity and Algorithmic Randomness"**
- The mathematical bedrock of information theory: Compression = Intelligence
- From-scratch implementation of Huffman and Arithmetic coding
- 5 exercises: entropy comparison, NCD similarity clustering, and incompressibility  
**[Start Day 26 â†’](papers/26_kolmogorov_complexity/)**

**Day 27: "Machine Super Intelligence (Shane Legg)"**
- Universal Intelligence (Î¥), Kolmogorov Complexity proxies, and the Agent-Environment loop
- Formal benchmarking of Random vs. RL vs. Predictive agents
- 5 exercises on Upsilon calculation, environment design, and complexity invariance
**[Start Day 27 â†’](papers/27_MSI/)**

---

## ğŸ¯ Mission

This is the most comprehensive, beginner-friendly, open-source journey through the papers that defined modern AI. No paywalls. No gatekeeping. Just pure knowledge.

Whether you're pivoting to AI, a student, or a curious mind - this is your roadmap.

## ğŸ“š What You'll Find Here

Each paper gets the full treatment:
- **ğŸ“– Deep-dive README** - Complete explanations with real-world analogies
- **ğŸ’¡ ELI5 Notes** - "Explain Like I'm 5" summaries
- **ğŸ’» Implementation** - Clean, commented, CPU-friendly code
- **ğŸ¨ Visualizations** - See the concepts come alive
- **ğŸ‹ï¸ Exercises** - Build it yourself (with solutions)
- **ğŸ““ Notebooks** - Interactive Jupyter walkthroughs
- **âš¡ Quick-start** - Minimal training scripts that run in minutes

## ğŸ—ºï¸ The Journey

### Unit 1: The Foundations (Days 1-7) - âœ… COMPLETE
| Day | Paper | Status | Core Concept |
|-----|-------|--------|--------------|
| 1 | [The Unreasonable Effectiveness of RNNs](papers/01_Unreasonable_Effectiveness/) | ğŸš€ **LIVE** | Why predicting = intelligence |
| 2 | [Understanding LSTM Networks](papers/02_Understanding_LSTM/) | ğŸš€ **LIVE** | The mechanics of memory |
| 3 | [RNN Regularization](papers/03_RNN_Regularization/) | ğŸš€ **LIVE** | Making RNNs generalize |
| 4 | [Minimizing Description Length](papers/04_Minimizing_Description_Length/) | ğŸš€ **LIVE** | Compression = Intelligence |
| 5 | [MDL Principle Tutorial](papers/05_MDL_Principle/) | ğŸš€ **LIVE** | Math of compression |
| 6 | [The First Law of Complexodynamics](papers/06_Complexodynamics/) | ğŸš€ **LIVE** | Physics of complexity |
| 7 | [The Coffee Automaton](papers/07_coffee_automaton/) | ğŸš€ **LIVE** | Why intelligence exists |

### Unit 2: Deep Learning Explosion (Days 8-12) - âœ… COMPLETE
*Vision, depth, and the techniques that changed everything*

| Day | Paper | Status | Core Concept |
|-----|-------|--------|-------------|
| 8 | [ImageNet Classification (AlexNet)](papers/08_alexnet/) | ğŸš€ **LIVE** | Deep learning revolution |
| 9 | [Deep Residual Learning (ResNet)](papers/09_resnet/) | ğŸš€ **LIVE** | Skip connections |
| 10 | [Identity Mappings in ResNets](papers/10_resnet_v2/) | ğŸš€ **LIVE** | Pre-activation design |
| 11 | [Multi-Scale Context (Dilated Conv)](papers/11_dilated_convolutions/) | ğŸš€ **LIVE** | Dilated convolutions |
| 12 | [Dropout (Srivastava et al.)](papers/12_dropout/) | ğŸš€ **LIVE** | Preventing overfitting |

### Unit 3: The Transformer Era (Days 13-16) - âœ… COMPLETE
*The architecture that ate the world*

| Day | Paper | Status | Core Concept |
|-----|-------|--------|-------------|
| 13 | [Attention Is All You Need](papers/13_attention/) | ğŸš€ **LIVE** | Self-attention, Transformer |
| 14 | [The Annotated Transformer](papers/14_annotated_transformer/) | ğŸš€ **LIVE** | Code-level Transformer |
| 15 | [Bahdanau Attention (NMT)](papers/15_bahdanau_attention/) | ğŸš€ **LIVE** | Original attention mechanism |
| 16 | [Order Matters (Pointer Networks)](papers/16_order_matters/) | ğŸš€ **LIVE** | Set-to-sequence problems |

### âœ… Unit 4: Specialized Architectures (Days 17-22) - COMPLETE
*Memory, graphs, and reasoning*

| Day | Paper | Status | Core Concept |
|-----|-------|--------|-------------|
| 17 | [Neural Turing Machines](papers/17_neural_turing_machines/) | ğŸš€ **LIVE** | Differentiable external memory |
| 18 | [Pointer Networks](papers/18_pointer_networks/) | ğŸš€ **LIVE** | Selecting input via attention |
| 19 | [Relational Reasoning](papers/19_relational_reasoning/) | ğŸš€ **LIVE** | Pairwise object relations; g_theta & f_phi modules |
| 20 | [Relational RNNs](papers/20_Relational_RNNs/) | ğŸš€ **LIVE** | Self-attention inside recurrence |
| 21 | [Neural Message Passing](papers/21_Neural_Message_Passing/) | ğŸš€ **LIVE** | MPNN framework for graph neural networks |
| 22 | [Deep Speech 2](papers/22_deep_speech_2/) | ğŸš€ **LIVE** | End-to-end speech recognition with CTC |

### Unit 5: Generative Models & Scaling (Days 23-28) - ğŸ”¥ IN PROGRESS
*From theory to massive models*

| Day | Paper | Status | Core Concept |
|-----|-------|--------|-------------|
| 23 | [Variational Lossy Autoencoder](papers/23_variational_lossy_autoencoder/) | ğŸš€ **LIVE** | Curing posterior collapse with IAF |
| 24 | [GPipe: Efficient Training of Giant Neural Networks](papers/24_gpipe/) | ğŸš€ **LIVE** | Pipeline parallelism |
| 25 | [Scaling Laws for Neural Language Models](papers/25_scaling_laws/) | ğŸš€ **LIVE** | The physics of AI scaling |
| 26 | [Kolmogorov Complexity](papers/26_kolmogorov_complexity/) | ğŸš€ **LIVE** | Math of compression & randomness |
| 27 | [Machine Super Intelligence](papers/27_MSI/) | ğŸš€ **LIVE** | Safety & intelligence definitions |
| 28 | [CS231n: CNNs for Visual Recognition](papers/28_cs231n/) | â³ Coming | Deep Vision & Neural Training |


### Modern Extensions (Days 29-30)
*RLHF and the path to ChatGPT*
- Reinforcement learning from human feedback, instruction tuning, alignment

**[Complete paper list with links â†’](ilya_30_papers.md)**

## âš¡ Quick Start

```bash
# Clone the repo
git clone https://github.com/yourusername/30u30.git
cd 30u30

# Start with Day 1
cd papers/01_Unreasonable_Effectiveness
```

### Prerequisites

**For beginners:** Basic Python knowledge. We'll teach you the rest.

**For practitioners:** Jump to any paper that interests you.

## ğŸ“ How to Use This Repo

**ğŸ¯ The 30-Day Challenge:**
- One paper per day
- Read the README
- Run the code
- Complete exercises
- Share your progress with #30u30

**ğŸ”€ Choose Your Path:**
- **Theory-First**: README â†’ Notes â†’ Code
- **Code-First**: Notebook â†’ Implementation â†’ README
- **Practice-First**: Exercises â†’ Solutions â†’ Deep-dive

## ğŸŒŸ Why This Project?

There are many paper summaries online. But this is different:

1. **You build everything** - No "import magic_ai_library"
2. **Multiple learning paths** - Theory-first, code-first, or interactive
3. **Production-quality** - Code that actually works and teaches
4. **Beginner-friendly** - Real-world analogies + rigorous math
5. **Community-driven** - Your feedback shapes future days

**Goal:** The best free resource for learning AI fundamentals.

If this helps you, â­ star the repo and share it with others!

---

## ğŸ¤ Contributing

We'd love your help making this better!

- ğŸ› **Found a bug?** [Open an issue](../../issues)
- ğŸ’¡ **Have an idea?** [Open an issue](../../issues) with the "enhancement" label or [Start a discussion](../../discussions)
- ğŸ“ **Want to contribute code?** See [CONTRIBUTING.md](CONTRIBUTING.md)

Every contribution helps thousands of learners.

## ğŸ“œ License

CC BY-NC-ND 4.0 â€” Free to read, learn, and share with attribution. Not for commercial use.

## ğŸ™ Acknowledgments

- **Ilya Sutskever** for the original reading list
- **All paper authors** for advancing the field
- **You** for taking this journey

---

## ğŸ’¬ Stay Connected

- ğŸ¦ **Twitter:** Share progress with **#30u30**
- ğŸ“§ **Issues:** [Report bugs or request features](../../issues)
- ğŸ’¬ **Discussions:** [Join the conversation](../../discussions)
- â­ **Star the repo** to stay updated on new releases!

---

**Ready to start?**  
â†’ **[Day 1: Character-Level RNN](papers/01_Unreasonable_Effectiveness/)**  
â†’ **[Day 2: Understanding LSTMs](papers/02_Understanding_LSTM/)**  
â†’ **[Day 3: RNN Regularization](papers/03_RNN_Regularization/)**  
â†’ **[Day 4: Minimizing Description Length](papers/04_Minimizing_Description_Length/)**  
â†’ **[Day 5: MDL Principle Tutorial](papers/05_MDL_Principle/)**  
â†’ **[Day 6: The First Law of Complexodynamics](papers/06_Complexodynamics/)**  
â†’ **[Day 7: The Coffee Automaton](papers/07_coffee_automaton/)**  
â†’ **[Day 8: ImageNet Classification (AlexNet)](papers/08_alexnet/)**  
â†’ **[Day 9: Deep Residual Learning (ResNet)](papers/09_resnet/)**  
â†’ **[Day 10: Identity Mappings (ResNet v2)](papers/10_resnet_v2/)**  
â†’ **[Day 11: Dilated Convolutions](papers/11_dilated_convolutions/)**  
â†’ **[Day 12: Dropout](papers/12_dropout/)**  
â†’ **[Day 13: Attention Is All You Need](papers/13_attention/)**  
â†’ **[Day 14: The Annotated Transformer](papers/14_annotated_transformer/)**  
â†’ **[Day 15: Bahdanau Attention (NMT)](papers/15_bahdanau_attention/)**  
â†’ **[Day 16: Order Matters (Pointer Networks)](papers/16_order_matters/)**  
â†’ **[Day 17: Neural Turing Machines](papers/17_neural_turing_machines/)**  
â†’ **[Day 18: Pointer Networks](papers/18_pointer_networks/)**  
â†’ **[Day 19: Relational Reasoning](papers/19_relational_reasoning/)**  
â†’ **[Day 20: Relational RNNs](papers/20_Relational_RNNs/)**  
â†’ **[Day 21: Neural Message Passing](papers/21_Neural_Message_Passing/)**  
â†’ **[Day 22: Deep Speech 2](papers/22_deep_speech_2/)**  
â†’ **[Day 23: Variational Lossy Autoencoder](papers/23_variational_lossy_autoencoder/)**  
â†’ **[Day 24: GPipe (Giant Neural Networks)](papers/24_gpipe/)**  
â†’ **[Day 25: Scaling Laws for Neural Language Models](papers/25_scaling_laws/)**  
â†’ **[Day 26: Kolmogorov Complexity](papers/26_kolmogorov_complexity/)**  
â†’ **[Day 27: Machine Super Intelligence](papers/27_MSI/)** ğŸ†• **â† START HERE!**

Let's build something amazing together! ğŸš€
